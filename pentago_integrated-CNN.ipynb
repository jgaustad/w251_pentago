{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from helper_func import *\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import keras\n",
    "from multiprocessing import Pool\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from collections import deque\n",
    "from keras.activations import relu, linear\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Tensor(\"conv3d/Relu:0\", shape=(4, 26, 26, 26, 2), dtype=float32)\n",
      "(4, 26, 26, 26, 2)\n"
     ]
    }
   ],
   "source": [
    "# The inputs are 28x28x28 volumes with a single channel, and the  \n",
    "# batch size is 4  \n",
    "input_shape =(4, 28, 28, 28, 1)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv3D(2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class pentago:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state = None):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        #print('initializing')\n",
    "        \n",
    "        if state == None:\n",
    "            self.state = state = np.zeros((6,6), dtype=np.int)\n",
    "        self.history = []\n",
    "        self.winner = None\n",
    "        self.gameover = False\n",
    "        self.player_turn = 1\n",
    "    \n",
    "    def current_board_state(self):\n",
    "        # need to return a copy or bad stuff happens\n",
    "        return copy.copy(self.state)\n",
    "    \n",
    "    def game_history(self, player, move, cuad, rotatation):\n",
    "        self.history.append((boardstate_to_ideal_key(self.state), ideal_state(self.state), player, move, cuad, rotatation))\n",
    "        #return self.history\n",
    "\n",
    "    def find_winner(self, board_state):\n",
    "        player1_win = False\n",
    "        player_min1_win = False\n",
    "        diagonal1 = board_state.diagonal()\n",
    "        diagonal2 = np.fliplr(board_state).diagonal()\n",
    "        winning_slices =  np.vstack([board_state[1:,:].T, board_state[:-1,:].T, # all columns\n",
    "                              board_state[:,1:], board_state[:,:-1], # all rows\n",
    "                              diagonal1[1:], diagonal1[:-1], # diagonal 1\n",
    "                              diagonal2[1:],diagonal2[1:], # diagonal 2\n",
    "                              board_state.diagonal(offset=1), board_state.diagonal(offset=-1), # diagonal offsets \n",
    "                              np.fliplr(board_state).diagonal(offset=1), np.fliplr(board_state).diagonal(offset=-1)] ) # diagonal offsets\n",
    "        sums = np.dot(winning_slices, np.array([1,1,1,1,1]))\n",
    "        if 5 in sums: player1_win = True\n",
    "        if -5 in sums: player_min1_win = True\n",
    "        if player1_win == True or player_min1_win == True:\n",
    "           # print(\"Player 1 winner?\", player1_win, \"Player -1 winner?\", player_min1_win)\n",
    "            self.gameover = True\n",
    "            if player1_win == True:\n",
    "                self.winner = 1\n",
    "            elif player_min1_win ==True:\n",
    "                self.winner = -1\n",
    "            self.history.append(self.winner)\n",
    "        return \"Win\"\n",
    "\n",
    "    def check_gameover(self):\n",
    "        if not 0 in self.state:\n",
    "              self.gameover = True\n",
    "              #print(\"The game board is full!\")\n",
    "        \n",
    "    def full_move(self, move, cuad, direction, player, dtype=np.int):\n",
    "        if player != self.player_turn:\n",
    "            print( \"error, wrong player turn. No move taken.\")\n",
    "            return 'Error, wrong player turn.'\n",
    "        self.state = fullmove(self.state,move, cuad, direction, player)\n",
    "\n",
    "\n",
    "        self.game_history(move, player, cuad, direction)\n",
    "        self.find_winner(self.state) #return in find_winner if a winner is found\n",
    "        self.check_gameover() #return in check_gameover\n",
    "        if player == 1:\n",
    "            self.player_turn = -1\n",
    "        else:\n",
    "            self.player_turn = 1\n",
    "        #print('Successful Move')\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class q_table:\n",
    "\n",
    "    def __init__(self,length=0, games_played=0):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        self.time = datetime.now()\n",
    "        self.length = length\n",
    "        self.q_dict = {}\n",
    "        self.games_played = games_played\n",
    "\n",
    "  #def time(self):\n",
    "    #self.time = time\n",
    "\n",
    "    def length(self):\n",
    "        self.length += 1\n",
    "    #self.length = length  \n",
    "    \n",
    "    def get_q_value(self, boardstate):\n",
    "        return self.q_dict.get(boardstate, (0, 0))\n",
    "    \n",
    "    def update_q_value(self, boardstate, new_val, update_function = None):\n",
    "        q_val, n = self.get_q_value(boardstate) \n",
    "        if update_function:\n",
    "            #print('using custom function')\n",
    "            self.q_dict[boardstate] = update_function(q_val, n, new_val)\n",
    "        else:\n",
    "            self.q_dict[boardstate] = [new_val, n+1]\n",
    "        return self.q_dict[boardstate]\n",
    "    \n",
    "    def update_post_game(self, history, update_fn):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "\n",
    "    def update_post_game2(self, history, update_fn, decay_reward = .9):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "            winner *= decay_reward\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "\n",
    "    def __init__(self,list_density=None,lr=0.02):\n",
    "        self.model = self.build_model(list_density,lr)\n",
    "\n",
    "    def build_model(self,den,lr):\n",
    "        model = Sequential()\n",
    "        #model.add(tf.keras.Input(shape=(6,6,2,)))\n",
    "        #for layer in den:\n",
    "        model.add(Conv2D(\n",
    "                            filters = 64, \n",
    "                            kernel_size = (3,3),\n",
    "                            strides = (3,3),\n",
    "                            padding = 'valid',\n",
    "                            activation = relu,\n",
    "                            input_shape = (6,6,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation = relu))\n",
    "        model.add(Dense(128, activation = relu))\n",
    "\n",
    "\n",
    "        model.add(Dense(1, activation=linear))\n",
    "        \n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def update_model(self,states_batch, q_batch, epochs = 1):\n",
    "\n",
    "        self.model.fit(states_batch,q_batch,epochs = epochs, verbose=1)\n",
    "\n",
    "    def predict_model(self, state_batch):\n",
    "        return self.model.predict_on_batch(state_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, 2)\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05547169]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,1,1,-1,1,1],[0,0,0,0,0,0],[0,0,0,1,1,0],[0,0,1,-1,-1,0],[0,0,1,-1,1,0],[-1,-1,1,0,0,0]])\n",
    "x = boardstate_to_cnn_input(x)\n",
    "print(x.shape)\n",
    "out= c.model.predict(x.reshape(1,6,6,2))\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.99995\n",
      "1000 0.9511806740132733\n",
      "2000 0.904789914112052\n",
      "3000 0.8606617134311852\n",
      "4000 0.8186857229650423\n",
      "5000 0.7787569756237134\n",
      "6000 0.7407756237474893\n",
      "7000 0.7046466894232127\n",
      "8000 0.6702798269781429\n",
      "9000 0.6375890970574211\n",
      "10000 0.6064927517201779\n",
      "11000 0.5769130300168653\n",
      "12000 0.5487759635366593\n",
      "13000 0.5220111914386566\n",
      "14000 0.49655178450431714\n",
      "15000 0.47233407777119996\n",
      "16000 0.44929751132941503\n",
      "17000 0.4273844788827431\n",
      "18000 0.40654018369568434\n",
      "19000 0.3867125015662202\n",
      "20000 0.367851850481642\n",
      "21000 0.34991106663148985\n",
      "22000 0.332845286467567\n",
      "23000 0.31661183451608793\n",
      "24000 0.30117011666142574\n",
      "25000 0.286481518634604\n",
      "26000 0.2725093094526817\n",
      "27000 0.259218549567572\n",
      "28000 0.246576003494601\n",
      "29000 0.23455005670232934\n",
      "30000 0.22311063655580088\n",
      "31000 0.21222913711553318\n",
      "32000 0.20187834760418902\n",
      "33000 0.19203238436205666\n",
      "34000 0.18266662612118353\n",
      "35000 0.17375765243629993\n",
      "36000 0.16528318511857973\n",
      "37000 0.15722203252577774\n",
      "38000 0.14955403656943475\n",
      "39000 0.14226002230663626\n",
      "40000 0.1353217499902697\n",
      "41000 0.12872186945787317\n",
      "42000 0.12244387674502544\n",
      "43000 0.11647207281477254\n",
      "44000 0.11079152429989349\n",
      "45000 0.10538802615983875\n",
      "46000 0.10024806615895202\n",
      "47000 0.09535879107715316\n",
      "48000 0.09070797456858609\n",
      "49000 0.08628398658785626\n",
      "50000 0.08207576430740496\n",
      "51000 0.07807278445329505\n",
      "52000 0.07426503699022786\n",
      "53000 0.07064300008999028\n",
      "54000 0.06719761632073298\n",
      "55000 0.06392026999754027\n",
      "56000 0.06080276563765279\n",
      "57000 0.05783730746646704\n",
      "58000 0.05501647992306336\n",
      "59000 0.05233322911651293\n",
      "60000 0.04978084518659529\n",
      "61000 0.04735294552481254\n",
      "62000 0.0450434588137458\n",
      "63000 0.04284660984484019\n",
      "64000 0.04075690507665269\n",
      "65000 0.03876911889745041\n",
      "66000 0.03687828055780553\n",
      "67000 0.03507966174051114\n",
      "68000 0.03336876473673393\n",
      "69000 0.031741311198836865\n",
      "70000 0.03019323144174688\n",
      "71000 0.0287206542661129\n",
      "72000 0.027319897277807384\n",
      "73000 0.025987457679562245\n",
      "74000 0.02472000351171302\n",
      "75000 0.0235143653201477\n",
      "76000 0.02236752823062398\n",
      "77000 0.021276624409636378\n",
      "78000 0.0202389258929799\n",
      "79000 0.019251837764077552\n",
      "80000 0.018312891665012748\n",
      "81000 0.017419739624040163\n",
      "82000 0.016570148184139464\n",
      "83000 0.015761992817930556\n",
      "84000 0.014993252614982331\n",
      "85000 0.014262005228231584\n",
      "86000 0.01356642206687351\n",
      "87000 0.012904763723703517\n",
      "88000 0.012275375625475645\n",
      "89000 0.011676683895400804\n",
      "90000 0.011107191417438142\n",
      "91000 0.010565474092537885\n",
      "92000 0.010050177277473855\n",
      "93000 0.009560012397360337\n",
      "94000 0.009093753723382633\n",
      "95000 0.00865023530768317\n",
      "96000 0.008228348067738887\n",
      "97000 0.007827037012938349\n",
      "98000 0.007445298606423867\n",
      "99000 0.007082178255601129\n"
     ]
    }
   ],
   "source": [
    "v = 1\n",
    "for x in range(100000):\n",
    "    v*=.99995\n",
    "    if x%1000 == 0:\n",
    "        print(x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_agent:\n",
    "    \n",
    "    def __init__(self, nn, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.nn = nn\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            #print(possible_boardstate)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            cnn_input = boardstate_to_cnn_input(possible_boardstate)\n",
    "            #print(cnn_input)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append((cnn_input, move))\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        t0 = time.time()\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        #print(\"get possible boardstates\", time.time()-t0)\n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0][1] # capture the move needed\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "           # t0 = time.time()\n",
    "            nn_input_boardstate = []\n",
    "            move = []\n",
    "            for k,v in next_possible_boardstates.items():\n",
    "                nn_input_boardstate.append(v[0][0])\n",
    "                #print(v[0][0].shape)\n",
    "                move.append(v[0][1])\n",
    "          \n",
    "            #print(nn_input_boardstate, type(nn_input_boardstate))\n",
    "            #nn_input_batch = np.concatenate(nn_input_boardstate)\n",
    "            nn_input_batch = np.array(nn_input_boardstate)\n",
    "            #print(nn_input_batch, type(nn_input_batch))\n",
    "            #print(nn_input_boardstate)\n",
    "            q_values = self.nn.predict_model(nn_input_batch)\n",
    "            #print(\"get q_values boardstates\", time.time()-t0)\n",
    "\n",
    "            #print(q_values, type(q_values))\n",
    "            #print(\"----\")\n",
    "            q_values *= self.player\n",
    "            #q_values_list = [x*self.player for x in list(q_values)]\n",
    "            #print(q_values_list)\n",
    "            # get random index of a max value\n",
    "            #print('length fo q_values_list = ', len(q_values_list))\n",
    "            max_q = np.argmax(q_values)\n",
    "            #index_of_all_max = [i for i in range(len(q_values)) if q_values[i] == max_q]\n",
    "            #random_max_q_index = random.choice(index_of_all_max)\n",
    "            #print(\"MAX VALUE:\", q_values_list[random_max_q_index])\n",
    "            mv_to_take = move[max_q]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        #if self.epsilon > self.epsilon_min:\n",
    "        #    self.epsilon *= self.epsilon_decay \n",
    "        #else:\n",
    "        #    self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_agent_multigame:\n",
    "    \n",
    "    def __init__(self, nn, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.nn = nn\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            #print(possible_boardstate)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            cnn_input = boardstate_to_cnn_input(possible_boardstate)\n",
    "            #print(cnn_input)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append((cnn_input, move))\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        \n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0][1] # capture the move needed\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "            nn_input_boardstate = []\n",
    "            move = []\n",
    "            for k,v in next_possible_boardstates.items():\n",
    "                nn_input_boardstate.append(v[0][0])\n",
    "                #print(v[0][0].shape)\n",
    "                move.append(v[0][1])\n",
    "            \n",
    "            \n",
    "            #print(nn_input_boardstate, type(nn_input_boardstate))\n",
    "            #nn_input_batch = np.concatenate(nn_input_boardstate)\n",
    "            nn_input_batch = np.array(nn_input_boardstate)\n",
    "            #print(nn_input_batch, type(nn_input_batch))\n",
    "            #print(nn_input_boardstate)\n",
    "            q_values = self.nn.predict_model(nn_input_batch)\n",
    "            #print(q_values, type(q_values))\n",
    "            #print(\"----\")\n",
    "            q_values *= self.player\n",
    "            #q_values_list = [x*self.player for x in list(q_values)]\n",
    "            #print(q_values_list)\n",
    "            # get random index of a max value\n",
    "            #print('length fo q_values_list = ', len(q_values_list))\n",
    "            max_q = np.argmax(q_values)\n",
    "            #index_of_all_max = [i for i in range(len(q_values)) if q_values[i] == max_q]\n",
    "            #random_max_q_index = random.choice(index_of_all_max)\n",
    "            #print(\"MAX VALUE:\", q_values_list[random_max_q_index])\n",
    "            mv_to_take = move[max_q]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4,2,1,4])\n",
    "np.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn_1 = cnn_model([72,144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = 0, epsilon_decay = .999, epsilon = 0)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_1, epsilon_min = 0, epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0494189262390137\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "g = pentago()\n",
    "while g.gameover == False:\n",
    "    agent1.make_move(g)\n",
    "    if g.gameover == True: break\n",
    "    agent2.make_move(g)\n",
    "    #break\n",
    "\n",
    "#bs = agent1.get_possible_next_boardstates(g.current_board_state())\n",
    "    \n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> 28 28\n",
      "(28, 6, 6, 2) (28,)\n",
      "Epoch 1/1\n",
      "28/28 [==============================] - 1s 34ms/step - loss: 0.1720\n"
     ]
    }
   ],
   "source": [
    "g.history[:2]\n",
    "def reward_func(history, decay_factor = .9):\n",
    "    winner = history[-1]\n",
    "    nn_inputs = []\n",
    "    rewards = []\n",
    "    for boardposition in history[-2::-1]:\n",
    "        nn_inputs.append(boardposition[1])\n",
    "        rewards.append(winner)\n",
    "        winner *= decay_factor\n",
    "    return nn_inputs, rewards\n",
    "a = reward_func(g.history)\n",
    "print(type(a[0]), type(a[1]), len(a[0]), len(a[1]))\n",
    "x = np.array([boardstate_to_cnn_input(bs) for bs in a[0]])\n",
    "Y = np.array(a[1])\n",
    "print(x.shape, Y.shape)\n",
    "\n",
    "nn_1.update_model(x,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def little_sim(agents):\n",
    "    agent1, agent2 = agents\n",
    "    g = pentago()\n",
    "    while g.gameover == False:\n",
    "        agent1.make_move(g)\n",
    "        if g.gameover ==True: break\n",
    "        agent2.make_move(g)\n",
    "    #print('gameover.')\n",
    "    return g\n",
    "#little_sim((agent1,agent2))\n",
    "#if __name__ == '__main__':\n",
    "#    with Pool(1) as p:\n",
    "#        game_returns = p.map(little_sim, [(agent1,agent2)]*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '2', 0.0), (1, '9', 1607207032.787425), (2, '2', 3214414065.575404), (3, '4', 4821621098.363877), (4, '6', 6428828131.152436), (5, '4', 8036035163.939341), (6, '8', 9643242196.727467), (7, '9', 11250449229.516958), (8, '1', 12857656262.305271), (9, '6', 14464863295.093979)]\n"
     ]
    }
   ],
   "source": [
    "def test(x):\n",
    "    return x, str(time.time())[-1], x*time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(6) as p:\n",
    "        returns = p.map(test, [x for x in range(10)])\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_sim_parallel_nn(agent1, agent2, n_steps = 1, games_per_step = 32, nn_to_update = [], parallel_threads = 6):\n",
    "    game_times = []\n",
    "    evals = []\n",
    "    eval_times = []\n",
    "    epsilons = []\n",
    "    nn_update_times = []\n",
    "    winner_list = []\n",
    "    \n",
    "    for n in range(n_steps):\n",
    "        print('game_step', n, end = ' ')\n",
    "        game_start = time.time()\n",
    "        \n",
    "        #if __name__ == '__main__':\n",
    "        #    with Pool(parallel_threads) as p:\n",
    "        #        game_returns = p.map(little_sim, [(agent1,agent2)]*games_per_step)\n",
    "        game_returns = [little_sim((agent1,agent2)) for x in range(games_per_step)] #comment our parallelilization if needed.\n",
    "        \n",
    "        #games_in_progress = [pentago() for g in games_per_step]\n",
    "        #finished_games = []\n",
    "        #while \n",
    "            \n",
    "        game_times.append(time.time()-game_start)\n",
    "            \n",
    "        player1_winner = 0\n",
    "        player2_winner = 0\n",
    "        # check for winner and create update batch\n",
    "        nn_input_batch = []\n",
    "        rewards_for_batch = []\n",
    "        for game in game_returns:\n",
    "            if game.winner:\n",
    "                if game.winner == 1: player1_winner += 1\n",
    "                else: player2_winner += 1\n",
    "                \n",
    "                # accumulate rewards and inputs for training\n",
    "                boardstates, rewards = reward_func(game.history)\n",
    "                nn_input_batch += [boardstate_to_cnn_input(bs) for bs in boardstates] # add nn_inputs to training list (x)\n",
    "                rewards_for_batch += rewards # add rewards to training list (Y)\n",
    "        \n",
    "        # evaluate the model\n",
    "        t0 = time.time()\n",
    "        for nn in nn_to_update:\n",
    "            evals.append(nn.model.evaluate(np.array(nn_input_batch), np.array(rewards_for_batch)))\n",
    "        eval_times.append(time.time() - t0)\n",
    "\n",
    "        \n",
    "        # train the neural network\n",
    "        t0 = time.time()\n",
    "        print(f\"updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\")\n",
    "        for nn in nn_to_update:\n",
    "            nn.update_model(np.array(nn_input_batch), np.array(rewards_for_batch))\n",
    "        nn_update_times.append(time.time() - t0)\n",
    "\n",
    "\n",
    "        print(\"player 1 wins:\", player1_winner)\n",
    "        print(\"player 2 wins:\", player2_winner)\n",
    "        print(\"simulations took\", game_times[-1], \"seconds.\")\n",
    "        print(\"neural network update time:\", nn_update_times[-1], \"seconds.\")\n",
    "        print(\"eval score on batch:\", evals[-1])\n",
    "        print(\"epsilons agent1 and agent2:\", agent1.epsilon, agent2.epsilon)\n",
    "        epsilons.append((agent1.epsilon, agent2.epsilon))\n",
    "        if agent1.epsilon > agent1.epsilon_min: agent1.epsilon *= agent1.epsilon_decay\n",
    "        else: agent1.epsilon == agent1.min\n",
    "        if agent2.epsilon > agent2.epsilon_min: agent2.epsilon *= agent2.epsilon_decay\n",
    "        else: agent2.epsilon == agent2.min\n",
    "\n",
    "    # end of simulation runs, save q_table(s) to disk\n",
    "    nn_num = 1\n",
    "    time_str = str(datetime.now())[:19].replace(':','_')\n",
    "    for nn in nn_to_update:\n",
    "        with open(f'CNN_{nn_num}_'+time_str+'.pickle', 'wb') as file:\n",
    "            pickle.dump(nn, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        nn_num += 1\n",
    "    \n",
    "    return epsilons, nn_update_times, player1_winner, player2_winner, evals, eval_times\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Note you will overwrite this q_table and agents if you run this cell again.    Verify you won't lose your data!\n",
    "#with open('decay_q_table1_2020-11-29 12_09_00.pickle', 'rb') as file:\n",
    "#    qtable1 =  pickle.load(file)\n",
    "nn_1 = cnn_model()\n",
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = .02, epsilon = 1, epsilon_decay = .99)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_1, epsilon_min = .15, epsilon = 1, epsilon_decay = .995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 1s 974us/step\n",
      "updating with 797, 797 training batch.\n",
      "Epoch 1/1\n",
      "797/797 [==============================] - 1s 2ms/step - loss: 0.3802\n",
      "player 1 wins: 16\n",
      "player 2 wins: 13\n",
      "simulations took 56.79386496543884 seconds.\n",
      "neural network update time: 1.7375359535217285 seconds.\n",
      "eval score on batch: 0.20059990673472022\n",
      "epsilons agent1 and agent2: 1 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(1, 1)],\n",
       " [1.7375359535217285],\n",
       " 16,\n",
       " 13,\n",
       " [0.20059990673472022],\n",
       " [0.7785100936889648])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sim_parallel_nn(agent1, agent2, n_steps=1, games_per_step=32, nn_to_update=[nn_1], parallel_threads=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/922 [==============================] - 0s 43us/step\n",
      "updating with 922, 922 training batch.\n",
      "Epoch 1/1\n",
      "922/922 [==============================] - 0s 179us/step - loss: 0.1767\n",
      "player 1 wins: 20\n",
      "player 2 wins: 12\n",
      "simulations took 56.840158224105835 seconds.\n",
      "neural network update time: 0.16878223419189453 seconds.\n",
      "eval score on batch: 0.1794746730824096\n",
      "epsilons agent1 and agent2: 0.8097278682212583 0.9000874278732445\n",
      "826/826 [==============================] - 0s 39us/step\n",
      "updating with 826, 826 training batch.\n",
      "Epoch 1/1\n",
      "826/826 [==============================] - 0s 184us/step - loss: 0.1917\n",
      "player 1 wins: 19\n",
      "player 2 wins: 12\n",
      "simulations took 55.107194900512695 seconds.\n",
      "neural network update time: 0.1554570198059082 seconds.\n",
      "eval score on batch: 0.19209788689168833\n",
      "epsilons agent1 and agent2: 0.8016305895390458 0.8955869907338783\n",
      "839/839 [==============================] - 0s 43us/step\n",
      "updating with 839, 839 training batch.\n",
      "Epoch 1/1\n",
      "839/839 [==============================] - 0s 179us/step - loss: 0.1834\n",
      "player 1 wins: 18\n",
      "player 2 wins: 12\n",
      "simulations took 56.8230299949646 seconds.\n",
      "neural network update time: 0.15291500091552734 seconds.\n",
      "eval score on batch: 0.1830200026682062\n",
      "epsilons agent1 and agent2: 0.7936142836436553 0.8911090557802088\n",
      "784/784 [==============================] - 0s 43us/step\n",
      "updating with 784, 784 training batch.\n",
      "Epoch 1/1\n",
      "784/784 [==============================] - 0s 179us/step - loss: 0.2002\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 54.44166612625122 seconds.\n",
      "neural network update time: 0.14406299591064453 seconds.\n",
      "eval score on batch: 0.20023303129235093\n",
      "epsilons agent1 and agent2: 0.7856781408072188 0.8866535105013078\n",
      "815/815 [==============================] - 0s 44us/step\n",
      "updating with 815, 815 training batch.\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 0s 177us/step - loss: 0.1993\n",
      "player 1 wins: 15\n",
      "player 2 wins: 16\n",
      "simulations took 55.189777135849 seconds.\n",
      "neural network update time: 0.14772891998291016 seconds.\n",
      "eval score on batch: 0.20020889075232978\n",
      "epsilons agent1 and agent2: 0.7778213593991465 0.8822202429488013\n",
      "836/836 [==============================] - 0s 42us/step\n",
      "updating with 836, 836 training batch.\n",
      "Epoch 1/1\n",
      "836/836 [==============================] - 0s 180us/step - loss: 0.1871\n",
      "player 1 wins: 17\n",
      "player 2 wins: 13\n",
      "simulations took 56.80720829963684 seconds.\n",
      "neural network update time: 0.15421104431152344 seconds.\n",
      "eval score on batch: 0.19157982159530718\n",
      "epsilons agent1 and agent2: 0.7700431458051551 0.8778091417340573\n",
      "877/877 [==============================] - 0s 43us/step\n",
      "updating with 877, 877 training batch.\n",
      "Epoch 1/1\n",
      "877/877 [==============================] - 0s 170us/step - loss: 0.1708\n",
      "player 1 wins: 23\n",
      "player 2 wins: 9\n",
      "simulations took 56.52186584472656 seconds.\n",
      "neural network update time: 0.1524038314819336 seconds.\n",
      "eval score on batch: 0.17407927551267494\n",
      "epsilons agent1 and agent2: 0.7623427143471035 0.8734200960253871\n",
      "733/733 [==============================] - 0s 43us/step\n",
      "updating with 733, 733 training batch.\n",
      "Epoch 1/1\n",
      "733/733 [==============================] - 0s 174us/step - loss: 0.1926\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 56.420430183410645 seconds.\n",
      "neural network update time: 0.13144898414611816 seconds.\n",
      "eval score on batch: 0.19937253942634367\n",
      "epsilons agent1 and agent2: 0.7547192872036325 0.8690529955452602\n",
      "826/826 [==============================] - 0s 42us/step\n",
      "updating with 826, 826 training batch.\n",
      "Epoch 1/1\n",
      "826/826 [==============================] - 0s 183us/step - loss: 0.1923\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 56.27109098434448 seconds.\n",
      "neural network update time: 0.15450382232666016 seconds.\n",
      "eval score on batch: 0.19194542851127666\n",
      "epsilons agent1 and agent2: 0.7471720943315961 0.8647077305675338\n",
      "883/883 [==============================] - 0s 39us/step\n",
      "updating with 883, 883 training batch.\n",
      "Epoch 1/1\n",
      "883/883 [==============================] - 0s 178us/step - loss: 0.1885\n",
      "player 1 wins: 18\n",
      "player 2 wins: 14\n",
      "simulations took 56.446979999542236 seconds.\n",
      "neural network update time: 0.16008806228637695 seconds.\n",
      "eval score on batch: 0.19133307818441503\n",
      "epsilons agent1 and agent2: 0.7397003733882802 0.8603841919146962\n",
      "21 outputs loaded.\n",
      "868/868 [==============================] - 0s 44us/step\n",
      "updating with 868, 868 training batch.\n",
      "Epoch 1/1\n",
      "868/868 [==============================] - 0s 176us/step - loss: 0.1778\n",
      "player 1 wins: 20\n",
      "player 2 wins: 11\n",
      "simulations took 56.78149485588074 seconds.\n",
      "neural network update time: 0.15553498268127441 seconds.\n",
      "eval score on batch: 0.17813499491932172\n",
      "epsilons agent1 and agent2: 0.7323033696543974 0.8560822709551227\n",
      "809/809 [==============================] - 0s 42us/step\n",
      "updating with 809, 809 training batch.\n",
      "Epoch 1/1\n",
      "809/809 [==============================] - 0s 199us/step - loss: 0.1963\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 55.85877013206482 seconds.\n",
      "neural network update time: 0.163970947265625 seconds.\n",
      "eval score on batch: 0.20084724807363505\n",
      "epsilons agent1 and agent2: 0.7249803359578534 0.851801859600347\n",
      "871/871 [==============================] - 0s 43us/step\n",
      "updating with 871, 871 training batch.\n",
      "Epoch 1/1\n",
      "871/871 [==============================] - 0s 176us/step - loss: 0.1833\n",
      "player 1 wins: 19\n",
      "player 2 wins: 12\n",
      "simulations took 56.74363589286804 seconds.\n",
      "neural network update time: 0.15729093551635742 seconds.\n",
      "eval score on batch: 0.18786188300130152\n",
      "epsilons agent1 and agent2: 0.7177305325982748 0.8475428503023453\n",
      "819/819 [==============================] - 0s 46us/step\n",
      "updating with 819, 819 training batch.\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 0s 178us/step - loss: 0.1710\n",
      "player 1 wins: 20\n",
      "player 2 wins: 9\n",
      "simulations took 57.22215795516968 seconds.\n",
      "neural network update time: 0.1489248275756836 seconds.\n",
      "eval score on batch: 0.17166936433031446\n",
      "epsilons agent1 and agent2: 0.7105532272722921 0.8433051360508336\n",
      "814/814 [==============================] - 0s 45us/step\n",
      "updating with 814, 814 training batch.\n",
      "Epoch 1/1\n",
      "814/814 [==============================] - 0s 205us/step - loss: 0.1431\n",
      "player 1 wins: 26\n",
      "player 2 wins: 5\n",
      "simulations took 55.82330298423767 seconds.\n",
      "neural network update time: 0.17211389541625977 seconds.\n",
      "eval score on batch: 0.1528975438499361\n",
      "epsilons agent1 and agent2: 0.7034476949995692 0.8390886103705794\n",
      "826/826 [==============================] - 0s 45us/step\n",
      "updating with 826, 826 training batch.\n",
      "Epoch 1/1\n",
      "826/826 [==============================] - 0s 186us/step - loss: 0.2126\n",
      "player 1 wins: 17\n",
      "player 2 wins: 14\n",
      "simulations took 55.76776385307312 seconds.\n",
      "neural network update time: 0.15708684921264648 seconds.\n",
      "eval score on batch: 0.2590673998828084\n",
      "epsilons agent1 and agent2: 0.6964132180495735 0.8348931673187264\n",
      "782/782 [==============================] - 0s 41us/step\n",
      "updating with 782, 782 training batch.\n",
      "Epoch 1/1\n",
      "782/782 [==============================] - 0s 181us/step - loss: 0.2029\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 55.71926784515381 seconds.\n",
      "neural network update time: 0.1447601318359375 seconds.\n",
      "eval score on batch: 0.20285569960275268\n",
      "epsilons agent1 and agent2: 0.6894490858690777 0.8307187014821328\n",
      "817/817 [==============================] - 0s 40us/step\n",
      "updating with 817, 817 training batch.\n",
      "Epoch 1/1\n",
      "817/817 [==============================] - 0s 207us/step - loss: 0.1920\n",
      "player 1 wins: 14\n",
      "player 2 wins: 16\n",
      "simulations took 56.049213886260986 seconds.\n",
      "neural network update time: 0.17492914199829102 seconds.\n",
      "eval score on batch: 0.19295726436095692\n",
      "epsilons agent1 and agent2: 0.682554595010387 0.8265651079747222\n",
      "815/815 [==============================] - 0s 44us/step\n",
      "updating with 815, 815 training batch.\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 0s 190us/step - loss: 0.1859\n",
      "player 1 wins: 18\n",
      "player 2 wins: 11\n",
      "simulations took 57.1260039806366 seconds.\n",
      "neural network update time: 0.15894293785095215 seconds.\n",
      "eval score on batch: 0.19121207479684632\n",
      "epsilons agent1 and agent2: 0.6757290490602831 0.8224322824348486\n",
      "808/808 [==============================] - 0s 44us/step\n",
      "updating with 808, 808 training batch.\n",
      "Epoch 1/1\n",
      "808/808 [==============================] - 0s 182us/step - loss: 0.1587\n",
      "player 1 wins: 24\n",
      "player 2 wins: 6\n",
      "simulations took 56.624382972717285 seconds.\n",
      "neural network update time: 0.15168190002441406 seconds.\n",
      "eval score on batch: 0.16686582452811227\n",
      "epsilons agent1 and agent2: 0.6689717585696803 0.8183201210226743\n",
      "22 outputs loaded.\n",
      "811/811 [==============================] - 0s 42us/step\n",
      "updating with 811, 811 training batch.\n",
      "Epoch 1/1\n",
      "811/811 [==============================] - 0s 197us/step - loss: 0.1815\n",
      "player 1 wins: 23\n",
      "player 2 wins: 9\n",
      "simulations took 53.26247596740723 seconds.\n",
      "neural network update time: 0.16294622421264648 seconds.\n",
      "eval score on batch: 0.19259296175271604\n",
      "epsilons agent1 and agent2: 0.6622820409839835 0.8142285204175609\n",
      "844/844 [==============================] - 0s 45us/step\n",
      "updating with 844, 844 training batch.\n",
      "Epoch 1/1\n",
      "844/844 [==============================] - 0s 179us/step - loss: 0.1837\n",
      "player 1 wins: 20\n",
      "player 2 wins: 11\n",
      "simulations took 55.931259870529175 seconds.\n",
      "neural network update time: 0.15433502197265625 seconds.\n",
      "eval score on batch: 0.1827929819690205\n",
      "epsilons agent1 and agent2: 0.6556592205741436 0.810157377815473\n",
      "817/817 [==============================] - 0s 46us/step\n",
      "updating with 817, 817 training batch.\n",
      "Epoch 1/1\n",
      "817/817 [==============================] - 0s 180us/step - loss: 0.1954\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 56.15848112106323 seconds.\n",
      "neural network update time: 0.15039467811584473 seconds.\n",
      "eval score on batch: 0.19969902911669182\n",
      "epsilons agent1 and agent2: 0.6491026283684022 0.8061065909263957\n",
      "757/757 [==============================] - 0s 39us/step\n",
      "updating with 757, 757 training batch.\n",
      "Epoch 1/1\n",
      "757/757 [==============================] - 0s 183us/step - loss: 0.1621\n",
      "player 1 wins: 24\n",
      "player 2 wins: 5\n",
      "simulations took 55.79420614242554 seconds.\n",
      "neural network update time: 0.1410067081451416 seconds.\n",
      "eval score on batch: 0.2088442297758168\n",
      "epsilons agent1 and agent2: 0.6426116020847181 0.8020760579717637\n",
      "727/727 [==============================] - 0s 37us/step\n",
      "updating with 727, 727 training batch.\n",
      "Epoch 1/1\n",
      "727/727 [==============================] - 0s 202us/step - loss: 0.2373\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 54.18493175506592 seconds.\n",
      "neural network update time: 0.1497337818145752 seconds.\n",
      "eval score on batch: 0.320974998780753\n",
      "epsilons agent1 and agent2: 0.6361854860638709 0.798065677681905\n",
      "843/843 [==============================] - 0s 42us/step\n",
      "updating with 843, 843 training batch.\n",
      "Epoch 1/1\n",
      "843/843 [==============================] - 0s 182us/step - loss: 0.2012\n",
      "player 1 wins: 20\n",
      "player 2 wins: 11\n",
      "simulations took 57.984880208969116 seconds.\n",
      "neural network update time: 0.15729475021362305 seconds.\n",
      "eval score on batch: 0.2332744604482869\n",
      "epsilons agent1 and agent2: 0.6298236312032323 0.7940753492934954\n",
      "827/827 [==============================] - 0s 43us/step\n",
      "updating with 827, 827 training batch.\n",
      "Epoch 1/1\n",
      "827/827 [==============================] - 0s 193us/step - loss: 0.1947\n",
      "player 1 wins: 20\n",
      "player 2 wins: 12\n",
      "simulations took 55.71865129470825 seconds.\n",
      "neural network update time: 0.16279387474060059 seconds.\n",
      "eval score on batch: 0.1999304168818219\n",
      "epsilons agent1 and agent2: 0.6235253948912 0.7901049725470279\n",
      "game_step 7 "
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## Change number of games to simulate here\n",
    "#_games = 10000\n",
    "##################################################\n",
    "time0 = time.time()\n",
    "for x in range(20):\n",
    "    outputs.append(big_sim_parallel_nn(agent1, agent2, n_steps=10, games_per_step=32, nn_to_update=[nn_1], parallel_threads=6))\n",
    "    print(len(outputs), \"outputs loaded.\")\n",
    "print(time.time()-time0, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.593888888888888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59738/60/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444204"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qtable1.q_dict)\n",
    "#agent1.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = [2,2,2,2,2]\n",
    "\n",
    "with open('test.pickle', 'wb') as file:\n",
    "    pickle.dump(x, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('test.pickle', 'rb') as file:\n",
    "    y = pickle.load(file)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = []\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    ns.append(v[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5444199,       3,       1,       0,       0,       0,       0,\n",
       "              0,       0,       1]),\n",
       " array([1.000000e+00, 1.797250e+04, 3.594400e+04, 5.391550e+04,\n",
       "        7.188700e+04, 8.985850e+04, 1.078300e+05, 1.258015e+05,\n",
       "        1.437730e+05, 1.617445e+05, 1.797160e+05]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(np.array(ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153873\n",
      "67959\n",
      "46602\n",
      "33784\n",
      "12230\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in ns if x != 1]))\n",
    "print(len([x for x in ns if x > 2]))\n",
    "print(len([x for x in ns if x > 3]))\n",
    "print(len([x for x in ns if x > 4]))\n",
    "print(len([x for x in ns if x > 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120020100111022211102201112221201221] (0.5, 1)\n",
      "[020011100211022012102201112221201221] (0.5, 1)\n",
      "[002011202211010012102201112201201221] (0.5, 1)\n",
      "[002011202211010012210201010201122221] (0.5, 1)\n",
      "[020011100211022012210001010201122221] (0.5, 1)\n",
      "[110020112001200220022012002010111221] (0.5, 1)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    print(k,v)\n",
    "    i += 1\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([94371,     6,     0,     0,     0,     0,     0,     0,     0,\n",
       "            1]),\n",
       " array([2.00000e+00, 1.15380e+04, 2.30740e+04, 3.46100e+04, 4.61460e+04,\n",
       "        5.76820e+04, 6.92180e+04, 8.07540e+04, 9.22900e+04, 1.03826e+05,\n",
       "        1.15362e+05]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram([x for x in ns if x != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
