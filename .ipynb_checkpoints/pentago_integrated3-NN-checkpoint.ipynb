{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from helper_func import *\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "from keras.activations import relu, linear\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pentago:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state = None):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        #print('initializing')\n",
    "        \n",
    "        if state == None:\n",
    "            self.state = state = np.zeros((6,6), dtype=np.int)\n",
    "        self.history = []\n",
    "        self.winner = None\n",
    "        self.gameover = False\n",
    "        self.player_turn = 1\n",
    "    \n",
    "    def current_board_state(self):\n",
    "        # need to return a copy or bad stuff happens\n",
    "        return copy.copy(self.state)\n",
    "    \n",
    "    def game_history(self, player, move, cuad, rotatation):\n",
    "        self.history.append((boardstate_to_ideal_key(self.state), ideal_state(self.state), player, move, cuad, rotatation))\n",
    "        #return self.history\n",
    "\n",
    "    def find_winner(self, board_state):\n",
    "        player1_win = False\n",
    "        player_min1_win = False\n",
    "        diagonal1 = board_state.diagonal()\n",
    "        diagonal2 = np.fliplr(board_state).diagonal()\n",
    "        winning_slices =  np.vstack([board_state[1:,:].T, board_state[:-1,:].T, # all columns\n",
    "                              board_state[:,1:], board_state[:,:-1], # all rows\n",
    "                              diagonal1[1:], diagonal1[:-1], # diagonal 1\n",
    "                              diagonal2[1:],diagonal2[1:], # diagonal 2\n",
    "                              board_state.diagonal(offset=1), board_state.diagonal(offset=-1), # diagonal offsets \n",
    "                              np.fliplr(board_state).diagonal(offset=1), np.fliplr(board_state).diagonal(offset=-1)] ) # diagonal offsets\n",
    "        sums = np.dot(winning_slices, np.array([1,1,1,1,1]))\n",
    "        if 5 in sums: player1_win = True\n",
    "        if -5 in sums: player_min1_win = True\n",
    "        if player1_win == True or player_min1_win == True:\n",
    "           # print(\"Player 1 winner?\", player1_win, \"Player -1 winner?\", player_min1_win)\n",
    "            self.gameover = True\n",
    "            if player1_win == True:\n",
    "                self.winner = 1\n",
    "            elif player_min1_win ==True:\n",
    "                self.winner = -1\n",
    "            self.history.append(self.winner)\n",
    "        return \"Win\"\n",
    "\n",
    "    def check_gameover(self):\n",
    "        if not 0 in self.state:\n",
    "              self.gameover = True\n",
    "              #print(\"The game board is full!\")\n",
    "        \n",
    "    def full_move(self, move, cuad, direction, player, dtype=np.int):\n",
    "        if player != self.player_turn:\n",
    "            print( \"error, wrong player turn. No move taken.\")\n",
    "            return 'Error, wrong player turn.'\n",
    "        self.state = fullmove(self.state,move, cuad, direction, player)\n",
    "\n",
    "\n",
    "        self.game_history(move, player, cuad, direction)\n",
    "        self.find_winner(self.state) #return in find_winner if a winner is found\n",
    "        self.check_gameover() #return in check_gameover\n",
    "        if player == 1:\n",
    "            self.player_turn = -1\n",
    "        else:\n",
    "            self.player_turn = 1\n",
    "        #print('Successful Move')\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class q_table:\n",
    "\n",
    "    def __init__(self,length=0, games_played=0):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        self.time = datetime.now()\n",
    "        self.length = length\n",
    "        self.q_dict = {}\n",
    "        self.games_played = games_played\n",
    "\n",
    "  #def time(self):\n",
    "    #self.time = time\n",
    "\n",
    "    def length(self):\n",
    "        self.length += 1\n",
    "    #self.length = length  \n",
    "    \n",
    "    def get_q_value(self, boardstate):\n",
    "        return self.q_dict.get(boardstate, (0, 0))\n",
    "    \n",
    "    def update_q_value(self, boardstate, new_val, update_function = None):\n",
    "        q_val, n = self.get_q_value(boardstate) \n",
    "        if update_function:\n",
    "            #print('using custom function')\n",
    "            self.q_dict[boardstate] = update_function(q_val, n, new_val)\n",
    "        else:\n",
    "            self.q_dict[boardstate] = [new_val, n+1]\n",
    "        return self.q_dict[boardstate]\n",
    "    \n",
    "    def update_post_game(self, history, update_fn):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "\n",
    "    def update_post_game2(self, history, update_fn, decay_reward = .9):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "            winner *= decay_reward\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_model:\n",
    "\n",
    "    def __init__(self,list_density=None,lr=0.02):\n",
    "        self.model = self.build_model(list_density,lr)\n",
    "\n",
    "    def build_model(self,den,lr):\n",
    "   \n",
    "        if den == None:\n",
    "            den = [254]\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(den[0], input_dim= 36*2, activation=relu))\n",
    "    \n",
    "        if len(den) > 1:\n",
    "            for k in range(1,len(den)):\n",
    "                model.add(Dense(den[k], activation=relu))\n",
    "    \n",
    "        model.add(Dense(1, activation=linear))\n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def update_model(self,states_batch, q_batch, epochs = 10):\n",
    "\n",
    "        nsamples = states_batch.shape[0]\n",
    "        code_batch = np.zeros((nsamples,2*36))\n",
    "\n",
    "        for k in range(nsamples):\n",
    "              code_batch[k,:] = boardstate_to_nn_input(states_batch[k,:,:])\n",
    "\n",
    "        self.model.fit(code_batch,q_batch,epochs = epochs, verbose=1)\n",
    "\n",
    "    def predict_model(self, state_batch):\n",
    "        return self.model.predict_on_batch(state_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(q, n, nn):\n",
    "    #print('here',q, n, nn, 'end')\n",
    "    #q, n = cv\n",
    "    return (q*n+nn)/(n+1), n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dampen_func(q, n, nn):\n",
    "    #print('here',q, n, nn, 'end')\n",
    "    #q, n = cv\n",
    "    return (q*(n+1)+nn)/(n+2), n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_agent:\n",
    "    \n",
    "    def __init__(self, nn, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.nn = nn\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            nn_input = boardstate_to_nn_input(possible_boardstate)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append((nn_input, move))\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        \n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0][1] # capture the move needed\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "            nn_input_boardstate = []\n",
    "            move = []\n",
    "            for k,v in next_possible_boardstates.items():\n",
    "                nn_input_boardstate.append(v[0][0])\n",
    "                #print(v[0][0].shape)\n",
    "                move.append(v[0][1])\n",
    "            \n",
    "            nn_input_batch = np.concatenate(nn_input_boardstate)\n",
    "            #print(len(nn_input_boardstate))\n",
    "            q_values = self.nn.predict_model(nn_input_batch)\n",
    "            #print(q_values, type(q_values))\n",
    "            #print(\"----\")\n",
    "            q_values_list = [x*self.player for x in list(q_values)]\n",
    "            #print(q_values_list)\n",
    "            # get random index of a max value\n",
    "            #print('length fo q_values_list = ', len(q_values_list))\n",
    "            max_q = np.max(q_values_list)\n",
    "            index_of_all_max = [i for i in range(len(q_values_list)) if q_values_list[i] == max_q]\n",
    "            random_max_q_index = random.choice(index_of_all_max)\n",
    "            #print(\"MAX VALUE:\", q_values_list[random_max_q_index])\n",
    "            mv_to_take = move[random_max_q_index]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 72)                5256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 144)               10512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 145       \n",
      "=================================================================\n",
      "Total params: 15,913\n",
      "Trainable params: 15,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn_1 = nn_model([72,144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = 0, epsilon = 0)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_1, epsilon_min = 0, epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9851741790771484\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "g = pentago()\n",
    "while g.gameover == False:\n",
    "    agent1.make_move(g)\n",
    "    if g.gameover == True: break\n",
    "    agent2.make_move(g)\n",
    "    #break\n",
    "\n",
    "#bs = agent1.get_possible_next_boardstates(g.current_board_state())\n",
    "    \n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 1, -1,  1,  1,  1,  1],\n",
       "         [-1,  0,  0,  1,  0,  0],\n",
       "         [-1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1, -1, -1,  1],\n",
       "         [ 1, -1, -1, -1, -1, -1],\n",
       "         [ 0,  0,  1, -1,  1, -1]]),\n",
       "  array([[ 1, -1, -1,  1,  1,  0],\n",
       "         [-1,  0, -1, -1,  0,  0],\n",
       "         [ 1,  0,  1,  1, -1,  1],\n",
       "         [ 1,  1,  1, -1,  1, -1],\n",
       "         [ 1,  0, -1, -1, -1, -1],\n",
       "         [ 1,  0,  1, -1, -1,  1]]),\n",
       "  array([[ 1, -1,  1,  1,  1,  1],\n",
       "         [-1,  0,  0,  1,  0,  0],\n",
       "         [-1, -1,  1,  0, -1,  1],\n",
       "         [ 1, -1,  1, -1, -1,  1],\n",
       "         [ 1,  0, -1, -1, -1, -1],\n",
       "         [ 0,  0,  1, -1,  1, -1]]),\n",
       "  array([[ 1, -1, -1,  1,  1,  0],\n",
       "         [-1,  0, -1, -1,  0,  0],\n",
       "         [ 1,  0,  1,  1, -1,  1],\n",
       "         [ 1,  1,  0, -1,  1, -1],\n",
       "         [ 1,  0, -1, -1, -1,  0],\n",
       "         [ 1,  0,  1, -1, -1,  1]]),\n",
       "  array([[ 1, -1,  1,  1,  1,  1],\n",
       "         [-1,  0,  0,  1,  0,  0],\n",
       "         [-1, -1,  1,  0, -1,  1],\n",
       "         [ 1, -1,  1, -1, -1,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  1, -1,  1, -1]]),\n",
       "  array([[ 1, -1, -1,  1,  0,  0],\n",
       "         [-1,  0, -1, -1,  0,  0],\n",
       "         [ 1,  0,  1,  1, -1,  1],\n",
       "         [ 1,  1,  0, -1,  1, -1],\n",
       "         [ 1,  0,  0, -1, -1,  0],\n",
       "         [ 1,  0,  1, -1, -1,  1]]),\n",
       "  array([[ 1, -1,  1,  1,  1,  1],\n",
       "         [-1,  0,  0,  1,  0,  0],\n",
       "         [-1, -1,  1,  0,  0,  1],\n",
       "         [ 0, -1,  1, -1, -1,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  1, -1,  1, -1]]),\n",
       "  array([[ 1, -1, -1,  0,  0,  0],\n",
       "         [-1,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  1,  1, -1,  1],\n",
       "         [ 1,  1,  0, -1,  1, -1],\n",
       "         [ 1,  0,  0, -1, -1,  0],\n",
       "         [ 1,  0,  1, -1, -1,  1]]),\n",
       "  array([[ 1, -1,  1,  1,  1,  1],\n",
       "         [-1,  0,  0,  1,  0,  0],\n",
       "         [-1,  0,  1,  0,  0,  0],\n",
       "         [ 0, -1,  1, -1, -1,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  1, -1,  1, -1]]),\n",
       "  array([[ 1,  0, -1,  0,  0,  0],\n",
       "         [-1,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  1,  1, -1,  1],\n",
       "         [ 1,  1,  0, -1,  1, -1],\n",
       "         [ 1,  0,  0, -1, -1,  0],\n",
       "         [ 1,  0,  0, -1, -1,  1]]),\n",
       "  array([[ 1, -1,  1,  0,  1,  1],\n",
       "         [ 0,  0,  0,  1,  0,  0],\n",
       "         [-1,  0,  1,  0,  0,  0],\n",
       "         [ 0, -1,  1, -1, -1,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  1, -1,  1, -1]]),\n",
       "  array([[ 1,  0, -1,  0,  0,  0],\n",
       "         [-1,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  1,  1, -1,  1],\n",
       "         [ 0,  1,  0, -1,  1, -1],\n",
       "         [ 1,  0,  0, -1, -1,  0],\n",
       "         [ 1,  0,  0, -1,  0,  1]]),\n",
       "  array([[ 1, -1,  1,  0,  1,  1],\n",
       "         [ 0,  0,  0,  1,  0,  0],\n",
       "         [-1,  0,  1,  0,  0,  0],\n",
       "         [ 0, -1,  1, -1,  0,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  0, -1,  1, -1]]),\n",
       "  array([[ 1,  0, -1,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  1,  1, -1,  0],\n",
       "         [ 0,  1,  0, -1,  1, -1],\n",
       "         [ 1,  0,  0, -1, -1,  0],\n",
       "         [ 1,  0,  0, -1,  0,  1]]),\n",
       "  array([[ 1,  0,  1,  0,  1,  1],\n",
       "         [ 0,  0,  0,  1,  0,  0],\n",
       "         [-1,  0,  1,  0,  0,  0],\n",
       "         [ 0, -1,  0, -1,  0,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  0, -1,  1, -1]]),\n",
       "  array([[ 1,  0, -1,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  1,  0, -1,  0],\n",
       "         [ 0,  1,  0, -1,  1,  0],\n",
       "         [ 1,  0,  0, -1, -1,  0],\n",
       "         [ 1,  0,  0, -1,  0,  1]]),\n",
       "  array([[ 1,  0,  1,  0,  1,  1],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [-1,  0,  1,  0,  0,  0],\n",
       "         [ 0, -1,  0, -1,  0,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0]]),\n",
       "  array([[ 1,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  1,  0, -1,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0],\n",
       "         [ 1,  0,  0, -1, -1,  0],\n",
       "         [ 1,  0,  0, -1,  0,  1]]),\n",
       "  array([[ 1,  0,  1,  0,  1,  1],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0, -1,  0, -1,  0,  1],\n",
       "         [ 0,  0, -1, -1, -1,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0]]),\n",
       "  array([[ 1,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  0,  0, -1,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0],\n",
       "         [ 1,  0,  0, -1,  0,  0],\n",
       "         [ 1,  0,  0, -1,  0,  1]]),\n",
       "  array([[ 1,  0,  1,  0,  1,  1],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0, -1,  0, -1,  0,  0],\n",
       "         [ 0,  0, -1, -1,  0,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0]]),\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 1,  0,  1,  0, -1,  0],\n",
       "         [ 0,  0,  0, -1, -1, -1],\n",
       "         [ 1,  0,  0,  0,  0,  1],\n",
       "         [ 1,  0,  0,  0,  0,  0]]),\n",
       "  array([[ 1,  0,  0,  0,  1,  1],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  0],\n",
       "         [ 0,  0, -1, -1,  0,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0]]),\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 1,  0,  0,  0, -1,  0],\n",
       "         [ 0,  0,  0,  0, -1, -1],\n",
       "         [ 1,  0,  0,  0,  0,  1],\n",
       "         [ 1,  0,  0,  0,  0,  0]]),\n",
       "  array([[ 0,  0,  0,  1,  0,  0],\n",
       "         [ 0,  0,  0,  1,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0, -1, -1,  0,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0]]),\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, -1,  0],\n",
       "         [ 0,  0,  0,  0,  0, -1],\n",
       "         [ 1,  0,  0,  0,  0,  1],\n",
       "         [ 1,  0,  0,  0,  0,  0]]),\n",
       "  array([[ 0,  0,  0,  0,  0,  1],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0, -1,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  1,  0]]),\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, -1,  0],\n",
       "         [ 0,  0,  0,  0,  1,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 1,  0,  0,  0,  0,  0]]),\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0, -1,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  1,  0]]),\n",
       "  array([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]])],\n",
       " [-1,\n",
       "  -0.9,\n",
       "  -0.81,\n",
       "  -0.7290000000000001,\n",
       "  -0.6561000000000001,\n",
       "  -0.5904900000000002,\n",
       "  -0.5314410000000002,\n",
       "  -0.47829690000000014,\n",
       "  -0.43046721000000016,\n",
       "  -0.38742048900000015,\n",
       "  -0.34867844010000015,\n",
       "  -0.31381059609000017,\n",
       "  -0.28242953648100017,\n",
       "  -0.25418658283290013,\n",
       "  -0.22876792454961012,\n",
       "  -0.2058911320946491,\n",
       "  -0.1853020188851842,\n",
       "  -0.16677181699666577,\n",
       "  -0.1500946352969992,\n",
       "  -0.13508517176729928,\n",
       "  -0.12157665459056936,\n",
       "  -0.10941898913151243,\n",
       "  -0.0984770902183612,\n",
       "  -0.08862938119652508,\n",
       "  -0.07976644307687257,\n",
       "  -0.07178979876918531,\n",
       "  -0.06461081889226679,\n",
       "  -0.05814973700304011,\n",
       "  -0.0523347633027361,\n",
       "  -0.04710128697246249])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.history[:2]\n",
    "def reward_func(history, decay_factor = .9):\n",
    "    winner = history[-1]\n",
    "    nn_inputs = []\n",
    "    rewards = []\n",
    "    for boardposition in history[-2::-1]:\n",
    "        nn_inputs.append(boardposition[1])\n",
    "        rewards.append(winner)\n",
    "        winner *= decay_factor\n",
    "    return nn_inputs, rewards\n",
    "reward_func(g.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0])\n",
    "x.shape\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=72, activation=relu))\n",
    "\n",
    "\n",
    "for k in range(2):\n",
    "    model.add(Dense(32, activation=relu))\n",
    "\n",
    "model.add(Dense(1, activation=linear))\n",
    "model.compile(loss=mean_squared_error,optimizer=Adam(lr=.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 72)\n",
      "(72,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01385177]], dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(1,72)\n",
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "model.predict(x[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "class qtable_agent:\n",
    "    \n",
    "    def __init__(self, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5, q_table = q_table()):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = q_table\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append(move)\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        \n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0]\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "            q_values_list = [self.q_table.get_q_value(bs)[0]*self.player for bs in key_list] # *player flips the q's for -1 player to allow max calc\n",
    "            #print(q_values_list)\n",
    "            \n",
    "            # get random index of a max value\n",
    "            max_q = (max(q_values_list))\n",
    "            index_of_all_max = [i for i in range(len(q_values_list)) if q_values_list[i] == max_q]\n",
    "            random_max_q_index = random.choice(index_of_all_max)\n",
    "            \n",
    "            mv_to_take = next_possible_boardstates[key_list[random_max_q_index]][0]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def little_sim(agents):\n",
    "    agent1, agent2 = agents\n",
    "    g = pentago()\n",
    "    while g.gameover == False:\n",
    "        agent1.make_move(g)\n",
    "        if g.gameover ==True: break\n",
    "        agent2.make_move(g)\n",
    "    #print('gameover.')\n",
    "    return g\n",
    "        \n",
    "#t0 = time.time()\n",
    "#if __name__ == '__main__':\n",
    "#    with Pool(6) as p:\n",
    "#        game_returns = p.map(little_sim, [(agent1,agent2)]*500)\n",
    "#time.time() -t0\n",
    "\n",
    "#6 on 500 is 222sec\n",
    "\n",
    "#6 100 is 79sec\n",
    "#4 100 is 67sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def little_sim(agents):\n",
    "    agent1, agent2 = agents\n",
    "    g = pentago()\n",
    "    while g.gameover == False:\n",
    "        agent1.make_move(g)\n",
    "        if g.gameover ==True: break\n",
    "        agent2.make_move(g)\n",
    "    print('gameover.')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_sim_parallel_nn(agent1, agent2, n_steps = 1, games_per_step = 500, nn_to_update = [], parallel_threads = 6):\n",
    "    game_times = []\n",
    "    nn_update_times = []\n",
    "    winner_list = []\n",
    "    \n",
    "    for n in range(n_steps):\n",
    "        print('game_step', n, end = ' ')\n",
    "        game_start = time.time()\n",
    "        \n",
    "       # if __name__ == '__main__':\n",
    "       #     with Pool(parallel_threads) as p:\n",
    "       #         game_returns = p.map(little_sim, [(agent1,agent2)]*games_per_step)\n",
    "        game_returns = [little_sim((agent1,agent2)) for x in range(games_per_step)]\n",
    "            \n",
    "        game_times.append(time.time()-game_start)\n",
    "            \n",
    "        player1_winner = 0\n",
    "        player2_winner = 0\n",
    "        # check for winner and create update batch\n",
    "        nn_input_batch = []\n",
    "        rewards_for_batch = []\n",
    "        for game in game_returns:\n",
    "            if game.winner:\n",
    "                if game.winner == 1: player1_winner += 1\n",
    "                else: player2_winner += 1\n",
    "                \n",
    "                #cumulate rewards\n",
    "                nn_inputs, rewards = reward_func(game.history)\n",
    "                nn_input_batch += nn_inputs\n",
    "                rewards_for_batch += rewards\n",
    "        # train the neural network\n",
    "        t0 = time.time()\n",
    "        for nn in nn_to_update:\n",
    "            nn.update_model(np.array(nn_input_batch), np.array(rewards_for_batch))\n",
    "        nn_update_times.append(time.time() - t0)\n",
    "        \n",
    "        print(\"player 1 wins:\", player1_winner)\n",
    "        print(\"player 2 wins:\", player2_winner)\n",
    "        print(\"parallelized batch took\", game_times[-1], \"seconds.\")\n",
    "        print(\"neural network update:\", nn_update_times[-1], \"seconds.\")\n",
    "        \n",
    "    # end of simulation runs, save q_table(s) to disk\n",
    "    qt_num = 1\n",
    "    time_str = str(datetime.now())[:19].replace(':','_')\n",
    "    for q_tab in qtables_to_update:\n",
    "        with open(f'decay_q_table{qt_num}_'+time_str+'.pickle', 'wb') as file:\n",
    "            pickle.dump(q_tab, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        qt_num += 1\n",
    "    \n",
    "    return game_times\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def big_sim_parallel(agent1, agent2, n_steps = 1, games_per_step = 500, nn_to_update = [], parallel_threads = 6,  update_cadence = 1):\n",
    "    game_times = []\n",
    "    q_dict_update_times = []\n",
    "    winner_list = []\n",
    "    \n",
    "    for n in range(n_steps):\n",
    "        print('game_step', n, end = ' ')\n",
    "        game_start = time.time()\n",
    "        \n",
    "        if __name__ == '__main__':\n",
    "            with Pool(parallel_threads) as p:\n",
    "                game_returns = p.map(little_sim, [(agent1,agent2)]*games_per_step)\n",
    "\n",
    "            \n",
    "        game_times.append(time.time()-game_start)\n",
    "        \n",
    "        player1_winner = 0\n",
    "        player2_winner = 0\n",
    "        # check for winner and update q_table(s)\n",
    "        for game in game_returns:\n",
    "            if game.winner:\n",
    "                if game.winner == 1: player1_winner += 1\n",
    "                else: player2_winner += 1\n",
    "                \n",
    "                for q_tab in qtables_to_update:\n",
    "                    q_tab.update_post_game2(game.history, dampen_func)\n",
    "        print(\"player 1 wins:\", player1_winner)\n",
    "        print(\"player 2 wins:\", player2_winner)\n",
    "        print(\"parallelized batch took\", game_times[-1], \"seconds.\")\n",
    "        \n",
    "    # end of simulation runs, save q_table(s) to disk\n",
    "    qt_num = 1\n",
    "    time_str = str(datetime.now())[:19].replace(':','_')\n",
    "    for q_tab in qtables_to_update:\n",
    "        with open(f'decay_q_table{qt_num}_'+time_str+'.pickle', 'wb') as file:\n",
    "            pickle.dump(q_tab, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        qt_num += 1\n",
    "    \n",
    "    return game_times\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_87 (Dense)             (None, 256)               18688     \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 51,713\n",
      "Trainable params: 51,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Note you will overwrite this q_table and agents if you run this cell again.    Verify you won't lose your data!\n",
    "#with open('decay_q_table1_2020-11-29 12_09_00.pickle', 'rb') as file:\n",
    "#    qtable1 =  pickle.load(file)\n",
    "nn_1 = nn_model([256,128])\n",
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = 0, epsilon = 1)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_1, epsilon_min = 0, epsilon = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_step 0 gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "gameover.\n",
      "Epoch 1/10\n",
      "239/239 [==============================] - 5s 21ms/step - loss: 2.3775\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 0s 1ms/step - loss: 0.2114\n",
      "Epoch 3/10\n",
      "239/239 [==============================] - 0s 702us/step - loss: 0.1458\n",
      "Epoch 4/10\n",
      "239/239 [==============================] - 0s 1ms/step - loss: 0.0937\n",
      "Epoch 5/10\n",
      "239/239 [==============================] - 0s 887us/step - loss: 0.0563\n",
      "Epoch 6/10\n",
      "239/239 [==============================] - 0s 830us/step - loss: 0.0347\n",
      "Epoch 7/10\n",
      "239/239 [==============================] - 0s 589us/step - loss: 0.0277\n",
      "Epoch 8/10\n",
      "239/239 [==============================] - 0s 775us/step - loss: 0.0228\n",
      "Epoch 9/10\n",
      "239/239 [==============================] - 0s 796us/step - loss: 0.0172\n",
      "Epoch 10/10\n",
      "239/239 [==============================] - 0s 832us/step - loss: 0.0132\n",
      "player 1 wins: 6\n",
      "player 2 wins: 4\n",
      "parallelized batch took 55.97850012779236 seconds.\n",
      "neural network update: 8.041104078292847 seconds.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qtables_to_update' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-64c2f1932284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbig_sim_parallel_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames_per_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_to_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-141-4dc2dd86951d>\u001b[0m in \u001b[0;36mbig_sim_parallel_nn\u001b[0;34m(agent1, agent2, n_steps, games_per_step, nn_to_update, parallel_threads)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mqt_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mq_tab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqtables_to_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'decay_q_table{qt_num}_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qtables_to_update' is not defined"
     ]
    }
   ],
   "source": [
    "big_sim_parallel_nn(agent1, agent2, n_steps=1, games_per_step=10, nn_to_update=[nn_1], parallel_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_step 0 player 1 wins: 279\n",
      "player 2 wins: 197\n",
      "parallelized batch took 224.71044516563416 seconds.\n",
      "game_step 1 player 1 wins: 268\n",
      "player 2 wins: 211\n",
      "parallelized batch took 221.90628099441528 seconds.\n",
      "game_step 2 player 1 wins: 270\n",
      "player 2 wins: 201\n",
      "parallelized batch took 214.08640217781067 seconds.\n",
      "game_step 3 player 1 wins: 286\n",
      "player 2 wins: 194\n",
      "parallelized batch took 191.36020588874817 seconds.\n",
      "game_step 4 player 1 wins: 273\n",
      "player 2 wins: 198\n",
      "parallelized batch took 193.9707989692688 seconds.\n",
      "game_step 5 player 1 wins: 276\n",
      "player 2 wins: 199\n",
      "parallelized batch took 194.22054505348206 seconds.\n",
      "game_step 6 player 1 wins: 284\n",
      "player 2 wins: 189\n",
      "parallelized batch took 194.6609170436859 seconds.\n",
      "game_step 7 player 1 wins: 264\n",
      "player 2 wins: 223\n",
      "parallelized batch took 194.56504607200623 seconds.\n",
      "game_step 8 player 1 wins: 283\n",
      "player 2 wins: 195\n",
      "parallelized batch took 194.12697410583496 seconds.\n",
      "game_step 9 player 1 wins: 273\n",
      "player 2 wins: 208\n",
      "parallelized batch took 197.58982229232788 seconds.\n",
      "game_step 10 player 1 wins: 269\n",
      "player 2 wins: 197\n",
      "parallelized batch took 197.2189691066742 seconds.\n",
      "game_step 11 player 1 wins: 273\n",
      "player 2 wins: 209\n",
      "parallelized batch took 196.46710300445557 seconds.\n",
      "game_step 12 player 1 wins: 288\n",
      "player 2 wins: 188\n",
      "parallelized batch took 195.70005130767822 seconds.\n",
      "game_step 13 player 1 wins: 286\n",
      "player 2 wins: 183\n",
      "parallelized batch took 196.75440001487732 seconds.\n",
      "game_step 14 player 1 wins: 281\n",
      "player 2 wins: 191\n",
      "parallelized batch took 196.389545917511 seconds.\n",
      "game_step 15 player 1 wins: 281\n",
      "player 2 wins: 196\n",
      "parallelized batch took 196.86068511009216 seconds.\n",
      "game_step 16 player 1 wins: 276\n",
      "player 2 wins: 199\n",
      "parallelized batch took 219.4210240840912 seconds.\n",
      "game_step 17 player 1 wins: 274\n",
      "player 2 wins: 211\n",
      "parallelized batch took 344.6976308822632 seconds.\n",
      "game_step 18 player 1 wins: 275\n",
      "player 2 wins: 201\n",
      "parallelized batch took 335.7871820926666 seconds.\n",
      "game_step 19 player 1 wins: 275\n",
      "player 2 wins: 205\n",
      "parallelized batch took 210.71790218353271 seconds.\n",
      "game_step 0 player 1 wins: 271\n",
      "player 2 wins: 208\n",
      "parallelized batch took 199.76094603538513 seconds.\n",
      "game_step 1 player 1 wins: 275\n",
      "player 2 wins: 196\n",
      "parallelized batch took 196.0301330089569 seconds.\n",
      "game_step 2 player 1 wins: 272\n",
      "player 2 wins: 206\n",
      "parallelized batch took 196.8996090888977 seconds.\n",
      "game_step 3 player 1 wins: 283\n",
      "player 2 wins: 203\n",
      "parallelized batch took 195.22446393966675 seconds.\n",
      "game_step 4 player 1 wins: 301\n",
      "player 2 wins: 179\n",
      "parallelized batch took 196.08327913284302 seconds.\n",
      "game_step 5 player 1 wins: 263\n",
      "player 2 wins: 213\n",
      "parallelized batch took 195.16010212898254 seconds.\n",
      "game_step 6 player 1 wins: 292\n",
      "player 2 wins: 193\n",
      "parallelized batch took 197.57835698127747 seconds.\n",
      "game_step 7 player 1 wins: 268\n",
      "player 2 wins: 201\n",
      "parallelized batch took 195.2257161140442 seconds.\n",
      "game_step 8 player 1 wins: 278\n",
      "player 2 wins: 205\n",
      "parallelized batch took 195.6964991092682 seconds.\n",
      "game_step 9 player 1 wins: 295\n",
      "player 2 wins: 188\n",
      "parallelized batch took 197.34246611595154 seconds.\n",
      "game_step 10 player 1 wins: 267\n",
      "player 2 wins: 209\n",
      "parallelized batch took 198.76827597618103 seconds.\n",
      "game_step 11 player 1 wins: 284\n",
      "player 2 wins: 191\n",
      "parallelized batch took 196.79371690750122 seconds.\n",
      "game_step 12 player 1 wins: 272\n",
      "player 2 wins: 206\n",
      "parallelized batch took 196.49945306777954 seconds.\n",
      "game_step 13 player 1 wins: 259\n",
      "player 2 wins: 210\n",
      "parallelized batch took 200.53016662597656 seconds.\n",
      "game_step 14 player 1 wins: 280\n",
      "player 2 wins: 189\n",
      "parallelized batch took 203.1188941001892 seconds.\n",
      "game_step 15 player 1 wins: 266\n",
      "player 2 wins: 215\n",
      "parallelized batch took 197.13577890396118 seconds.\n",
      "game_step 16 player 1 wins: 274\n",
      "player 2 wins: 207\n",
      "parallelized batch took 198.55098009109497 seconds.\n",
      "game_step 17 player 1 wins: 272\n",
      "player 2 wins: 201\n",
      "parallelized batch took 198.13561296463013 seconds.\n",
      "game_step 18 player 1 wins: 270\n",
      "player 2 wins: 218\n",
      "parallelized batch took 197.23844194412231 seconds.\n",
      "game_step 19 player 1 wins: 275\n",
      "player 2 wins: 201\n",
      "parallelized batch took 196.92082977294922 seconds.\n",
      "game_step 0 player 1 wins: 291\n",
      "player 2 wins: 194\n",
      "parallelized batch took 197.08554601669312 seconds.\n",
      "game_step 1 player 1 wins: 289\n",
      "player 2 wins: 193\n",
      "parallelized batch took 196.74713397026062 seconds.\n",
      "game_step 2 player 1 wins: 281\n",
      "player 2 wins: 196\n",
      "parallelized batch took 198.61492609977722 seconds.\n",
      "game_step 3 player 1 wins: 262\n",
      "player 2 wins: 212\n",
      "parallelized batch took 198.84967184066772 seconds.\n",
      "game_step 4 player 1 wins: 287\n",
      "player 2 wins: 197\n",
      "parallelized batch took 196.70008277893066 seconds.\n",
      "game_step 5 player 1 wins: 279\n",
      "player 2 wins: 205\n",
      "parallelized batch took 200.3675503730774 seconds.\n",
      "game_step 6 player 1 wins: 266\n",
      "player 2 wins: 202\n",
      "parallelized batch took 200.36743307113647 seconds.\n",
      "game_step 7 player 1 wins: 279\n",
      "player 2 wins: 194\n",
      "parallelized batch took 198.35580229759216 seconds.\n",
      "game_step 8 player 1 wins: 273\n",
      "player 2 wins: 198\n",
      "parallelized batch took 198.99283409118652 seconds.\n",
      "game_step 9 player 1 wins: 269\n",
      "player 2 wins: 205\n",
      "parallelized batch took 198.93993496894836 seconds.\n",
      "game_step 10 player 1 wins: 265\n",
      "player 2 wins: 216\n",
      "parallelized batch took 198.73240494728088 seconds.\n",
      "game_step 11 player 1 wins: 305\n",
      "player 2 wins: 173\n",
      "parallelized batch took 199.92935824394226 seconds.\n",
      "game_step 12 player 1 wins: 293\n",
      "player 2 wins: 189\n",
      "parallelized batch took 199.24459290504456 seconds.\n",
      "game_step 13 player 1 wins: 293\n",
      "player 2 wins: 188\n",
      "parallelized batch took 197.99714994430542 seconds.\n",
      "game_step 14 player 1 wins: 277\n",
      "player 2 wins: 199\n",
      "parallelized batch took 201.62965774536133 seconds.\n",
      "game_step 15 player 1 wins: 267\n",
      "player 2 wins: 212\n",
      "parallelized batch took 200.33756685256958 seconds.\n",
      "game_step 16 player 1 wins: 273\n",
      "player 2 wins: 199\n",
      "parallelized batch took 201.61805963516235 seconds.\n",
      "game_step 17 player 1 wins: 275\n",
      "player 2 wins: 191\n",
      "parallelized batch took 201.770005941391 seconds.\n",
      "game_step 18 player 1 wins: 276\n",
      "player 2 wins: 205\n",
      "parallelized batch took 200.96901202201843 seconds.\n",
      "game_step 19 player 1 wins: 281\n",
      "player 2 wins: 189\n",
      "parallelized batch took 202.14541101455688 seconds.\n",
      "game_step 0 player 1 wins: 280\n",
      "player 2 wins: 197\n",
      "parallelized batch took 204.5785481929779 seconds.\n",
      "game_step 1 player 1 wins: 276\n",
      "player 2 wins: 206\n",
      "parallelized batch took 201.0417149066925 seconds.\n",
      "game_step 2 player 1 wins: 278\n",
      "player 2 wins: 196\n",
      "parallelized batch took 200.32930493354797 seconds.\n",
      "game_step 3 player 1 wins: 260\n",
      "player 2 wins: 220\n",
      "parallelized batch took 203.6497459411621 seconds.\n",
      "game_step 4 player 1 wins: 270\n",
      "player 2 wins: 205\n",
      "parallelized batch took 202.38674306869507 seconds.\n",
      "game_step 5 player 1 wins: 278\n",
      "player 2 wins: 202\n",
      "parallelized batch took 201.53505086898804 seconds.\n",
      "game_step 6 player 1 wins: 273\n",
      "player 2 wins: 199\n",
      "parallelized batch took 201.1207640171051 seconds.\n",
      "game_step 7 player 1 wins: 278\n",
      "player 2 wins: 196\n",
      "parallelized batch took 196.95413279533386 seconds.\n",
      "game_step 8 player 1 wins: 286\n",
      "player 2 wins: 190\n",
      "parallelized batch took 201.64974999427795 seconds.\n",
      "game_step 9 player 1 wins: 279\n",
      "player 2 wins: 191\n",
      "parallelized batch took 201.66679501533508 seconds.\n",
      "game_step 10 player 1 wins: 262\n",
      "player 2 wins: 208\n",
      "parallelized batch took 200.3271288871765 seconds.\n",
      "game_step 11 player 1 wins: 268\n",
      "player 2 wins: 208\n",
      "parallelized batch took 200.9149408340454 seconds.\n",
      "game_step 12 player 1 wins: 280\n",
      "player 2 wins: 199\n",
      "parallelized batch took 200.49698305130005 seconds.\n",
      "game_step 13 player 1 wins: 269\n",
      "player 2 wins: 219\n",
      "parallelized batch took 200.27084684371948 seconds.\n",
      "game_step 14 player 1 wins: 274\n",
      "player 2 wins: 200\n",
      "parallelized batch took 201.58929109573364 seconds.\n",
      "game_step 15 player 1 wins: 287\n",
      "player 2 wins: 187\n",
      "parallelized batch took 201.67353296279907 seconds.\n",
      "game_step 16 player 1 wins: 289\n",
      "player 2 wins: 186\n",
      "parallelized batch took 201.14994478225708 seconds.\n",
      "game_step 17 player 1 wins: 310\n",
      "player 2 wins: 170\n",
      "parallelized batch took 201.98652505874634 seconds.\n",
      "game_step 18 player 1 wins: 257\n",
      "player 2 wins: 219\n",
      "parallelized batch took 201.89437794685364 seconds.\n",
      "game_step 19 player 1 wins: 288\n",
      "player 2 wins: 185\n",
      "parallelized batch took 202.98700284957886 seconds.\n",
      "game_step 0 player 1 wins: 268\n",
      "player 2 wins: 207\n",
      "parallelized batch took 205.28503108024597 seconds.\n",
      "game_step 1 player 1 wins: 278\n",
      "player 2 wins: 198\n",
      "parallelized batch took 206.13703870773315 seconds.\n",
      "game_step 2 player 1 wins: 286\n",
      "player 2 wins: 183\n",
      "parallelized batch took 208.11652398109436 seconds.\n",
      "game_step 3 player 1 wins: 274\n",
      "player 2 wins: 202\n",
      "parallelized batch took 206.9056839942932 seconds.\n",
      "game_step 4 player 1 wins: 266\n",
      "player 2 wins: 212\n",
      "parallelized batch took 207.18407917022705 seconds.\n",
      "game_step 5 player 1 wins: 280\n",
      "player 2 wins: 202\n",
      "parallelized batch took 205.125009059906 seconds.\n",
      "game_step 6 player 1 wins: 294\n",
      "player 2 wins: 175\n",
      "parallelized batch took 206.33259773254395 seconds.\n",
      "game_step 7 player 1 wins: 286\n",
      "player 2 wins: 181\n",
      "parallelized batch took 205.81791305541992 seconds.\n",
      "game_step 8 player 1 wins: 278\n",
      "player 2 wins: 199\n",
      "parallelized batch took 208.02029299736023 seconds.\n",
      "game_step 9 player 1 wins: 283\n",
      "player 2 wins: 192\n",
      "parallelized batch took 208.13642287254333 seconds.\n",
      "game_step 10 player 1 wins: 282\n",
      "player 2 wins: 198\n",
      "parallelized batch took 206.51509809494019 seconds.\n",
      "game_step 11 player 1 wins: 271\n",
      "player 2 wins: 204\n",
      "parallelized batch took 208.49059796333313 seconds.\n",
      "game_step 12 player 1 wins: 264\n",
      "player 2 wins: 209\n",
      "parallelized batch took 207.2037389278412 seconds.\n",
      "game_step 13 player 1 wins: 279\n",
      "player 2 wins: 191\n",
      "parallelized batch took 206.70581483840942 seconds.\n",
      "game_step 14 player 1 wins: 272\n",
      "player 2 wins: 204\n",
      "parallelized batch took 208.77029180526733 seconds.\n",
      "game_step 15 player 1 wins: 277\n",
      "player 2 wins: 203\n",
      "parallelized batch took 208.68674612045288 seconds.\n",
      "game_step 16 player 1 wins: 281\n",
      "player 2 wins: 185\n",
      "parallelized batch took 208.66913509368896 seconds.\n",
      "game_step 17 player 1 wins: 291\n",
      "player 2 wins: 188\n",
      "parallelized batch took 206.6117560863495 seconds.\n",
      "game_step 18 player 1 wins: 277\n",
      "player 2 wins: 197\n",
      "parallelized batch took 207.81860518455505 seconds.\n",
      "game_step 19 player 1 wins: 273\n",
      "player 2 wins: 199\n",
      "parallelized batch took 206.40284991264343 seconds.\n",
      "game_step 0 player 1 wins: 286\n",
      "player 2 wins: 192\n",
      "parallelized batch took 206.47531414031982 seconds.\n",
      "game_step 1 player 1 wins: 278\n",
      "player 2 wins: 203\n",
      "parallelized batch took 207.39742183685303 seconds.\n",
      "game_step 2 player 1 wins: 286\n",
      "player 2 wins: 192\n",
      "parallelized batch took 207.0042209625244 seconds.\n",
      "game_step 3 player 1 wins: 278\n",
      "player 2 wins: 196\n",
      "parallelized batch took 207.2990837097168 seconds.\n",
      "game_step 4 player 1 wins: 258\n",
      "player 2 wins: 215\n",
      "parallelized batch took 209.9775333404541 seconds.\n",
      "game_step 5 player 1 wins: 290\n",
      "player 2 wins: 186\n",
      "parallelized batch took 207.13426089286804 seconds.\n",
      "game_step 6 player 1 wins: 272\n",
      "player 2 wins: 208\n",
      "parallelized batch took 208.957927942276 seconds.\n",
      "game_step 7 player 1 wins: 282\n",
      "player 2 wins: 195\n",
      "parallelized batch took 209.61304998397827 seconds.\n",
      "game_step 8 player 1 wins: 271\n",
      "player 2 wins: 196\n",
      "parallelized batch took 209.02311205863953 seconds.\n",
      "game_step 9 player 1 wins: 278\n",
      "player 2 wins: 199\n",
      "parallelized batch took 209.941908121109 seconds.\n",
      "game_step 10 player 1 wins: 276\n",
      "player 2 wins: 198\n",
      "parallelized batch took 208.61165499687195 seconds.\n",
      "game_step 11 player 1 wins: 285\n",
      "player 2 wins: 189\n",
      "parallelized batch took 207.83149218559265 seconds.\n",
      "game_step 12 player 1 wins: 270\n",
      "player 2 wins: 209\n",
      "parallelized batch took 207.77140188217163 seconds.\n",
      "game_step 13 player 1 wins: 257\n",
      "player 2 wins: 213\n",
      "parallelized batch took 211.7501561641693 seconds.\n",
      "game_step 14 player 1 wins: 294\n",
      "player 2 wins: 176\n",
      "parallelized batch took 212.45312190055847 seconds.\n",
      "game_step 15 player 1 wins: 272\n",
      "player 2 wins: 205\n",
      "parallelized batch took 209.87523698806763 seconds.\n",
      "game_step 16 player 1 wins: 290\n",
      "player 2 wins: 191\n",
      "parallelized batch took 211.5503568649292 seconds.\n",
      "game_step 17 player 1 wins: 276\n",
      "player 2 wins: 194\n",
      "parallelized batch took 211.58246612548828 seconds.\n",
      "game_step 18 player 1 wins: 283\n",
      "player 2 wins: 193\n",
      "parallelized batch took 209.06755208969116 seconds.\n",
      "game_step 19 player 1 wins: 293\n",
      "player 2 wins: 185\n",
      "parallelized batch took 209.27316093444824 seconds.\n",
      "game_step 0 player 1 wins: 284\n",
      "player 2 wins: 197\n",
      "parallelized batch took 209.26784706115723 seconds.\n",
      "game_step 1 player 1 wins: 296\n",
      "player 2 wins: 181\n",
      "parallelized batch took 213.49594378471375 seconds.\n",
      "game_step 2 player 1 wins: 283\n",
      "player 2 wins: 190\n",
      "parallelized batch took 211.87420392036438 seconds.\n",
      "game_step 3 player 1 wins: 265\n",
      "player 2 wins: 211\n",
      "parallelized batch took 211.19371914863586 seconds.\n",
      "game_step 4 player 1 wins: 293\n",
      "player 2 wins: 189\n",
      "parallelized batch took 212.2200038433075 seconds.\n",
      "game_step 5 player 1 wins: 257\n",
      "player 2 wins: 230\n",
      "parallelized batch took 209.42067313194275 seconds.\n",
      "game_step 6 player 1 wins: 278\n",
      "player 2 wins: 198\n",
      "parallelized batch took 211.6853449344635 seconds.\n",
      "game_step 7 player 1 wins: 282\n",
      "player 2 wins: 196\n",
      "parallelized batch took 210.48545694351196 seconds.\n",
      "game_step 8 player 1 wins: 281\n",
      "player 2 wins: 195\n",
      "parallelized batch took 211.10183882713318 seconds.\n",
      "game_step 9 player 1 wins: 276\n",
      "player 2 wins: 201\n",
      "parallelized batch took 212.91951417922974 seconds.\n",
      "game_step 10 player 1 wins: 277\n",
      "player 2 wins: 190\n",
      "parallelized batch took 211.2962498664856 seconds.\n",
      "game_step 11 player 1 wins: 282\n",
      "player 2 wins: 193\n",
      "parallelized batch took 213.7854709625244 seconds.\n",
      "game_step 12 player 1 wins: 274\n",
      "player 2 wins: 205\n",
      "parallelized batch took 214.9735140800476 seconds.\n",
      "game_step 13 player 1 wins: 285\n",
      "player 2 wins: 191\n",
      "parallelized batch took 212.87469911575317 seconds.\n",
      "game_step 14 player 1 wins: 287\n",
      "player 2 wins: 192\n",
      "parallelized batch took 211.71319794654846 seconds.\n",
      "game_step 15 player 1 wins: 281\n",
      "player 2 wins: 193\n",
      "parallelized batch took 212.50595211982727 seconds.\n",
      "game_step 16 player 1 wins: 292\n",
      "player 2 wins: 183\n",
      "parallelized batch took 212.90683603286743 seconds.\n",
      "game_step 17 player 1 wins: 290\n",
      "player 2 wins: 187\n",
      "parallelized batch took 210.4268548488617 seconds.\n",
      "game_step 18 player 1 wins: 282\n",
      "player 2 wins: 191\n",
      "parallelized batch took 215.01738595962524 seconds.\n",
      "game_step 19 player 1 wins: 291\n",
      "player 2 wins: 183\n",
      "parallelized batch took 212.33959913253784 seconds.\n",
      "game_step 0 player 1 wins: 277\n",
      "player 2 wins: 194\n",
      "parallelized batch took 212.58282327651978 seconds.\n",
      "game_step 1 player 1 wins: 269\n",
      "player 2 wins: 208\n",
      "parallelized batch took 216.51878380775452 seconds.\n",
      "game_step 2 player 1 wins: 288\n",
      "player 2 wins: 196\n",
      "parallelized batch took 210.46034932136536 seconds.\n",
      "game_step 3 player 1 wins: 290\n",
      "player 2 wins: 188\n",
      "parallelized batch took 212.99496030807495 seconds.\n",
      "game_step 4 player 1 wins: 281\n",
      "player 2 wins: 187\n",
      "parallelized batch took 252.36074781417847 seconds.\n",
      "game_step 5 player 1 wins: 274\n",
      "player 2 wins: 203\n",
      "parallelized batch took 248.558580160141 seconds.\n",
      "game_step 6 player 1 wins: 306\n",
      "player 2 wins: 177\n",
      "parallelized batch took 217.4505488872528 seconds.\n",
      "game_step 7 player 1 wins: 282\n",
      "player 2 wins: 195\n",
      "parallelized batch took 214.4329001903534 seconds.\n",
      "game_step 8 player 1 wins: 275\n",
      "player 2 wins: 203\n",
      "parallelized batch took 214.89436388015747 seconds.\n",
      "game_step 9 player 1 wins: 272\n",
      "player 2 wins: 205\n",
      "parallelized batch took 213.56093001365662 seconds.\n",
      "game_step 10 player 1 wins: 278\n",
      "player 2 wins: 204\n",
      "parallelized batch took 214.3418161869049 seconds.\n",
      "game_step 11 player 1 wins: 283\n",
      "player 2 wins: 196\n",
      "parallelized batch took 212.15003085136414 seconds.\n",
      "game_step 12 player 1 wins: 279\n",
      "player 2 wins: 198\n",
      "parallelized batch took 214.49632596969604 seconds.\n",
      "game_step 13 player 1 wins: 277\n",
      "player 2 wins: 207\n",
      "parallelized batch took 214.33318614959717 seconds.\n",
      "game_step 14 player 1 wins: 279\n",
      "player 2 wins: 202\n",
      "parallelized batch took 213.86790418624878 seconds.\n",
      "game_step 15 player 1 wins: 275\n",
      "player 2 wins: 191\n",
      "parallelized batch took 215.37844610214233 seconds.\n",
      "game_step 16 player 1 wins: 278\n",
      "player 2 wins: 199\n",
      "parallelized batch took 213.11510705947876 seconds.\n",
      "game_step 17 player 1 wins: 283\n",
      "player 2 wins: 189\n",
      "parallelized batch took 213.5303089618683 seconds.\n",
      "game_step 18 player 1 wins: 279\n",
      "player 2 wins: 201\n",
      "parallelized batch took 215.54336428642273 seconds.\n",
      "game_step 19 player 1 wins: 283\n",
      "player 2 wins: 195\n",
      "parallelized batch took 215.82555508613586 seconds.\n",
      "game_step 0 player 1 wins: 301\n",
      "player 2 wins: 179\n",
      "parallelized batch took 217.10398697853088 seconds.\n",
      "game_step 1 player 1 wins: 279\n",
      "player 2 wins: 181\n",
      "parallelized batch took 216.04961109161377 seconds.\n",
      "game_step 2 player 1 wins: 269\n",
      "player 2 wins: 209\n",
      "parallelized batch took 216.58361172676086 seconds.\n",
      "game_step 3 player 1 wins: 253\n",
      "player 2 wins: 220\n",
      "parallelized batch took 217.79630994796753 seconds.\n",
      "game_step 4 player 1 wins: 274\n",
      "player 2 wins: 202\n",
      "parallelized batch took 215.27747988700867 seconds.\n",
      "game_step 5 player 1 wins: 266\n",
      "player 2 wins: 208\n",
      "parallelized batch took 216.13779878616333 seconds.\n",
      "game_step 6 player 1 wins: 272\n",
      "player 2 wins: 196\n",
      "parallelized batch took 216.16045427322388 seconds.\n",
      "game_step 7 player 1 wins: 281\n",
      "player 2 wins: 194\n",
      "parallelized batch took 215.9874300956726 seconds.\n",
      "game_step 8 player 1 wins: 249\n",
      "player 2 wins: 224\n",
      "parallelized batch took 214.9595398902893 seconds.\n",
      "game_step 9 player 1 wins: 282\n",
      "player 2 wins: 194\n",
      "parallelized batch took 217.4600601196289 seconds.\n",
      "game_step 10 player 1 wins: 273\n",
      "player 2 wins: 208\n",
      "parallelized batch took 213.65905714035034 seconds.\n",
      "game_step 11 player 1 wins: 295\n",
      "player 2 wins: 186\n",
      "parallelized batch took 216.58692693710327 seconds.\n",
      "game_step 12 player 1 wins: 291\n",
      "player 2 wins: 185\n",
      "parallelized batch took 222.19637489318848 seconds.\n",
      "game_step 13 player 1 wins: 271\n",
      "player 2 wins: 192\n",
      "parallelized batch took 234.5436680316925 seconds.\n",
      "game_step 14 player 1 wins: 284\n",
      "player 2 wins: 191\n",
      "parallelized batch took 256.49422693252563 seconds.\n",
      "game_step 15 player 1 wins: 280\n",
      "player 2 wins: 185\n",
      "parallelized batch took 229.78254008293152 seconds.\n",
      "game_step 16 player 1 wins: 276\n",
      "player 2 wins: 192\n",
      "parallelized batch took 239.4097089767456 seconds.\n",
      "game_step 17 player 1 wins: 279\n",
      "player 2 wins: 191\n",
      "parallelized batch took 247.61599731445312 seconds.\n",
      "game_step 18 player 1 wins: 284\n",
      "player 2 wins: 195\n",
      "parallelized batch took 243.83592796325684 seconds.\n",
      "game_step 19 player 1 wins: 276\n",
      "player 2 wins: 208\n",
      "parallelized batch took 243.59342122077942 seconds.\n",
      "game_step 0 player 1 wins: 270\n",
      "player 2 wins: 209\n",
      "parallelized batch took 249.39173293113708 seconds.\n",
      "game_step 1 player 1 wins: 264\n",
      "player 2 wins: 206\n",
      "parallelized batch took 251.355553150177 seconds.\n",
      "game_step 2 player 1 wins: 281\n",
      "player 2 wins: 199\n",
      "parallelized batch took 251.8945710659027 seconds.\n",
      "game_step 3 player 1 wins: 289\n",
      "player 2 wins: 183\n",
      "parallelized batch took 249.28221607208252 seconds.\n",
      "game_step 4 player 1 wins: 269\n",
      "player 2 wins: 210\n",
      "parallelized batch took 251.5611219406128 seconds.\n",
      "game_step 5 player 1 wins: 290\n",
      "player 2 wins: 190\n",
      "parallelized batch took 252.89586687088013 seconds.\n",
      "game_step 6 player 1 wins: 281\n",
      "player 2 wins: 190\n",
      "parallelized batch took 260.5613577365875 seconds.\n",
      "game_step 7 player 1 wins: 286\n",
      "player 2 wins: 191\n",
      "parallelized batch took 264.4354622364044 seconds.\n",
      "game_step 8 player 1 wins: 288\n",
      "player 2 wins: 191\n",
      "parallelized batch took 262.6272130012512 seconds.\n",
      "game_step 9 player 1 wins: 293\n",
      "player 2 wins: 190\n",
      "parallelized batch took 246.59366297721863 seconds.\n",
      "game_step 10 player 1 wins: 271\n",
      "player 2 wins: 207\n",
      "parallelized batch took 254.53933596611023 seconds.\n",
      "game_step 11 player 1 wins: 283\n",
      "player 2 wins: 190\n",
      "parallelized batch took 256.00051403045654 seconds.\n",
      "game_step 12 player 1 wins: 278\n",
      "player 2 wins: 197\n",
      "parallelized batch took 261.2599091529846 seconds.\n",
      "game_step 13 player 1 wins: 292\n",
      "player 2 wins: 183\n",
      "parallelized batch took 266.667631149292 seconds.\n",
      "game_step 14 player 1 wins: 271\n",
      "player 2 wins: 200\n",
      "parallelized batch took 260.64631509780884 seconds.\n",
      "game_step 15 player 1 wins: 283\n",
      "player 2 wins: 194\n",
      "parallelized batch took 270.4728138446808 seconds.\n",
      "game_step 16 player 1 wins: 270\n",
      "player 2 wins: 200\n",
      "parallelized batch took 256.7807250022888 seconds.\n",
      "game_step 17 player 1 wins: 265\n",
      "player 2 wins: 210\n",
      "parallelized batch took 247.86963176727295 seconds.\n",
      "game_step 18 player 1 wins: 293\n",
      "player 2 wins: 180\n",
      "parallelized batch took 238.30346989631653 seconds.\n",
      "game_step 19 player 1 wins: 277\n",
      "player 2 wins: 201\n",
      "parallelized batch took 324.5638782978058 seconds.\n",
      "game_step 0 "
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## Change number of games to simulate here\n",
    "n_games = 10000\n",
    "##################################################\n",
    "\n",
    "time0 = time.time()\n",
    "for x in range(20):\n",
    "    game_t = big_sim_parallel(agent1, agent2, n_steps=20, qtables_to_update=[qtable1])\n",
    "print(time.time()-time0, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.593888888888888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59738/60/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444204"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qtable1.q_dict)\n",
    "#agent1.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = [2,2,2,2,2]\n",
    "\n",
    "with open('test.pickle', 'wb') as file:\n",
    "    pickle.dump(x, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('test.pickle', 'rb') as file:\n",
    "    y = pickle.load(file)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = []\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    ns.append(v[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5444199,       3,       1,       0,       0,       0,       0,\n",
       "              0,       0,       1]),\n",
       " array([1.000000e+00, 1.797250e+04, 3.594400e+04, 5.391550e+04,\n",
       "        7.188700e+04, 8.985850e+04, 1.078300e+05, 1.258015e+05,\n",
       "        1.437730e+05, 1.617445e+05, 1.797160e+05]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(np.array(ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153873\n",
      "67959\n",
      "46602\n",
      "33784\n",
      "12230\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in ns if x != 1]))\n",
    "print(len([x for x in ns if x > 2]))\n",
    "print(len([x for x in ns if x > 3]))\n",
    "print(len([x for x in ns if x > 4]))\n",
    "print(len([x for x in ns if x > 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120020100111022211102201112221201221] (0.5, 1)\n",
      "[020011100211022012102201112221201221] (0.5, 1)\n",
      "[002011202211010012102201112201201221] (0.5, 1)\n",
      "[002011202211010012210201010201122221] (0.5, 1)\n",
      "[020011100211022012210001010201122221] (0.5, 1)\n",
      "[110020112001200220022012002010111221] (0.5, 1)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    print(k,v)\n",
    "    i += 1\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([94371,     6,     0,     0,     0,     0,     0,     0,     0,\n",
       "            1]),\n",
       " array([2.00000e+00, 1.15380e+04, 2.30740e+04, 3.46100e+04, 4.61460e+04,\n",
       "        5.76820e+04, 6.92180e+04, 8.07540e+04, 9.22900e+04, 1.03826e+05,\n",
       "        1.15362e+05]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram([x for x in ns if x != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
