{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from helper_func import *\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import keras\n",
    "from multiprocessing import Pool\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from collections import deque\n",
    "from keras.activations import relu, linear\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv3d_2/Relu:0\", shape=(4, 26, 26, 26, 2), dtype=float32)\n",
      "(4, 26, 26, 26, 2)\n"
     ]
    }
   ],
   "source": [
    "# The inputs are 28x28x28 volumes with a single channel, and the  \n",
    "# batch size is 4  \n",
    "input_shape =(4, 28, 28, 28, 1)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv3D(2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class pentago:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state = None):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        #print('initializing')\n",
    "        \n",
    "        if state == None:\n",
    "            self.state = state = np.zeros((6,6), dtype=np.int)\n",
    "        self.history = []\n",
    "        self.winner = None\n",
    "        self.gameover = False\n",
    "        self.player_turn = 1\n",
    "    \n",
    "    def current_board_state(self):\n",
    "        # need to return a copy or bad stuff happens\n",
    "        return copy.copy(self.state)\n",
    "    \n",
    "    def game_history(self, player, move, cuad, rotatation):\n",
    "        self.history.append((boardstate_to_ideal_key(self.state), ideal_state(self.state), player, move, cuad, rotatation))\n",
    "        #return self.history\n",
    "\n",
    "    def find_winner(self, board_state):\n",
    "        player1_win = False\n",
    "        player_min1_win = False\n",
    "        diagonal1 = board_state.diagonal()\n",
    "        diagonal2 = np.fliplr(board_state).diagonal()\n",
    "        winning_slices =  np.vstack([board_state[1:,:].T, board_state[:-1,:].T, # all columns\n",
    "                              board_state[:,1:], board_state[:,:-1], # all rows\n",
    "                              diagonal1[1:], diagonal1[:-1], # diagonal 1\n",
    "                              diagonal2[1:],diagonal2[1:], # diagonal 2\n",
    "                              board_state.diagonal(offset=1), board_state.diagonal(offset=-1), # diagonal offsets \n",
    "                              np.fliplr(board_state).diagonal(offset=1), np.fliplr(board_state).diagonal(offset=-1)] ) # diagonal offsets\n",
    "        sums = np.dot(winning_slices, np.array([1,1,1,1,1]))\n",
    "        if 5 in sums: player1_win = True\n",
    "        if -5 in sums: player_min1_win = True\n",
    "        if player1_win == True or player_min1_win == True:\n",
    "           # print(\"Player 1 winner?\", player1_win, \"Player -1 winner?\", player_min1_win)\n",
    "            self.gameover = True\n",
    "            if player1_win == True:\n",
    "                self.winner = 1\n",
    "            elif player_min1_win ==True:\n",
    "                self.winner = -1\n",
    "            self.history.append(self.winner)\n",
    "        return \"Win\"\n",
    "\n",
    "    def check_gameover(self):\n",
    "        if not 0 in self.state:\n",
    "              self.gameover = True\n",
    "              #print(\"The game board is full!\")\n",
    "        \n",
    "    def full_move(self, move, cuad, direction, player, dtype=np.int):\n",
    "        if player != self.player_turn:\n",
    "            print( \"error, wrong player turn. No move taken.\")\n",
    "            return 'Error, wrong player turn.'\n",
    "        self.state = fullmove(self.state,move, cuad, direction, player)\n",
    "\n",
    "\n",
    "        self.game_history(move, player, cuad, direction)\n",
    "        self.find_winner(self.state) #return in find_winner if a winner is found\n",
    "        self.check_gameover() #return in check_gameover\n",
    "        if player == 1:\n",
    "            self.player_turn = -1\n",
    "        else:\n",
    "            self.player_turn = 1\n",
    "        #print('Successful Move')\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class q_table:\n",
    "\n",
    "    def __init__(self,length=0, games_played=0):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        self.time = datetime.now()\n",
    "        self.length = length\n",
    "        self.q_dict = {}\n",
    "        self.games_played = games_played\n",
    "\n",
    "  #def time(self):\n",
    "    #self.time = time\n",
    "\n",
    "    def length(self):\n",
    "        self.length += 1\n",
    "    #self.length = length  \n",
    "    \n",
    "    def get_q_value(self, boardstate):\n",
    "        return self.q_dict.get(boardstate, (0, 0))\n",
    "    \n",
    "    def update_q_value(self, boardstate, new_val, update_function = None):\n",
    "        q_val, n = self.get_q_value(boardstate) \n",
    "        if update_function:\n",
    "            #print('using custom function')\n",
    "            self.q_dict[boardstate] = update_function(q_val, n, new_val)\n",
    "        else:\n",
    "            self.q_dict[boardstate] = [new_val, n+1]\n",
    "        return self.q_dict[boardstate]\n",
    "    \n",
    "    def update_post_game(self, history, update_fn):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "\n",
    "    def update_post_game2(self, history, update_fn, decay_reward = .9):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "            winner *= decay_reward\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "\n",
    "    def __init__(self,list_density=None,lr=0.02):\n",
    "        self.model = self.build_model(list_density,lr)\n",
    "\n",
    "    def build_model(self,den,lr):\n",
    "        model = Sequential()\n",
    "        #model.add(tf.keras.Input(shape=(6,6,2,)))\n",
    "        #for layer in den:\n",
    "        model.add(Conv2D(\n",
    "                            filters = 64, \n",
    "                            kernel_size = (3,3),\n",
    "                            strides = (3,3),\n",
    "                            padding = 'valid',\n",
    "                            activation = relu,\n",
    "                            input_shape = (6,6,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation = relu))\n",
    "        model.add(Dense(128, activation = relu))\n",
    "\n",
    "\n",
    "        model.add(Dense(1, activation=linear))\n",
    "        \n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def update_model(self,states_batch, q_batch, epochs = 1):\n",
    "\n",
    "        self.model.fit(states_batch,q_batch,epochs = epochs, verbose=1)\n",
    "\n",
    "    def predict_model(self, state_batch):\n",
    "        return self.model.predict_on_batch(state_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, 2)\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.08607867]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,1,1,-1,1,1],[0,0,0,0,0,0],[0,0,0,1,1,0],[0,0,1,-1,-1,0],[0,0,1,-1,1,0],[-1,-1,1,0,0,0]])\n",
    "x = boardstate_to_cnn_input(x)\n",
    "print(x.shape)\n",
    "out= c.model.predict(x.reshape(1,6,6,2))\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.99995\n",
      "1000 0.9511806740132733\n",
      "2000 0.904789914112052\n",
      "3000 0.8606617134311852\n",
      "4000 0.8186857229650423\n",
      "5000 0.7787569756237134\n",
      "6000 0.7407756237474893\n",
      "7000 0.7046466894232127\n",
      "8000 0.6702798269781429\n",
      "9000 0.6375890970574211\n",
      "10000 0.6064927517201779\n",
      "11000 0.5769130300168653\n",
      "12000 0.5487759635366593\n",
      "13000 0.5220111914386566\n",
      "14000 0.49655178450431714\n",
      "15000 0.47233407777119996\n",
      "16000 0.44929751132941503\n",
      "17000 0.4273844788827431\n",
      "18000 0.40654018369568434\n",
      "19000 0.3867125015662202\n",
      "20000 0.367851850481642\n",
      "21000 0.34991106663148985\n",
      "22000 0.332845286467567\n",
      "23000 0.31661183451608793\n",
      "24000 0.30117011666142574\n",
      "25000 0.286481518634604\n",
      "26000 0.2725093094526817\n",
      "27000 0.259218549567572\n",
      "28000 0.246576003494601\n",
      "29000 0.23455005670232934\n",
      "30000 0.22311063655580088\n",
      "31000 0.21222913711553318\n",
      "32000 0.20187834760418902\n",
      "33000 0.19203238436205666\n",
      "34000 0.18266662612118353\n",
      "35000 0.17375765243629993\n",
      "36000 0.16528318511857973\n",
      "37000 0.15722203252577774\n",
      "38000 0.14955403656943475\n",
      "39000 0.14226002230663626\n",
      "40000 0.1353217499902697\n",
      "41000 0.12872186945787317\n",
      "42000 0.12244387674502544\n",
      "43000 0.11647207281477254\n",
      "44000 0.11079152429989349\n",
      "45000 0.10538802615983875\n",
      "46000 0.10024806615895202\n",
      "47000 0.09535879107715316\n",
      "48000 0.09070797456858609\n",
      "49000 0.08628398658785626\n",
      "50000 0.08207576430740496\n",
      "51000 0.07807278445329505\n",
      "52000 0.07426503699022786\n",
      "53000 0.07064300008999028\n",
      "54000 0.06719761632073298\n",
      "55000 0.06392026999754027\n",
      "56000 0.06080276563765279\n",
      "57000 0.05783730746646704\n",
      "58000 0.05501647992306336\n",
      "59000 0.05233322911651293\n",
      "60000 0.04978084518659529\n",
      "61000 0.04735294552481254\n",
      "62000 0.0450434588137458\n",
      "63000 0.04284660984484019\n",
      "64000 0.04075690507665269\n",
      "65000 0.03876911889745041\n",
      "66000 0.03687828055780553\n",
      "67000 0.03507966174051114\n",
      "68000 0.03336876473673393\n",
      "69000 0.031741311198836865\n",
      "70000 0.03019323144174688\n",
      "71000 0.0287206542661129\n",
      "72000 0.027319897277807384\n",
      "73000 0.025987457679562245\n",
      "74000 0.02472000351171302\n",
      "75000 0.0235143653201477\n",
      "76000 0.02236752823062398\n",
      "77000 0.021276624409636378\n",
      "78000 0.0202389258929799\n",
      "79000 0.019251837764077552\n",
      "80000 0.018312891665012748\n",
      "81000 0.017419739624040163\n",
      "82000 0.016570148184139464\n",
      "83000 0.015761992817930556\n",
      "84000 0.014993252614982331\n",
      "85000 0.014262005228231584\n",
      "86000 0.01356642206687351\n",
      "87000 0.012904763723703517\n",
      "88000 0.012275375625475645\n",
      "89000 0.011676683895400804\n",
      "90000 0.011107191417438142\n",
      "91000 0.010565474092537885\n",
      "92000 0.010050177277473855\n",
      "93000 0.009560012397360337\n",
      "94000 0.009093753723382633\n",
      "95000 0.00865023530768317\n",
      "96000 0.008228348067738887\n",
      "97000 0.007827037012938349\n",
      "98000 0.007445298606423867\n",
      "99000 0.007082178255601129\n"
     ]
    }
   ],
   "source": [
    "v = 1\n",
    "for x in range(100000):\n",
    "    v*=.99995\n",
    "    if x%1000 == 0:\n",
    "        print(x, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_agent:\n",
    "    \n",
    "    def __init__(self, nn, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.nn = nn\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            #print(possible_boardstate)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            cnn_input = boardstate_to_cnn_input(possible_boardstate)\n",
    "            #print(cnn_input)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append((cnn_input, move))\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        t0 = time.time()\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        print(\"get possible boardstates\", time.time()-t0)\n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0][1] # capture the move needed\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "            t0 = time.time()\n",
    "            nn_input_boardstate = []\n",
    "            move = []\n",
    "            for k,v in next_possible_boardstates.items():\n",
    "                nn_input_boardstate.append(v[0][0])\n",
    "                #print(v[0][0].shape)\n",
    "                move.append(v[0][1])\n",
    "          \n",
    "            #print(nn_input_boardstate, type(nn_input_boardstate))\n",
    "            #nn_input_batch = np.concatenate(nn_input_boardstate)\n",
    "            nn_input_batch = np.array(nn_input_boardstate)\n",
    "            #print(nn_input_batch, type(nn_input_batch))\n",
    "            #print(nn_input_boardstate)\n",
    "            q_values = self.nn.predict_model(nn_input_batch)\n",
    "            print(\"get q_values boardstates\", time.time()-t0)\n",
    "\n",
    "            #print(q_values, type(q_values))\n",
    "            #print(\"----\")\n",
    "            q_values *= self.player\n",
    "            #q_values_list = [x*self.player for x in list(q_values)]\n",
    "            #print(q_values_list)\n",
    "            # get random index of a max value\n",
    "            #print('length fo q_values_list = ', len(q_values_list))\n",
    "            max_q = np.argmax(q_values)\n",
    "            #index_of_all_max = [i for i in range(len(q_values)) if q_values[i] == max_q]\n",
    "            #random_max_q_index = random.choice(index_of_all_max)\n",
    "            #print(\"MAX VALUE:\", q_values_list[random_max_q_index])\n",
    "            mv_to_take = move[max_q]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_agent_multigame:\n",
    "    \n",
    "    def __init__(self, nn, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.nn = nn\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            #print(possible_boardstate)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            cnn_input = boardstate_to_cnn_input(possible_boardstate)\n",
    "            #print(cnn_input)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append((cnn_input, move))\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        \n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0][1] # capture the move needed\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "            nn_input_boardstate = []\n",
    "            move = []\n",
    "            for k,v in next_possible_boardstates.items():\n",
    "                nn_input_boardstate.append(v[0][0])\n",
    "                #print(v[0][0].shape)\n",
    "                move.append(v[0][1])\n",
    "            \n",
    "            \n",
    "            #print(nn_input_boardstate, type(nn_input_boardstate))\n",
    "            #nn_input_batch = np.concatenate(nn_input_boardstate)\n",
    "            nn_input_batch = np.array(nn_input_boardstate)\n",
    "            #print(nn_input_batch, type(nn_input_batch))\n",
    "            #print(nn_input_boardstate)\n",
    "            q_values = self.nn.predict_model(nn_input_batch)\n",
    "            #print(q_values, type(q_values))\n",
    "            #print(\"----\")\n",
    "            q_values *= self.player\n",
    "            #q_values_list = [x*self.player for x in list(q_values)]\n",
    "            #print(q_values_list)\n",
    "            # get random index of a max value\n",
    "            #print('length fo q_values_list = ', len(q_values_list))\n",
    "            max_q = np.argmax(q_values)\n",
    "            #index_of_all_max = [i for i in range(len(q_values)) if q_values[i] == max_q]\n",
    "            #random_max_q_index = random.choice(index_of_all_max)\n",
    "            #print(\"MAX VALUE:\", q_values_list[random_max_q_index])\n",
    "            mv_to_take = move[max_q]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4,2,1,4])\n",
    "np.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn_1 = cnn_model([72,144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = 0, epsilon_decay = .999, epsilon = 0)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_1, epsilon_min = 0, epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get possible boardstates 0.13018488883972168\n",
      "get q_values boardstates 0.0014069080352783203\n",
      "get possible boardstates 0.10476422309875488\n",
      "get q_values boardstates 0.0016062259674072266\n",
      "get possible boardstates 0.10352015495300293\n",
      "get q_values boardstates 0.0027120113372802734\n",
      "get possible boardstates 0.1006619930267334\n",
      "get q_values boardstates 0.0031909942626953125\n",
      "get possible boardstates 0.09388208389282227\n",
      "get q_values boardstates 0.004374980926513672\n",
      "get possible boardstates 0.11308002471923828\n",
      "get q_values boardstates 0.003487110137939453\n",
      "get possible boardstates 0.11890411376953125\n",
      "get q_values boardstates 0.003551006317138672\n",
      "get possible boardstates 0.0913999080657959\n",
      "get q_values boardstates 0.004165172576904297\n",
      "get possible boardstates 0.08824563026428223\n",
      "get q_values boardstates 0.0031189918518066406\n",
      "get possible boardstates 0.08778977394104004\n",
      "get q_values boardstates 0.003165006637573242\n",
      "get possible boardstates 0.07796311378479004\n",
      "get q_values boardstates 0.0028090476989746094\n",
      "get possible boardstates 0.07299017906188965\n",
      "get q_values boardstates 0.0028731822967529297\n",
      "get possible boardstates 0.08006405830383301\n",
      "get q_values boardstates 0.0039730072021484375\n",
      "get possible boardstates 0.07940196990966797\n",
      "get q_values boardstates 0.002719879150390625\n",
      "get possible boardstates 0.07558703422546387\n",
      "get q_values boardstates 0.0026428699493408203\n",
      "get possible boardstates 0.0674898624420166\n",
      "get q_values boardstates 0.002427816390991211\n",
      "get possible boardstates 0.06100797653198242\n",
      "get q_values boardstates 0.002466917037963867\n",
      "get possible boardstates 0.057515621185302734\n",
      "get q_values boardstates 0.002435922622680664\n",
      "get possible boardstates 0.05442690849304199\n",
      "get q_values boardstates 0.0024938583374023438\n",
      "get possible boardstates 0.05570197105407715\n",
      "get q_values boardstates 0.0023517608642578125\n",
      "get possible boardstates 0.048135995864868164\n",
      "get q_values boardstates 0.0024161338806152344\n",
      "get possible boardstates 0.04708504676818848\n",
      "get q_values boardstates 0.0031731128692626953\n",
      "get possible boardstates 0.04176592826843262\n",
      "get q_values boardstates 0.002410888671875\n",
      "get possible boardstates 0.03962278366088867\n",
      "get q_values boardstates 0.0023491382598876953\n",
      "get possible boardstates 0.04057598114013672\n",
      "get q_values boardstates 0.0018460750579833984\n",
      "get possible boardstates 0.03402304649353027\n",
      "get q_values boardstates 0.0022759437561035156\n",
      "2.072113037109375\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "g = pentago()\n",
    "while g.gameover == False:\n",
    "    agent1.make_move(g)\n",
    "    if g.gameover == True: break\n",
    "    agent2.make_move(g)\n",
    "    #break\n",
    "\n",
    "#bs = agent1.get_possible_next_boardstates(g.current_board_state())\n",
    "    \n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> 32 32\n",
      "(32, 6, 6, 2) (32,)\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.1705\n"
     ]
    }
   ],
   "source": [
    "g.history[:2]\n",
    "def reward_func(history, decay_factor = .9):\n",
    "    winner = history[-1]\n",
    "    nn_inputs = []\n",
    "    rewards = []\n",
    "    for boardposition in history[-2::-1]:\n",
    "        nn_inputs.append(boardposition[1])\n",
    "        rewards.append(winner)\n",
    "        winner *= decay_factor\n",
    "    return nn_inputs, rewards\n",
    "a = reward_func(g.history)\n",
    "print(type(a[0]), type(a[1]), len(a[0]), len(a[1]))\n",
    "x = np.array([boardstate_to_cnn_input(bs) for bs in a[0]])\n",
    "Y = np.array(a[1])\n",
    "print(x.shape, Y.shape)\n",
    "\n",
    "nn_1.update_model(x,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def little_sim(agents):\n",
    "    agent1, agent2 = agents\n",
    "    g = pentago()\n",
    "    while g.gameover == False:\n",
    "        agent1.make_move(g)\n",
    "        if g.gameover ==True: break\n",
    "        agent2.make_move(g)\n",
    "    #print('gameover.')\n",
    "    return g\n",
    "#little_sim((agent1,agent2))\n",
    "#if __name__ == '__main__':\n",
    "#    with Pool(1) as p:\n",
    "#        game_returns = p.map(little_sim, [(agent1,agent2)]*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '9', 0.0), (1, '6', 1607202463.258475), (2, '8', 3214404926.517396), (3, '2', 4821607389.776707), (4, '6', 6428809853.036744), (5, '9', 8036012316.29691), (6, '6', 9643214779.55541), (7, '3', 11250417242.814806), (8, '4', 12857619706.074352), (9, '2', 14464822169.333754)]\n"
     ]
    }
   ],
   "source": [
    "def test(x):\n",
    "    return x, str(time.time())[-1], x*time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(6) as p:\n",
    "        returns = p.map(test, [x for x in range(10)])\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_sim_parallel_nn(agent1, agent2, n_steps = 1, games_per_step = 32, nn_to_update = [], parallel_threads = 6):\n",
    "    game_times = []\n",
    "    nn_update_times = []\n",
    "    winner_list = []\n",
    "    \n",
    "    for n in range(n_steps):\n",
    "        print('game_step', n, end = ' ')\n",
    "        game_start = time.time()\n",
    "        \n",
    "        #if __name__ == '__main__':\n",
    "        #    with Pool(parallel_threads) as p:\n",
    "        #        game_returns = p.map(little_sim, [(agent1,agent2)]*games_per_step)\n",
    "        game_returns = [little_sim((agent1,agent2)) for x in range(games_per_step)] #comment our parallelilization if needed.\n",
    "        \n",
    "        #games_in_progress = [pentago() for g in games_per_step]\n",
    "        #finished_games = []\n",
    "        #while \n",
    "            \n",
    "        game_times.append(time.time()-game_start)\n",
    "            \n",
    "        player1_winner = 0\n",
    "        player2_winner = 0\n",
    "        # check for winner and create update batch\n",
    "        nn_input_batch = []\n",
    "        rewards_for_batch = []\n",
    "        for game in game_returns:\n",
    "            if game.winner:\n",
    "                if game.winner == 1: player1_winner += 1\n",
    "                else: player2_winner += 1\n",
    "                \n",
    "                #cumulate rewards\n",
    "                boardstates, rewards = reward_func(game.history)\n",
    "                nn_input_batch += [boardstate_to_cnn_input(bs) for bs in boardstates] # add nn_inputs to training list (x)\n",
    "                rewards_for_batch += rewards # add rewards to training list (Y)\n",
    "                \n",
    "        # train the neural network\n",
    "        t0 = time.time()\n",
    "        print(\"updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\")\n",
    "        for nn in nn_to_update:\n",
    "            nn.update_model(np.array(nn_input_batch), np.array(rewards_for_batch))\n",
    "        nn_update_times.append(time.time() - t0)\n",
    "        \n",
    "        #if agent1.epsilon > agent1.epsilon_min: agent1.epsilon *= agent1.epsilon_decay\n",
    "        #if agent2.epsilon > agent2.epsilon_min: agent2.epsilon *= agent2.epsilon_decay\n",
    "        print(\"player 1 wins:\", player1_winner)\n",
    "        print(\"player 2 wins:\", player2_winner)\n",
    "        print(\"parallelized batch took\", game_times[-1], \"seconds.\")\n",
    "        print(\"neural network update:\", nn_update_times[-1], \"seconds.\")\n",
    "        \n",
    "    # end of simulation runs, save q_table(s) to disk\n",
    "    nn_num = 1\n",
    "    time_str = str(datetime.now())[:19].replace(':','_')\n",
    "    for nn in nn_to_update:\n",
    "        with open(f'CNN_{nn_num}_'+time_str+'.pickle', 'wb') as file:\n",
    "            pickle.dump(nn, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        nn_num += 1\n",
    "    \n",
    "    return game_times\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Note you will overwrite this q_table and agents if you run this cell again.    Verify you won't lose your data!\n",
    "#with open('decay_q_table1_2020-11-29 12_09_00.pickle', 'rb') as file:\n",
    "#    qtable1 =  pickle.load(file)\n",
    "nn_1 = cnn_model()\n",
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = .02, epsilon = 1)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_1, epsilon_min = .15, epsilon = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_step 0 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 2s 2ms/step - loss: 0.6998\n",
      "player 1 wins: 25\n",
      "player 2 wins: 6\n",
      "parallelized batch took 63.213449001312256 seconds.\n",
      "neural network update: 1.9857659339904785 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[63.213449001312256]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sim_parallel_nn(agent1, agent2, n_steps=1, games_per_step=32, nn_to_update=[nn_1], parallel_threads=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent2.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_step 0 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "707/707 [==============================] - 0s 177us/step - loss: 0.2039\n",
      "player 1 wins: 19\n",
      "player 2 wins: 10\n",
      "parallelized batch took 56.845152139663696 seconds.\n",
      "neural network update: 0.12768101692199707 seconds.\n",
      "game_step 1 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "850/850 [==============================] - 0s 169us/step - loss: 0.1874\n",
      "player 1 wins: 19\n",
      "player 2 wins: 12\n",
      "parallelized batch took 60.10708212852478 seconds.\n",
      "neural network update: 0.145676851272583 seconds.\n",
      "game_step 2 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 181us/step - loss: 0.1691\n",
      "player 1 wins: 25\n",
      "player 2 wins: 6\n",
      "parallelized batch took 57.84349822998047 seconds.\n",
      "neural network update: 0.14233803749084473 seconds.\n",
      "game_step 3 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "703/703 [==============================] - 0s 170us/step - loss: 0.1518\n",
      "player 1 wins: 27\n",
      "player 2 wins: 4\n",
      "parallelized batch took 53.924663066864014 seconds.\n",
      "neural network update: 0.12165617942810059 seconds.\n",
      "game_step 4 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "786/786 [==============================] - 0s 173us/step - loss: 0.1730\n",
      "player 1 wins: 22\n",
      "player 2 wins: 8\n",
      "parallelized batch took 59.06911897659302 seconds.\n",
      "neural network update: 0.13857316970825195 seconds.\n",
      "game_step 5 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "690/690 [==============================] - 0s 185us/step - loss: 0.1656\n",
      "player 1 wins: 27\n",
      "player 2 wins: 5\n",
      "parallelized batch took 52.32254195213318 seconds.\n",
      "neural network update: 0.12985706329345703 seconds.\n",
      "game_step 6 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "738/738 [==============================] - 0s 172us/step - loss: 0.1409\n",
      "player 1 wins: 28\n",
      "player 2 wins: 4\n",
      "parallelized batch took 56.269805908203125 seconds.\n",
      "neural network update: 0.12915802001953125 seconds.\n",
      "game_step 7 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "702/702 [==============================] - 0s 173us/step - loss: 0.1137\n",
      "player 1 wins: 30\n",
      "player 2 wins: 2\n",
      "parallelized batch took 54.090025186538696 seconds.\n",
      "neural network update: 0.12353682518005371 seconds.\n",
      "game_step 8 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "604/604 [==============================] - 0s 177us/step - loss: 0.1453\n",
      "player 1 wins: 26\n",
      "player 2 wins: 3\n",
      "parallelized batch took 55.296996116638184 seconds.\n",
      "neural network update: 0.10882306098937988 seconds.\n",
      "game_step 9 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "662/662 [==============================] - 0s 184us/step - loss: 0.1812\n",
      "player 1 wins: 24\n",
      "player 2 wins: 6\n",
      "parallelized batch took 55.20333790779114 seconds.\n",
      "neural network update: 0.12394857406616211 seconds.\n",
      "game_step 0 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 287us/step - loss: 0.1280\n",
      "player 1 wins: 29\n",
      "player 2 wins: 3\n",
      "parallelized batch took 56.31709814071655 seconds.\n",
      "neural network update: 0.21080493927001953 seconds.\n",
      "game_step 1 updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\n",
      "Epoch 1/1\n",
      "735/735 [==============================] - 0s 213us/step - loss: 0.1598\n",
      "player 1 wins: 27\n",
      "player 2 wins: 5\n",
      "parallelized batch took 57.0644428730011 seconds.\n",
      "neural network update: 0.1589491367340088 seconds.\n",
      "game_step 2 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-c7d4034dc925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtime0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbig_sim_parallel_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames_per_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_to_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seconds.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-09663b3ae365>\u001b[0m in \u001b[0;36mbig_sim_parallel_nn\u001b[0;34m(agent1, agent2, n_steps, games_per_step, nn_to_update, parallel_threads)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#    with Pool(parallel_threads) as p:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#        game_returns = p.map(little_sim, [(agent1,agent2)]*games_per_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mgame_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlittle_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames_per_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#comment our parallelilization if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#games_in_progress = [pentago() for g in games_per_step]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-09663b3ae365>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#    with Pool(parallel_threads) as p:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#        game_returns = p.map(little_sim, [(agent1,agent2)]*games_per_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mgame_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlittle_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgames_per_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#comment our parallelilization if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#games_in_progress = [pentago() for g in games_per_step]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-b1ada5c448c2>\u001b[0m in \u001b[0;36mlittle_sim\u001b[0;34m(agents)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpentago\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgameover\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgameover\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-21d6d3f3cb59>\u001b[0m in \u001b[0;36mmake_move\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# get possible next possible boardstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnext_possible_boardstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_possible_next_boardstates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboardstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mkey_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_possible_boardstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-21d6d3f3cb59>\u001b[0m in \u001b[0;36mget_possible_next_boardstates\u001b[0;34m(self, boardstate)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnext_possible_boardstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_avail_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboardstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mpossible_boardstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboardstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m#print(possible_boardstate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboardstate_to_ideal_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_boardstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Fall2020MIDS/w251_pentago/helper_func.py\u001b[0m in \u001b[0;36mfullmove\u001b[0;34m(boardstate, placement_location, quadrant, rotation, marble)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mnew_boardstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplace_marble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboardstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mnew_boardstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate_quadrant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_boardstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquadrant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_boardstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Fall2020MIDS/w251_pentago/helper_func.py\u001b[0m in \u001b[0;36mrotate_quadrant\u001b[0;34m(boardstate, quadrant, rotation)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_boardstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# slice out quadrant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#print(q)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m#print(q)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mnew_boardstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mrot90\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mrot90\u001b[0;34m(m, k, axes)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Axes must be different.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## Change number of games to simulate here\n",
    "#_games = 10000\n",
    "##################################################\n",
    "\n",
    "time0 = time.time()\n",
    "for x in range(20):\n",
    "    big_sim_parallel_nn(agent1, agent2, n_steps=10, games_per_step=32, nn_to_update=[nn_1], parallel_threads=6)\n",
    "print(time.time()-time0, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.593888888888888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59738/60/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444204"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qtable1.q_dict)\n",
    "#agent1.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = [2,2,2,2,2]\n",
    "\n",
    "with open('test.pickle', 'wb') as file:\n",
    "    pickle.dump(x, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('test.pickle', 'rb') as file:\n",
    "    y = pickle.load(file)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = []\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    ns.append(v[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5444199,       3,       1,       0,       0,       0,       0,\n",
       "              0,       0,       1]),\n",
       " array([1.000000e+00, 1.797250e+04, 3.594400e+04, 5.391550e+04,\n",
       "        7.188700e+04, 8.985850e+04, 1.078300e+05, 1.258015e+05,\n",
       "        1.437730e+05, 1.617445e+05, 1.797160e+05]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(np.array(ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153873\n",
      "67959\n",
      "46602\n",
      "33784\n",
      "12230\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in ns if x != 1]))\n",
    "print(len([x for x in ns if x > 2]))\n",
    "print(len([x for x in ns if x > 3]))\n",
    "print(len([x for x in ns if x > 4]))\n",
    "print(len([x for x in ns if x > 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120020100111022211102201112221201221] (0.5, 1)\n",
      "[020011100211022012102201112221201221] (0.5, 1)\n",
      "[002011202211010012102201112201201221] (0.5, 1)\n",
      "[002011202211010012210201010201122221] (0.5, 1)\n",
      "[020011100211022012210001010201122221] (0.5, 1)\n",
      "[110020112001200220022012002010111221] (0.5, 1)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    print(k,v)\n",
    "    i += 1\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([94371,     6,     0,     0,     0,     0,     0,     0,     0,\n",
       "            1]),\n",
       " array([2.00000e+00, 1.15380e+04, 2.30740e+04, 3.46100e+04, 4.61460e+04,\n",
       "        5.76820e+04, 6.92180e+04, 8.07540e+04, 9.22900e+04, 1.03826e+05,\n",
       "        1.15362e+05]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram([x for x in ns if x != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
