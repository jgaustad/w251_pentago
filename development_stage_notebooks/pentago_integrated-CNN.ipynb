{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from helper_func import *\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import keras\n",
    "from multiprocessing import Pool\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from collections import deque\n",
    "from keras.activations import relu, linear\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Tensor(\"conv3d/Relu:0\", shape=(4, 26, 26, 26, 2), dtype=float32)\n",
      "(4, 26, 26, 26, 2)\n"
     ]
    }
   ],
   "source": [
    "# The inputs are 28x28x28 volumes with a single channel, and the  \n",
    "# batch size is 4  \n",
    "input_shape =(4, 28, 28, 28, 1)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv3D(2, 3, activation='relu', input_shape=input_shape[1:])(x)\n",
    "print(y)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class pentago:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state = None):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        #print('initializing')\n",
    "        \n",
    "        if state == None:\n",
    "            self.state = state = np.zeros((6,6), dtype=np.int)\n",
    "        self.history = []\n",
    "        self.winner = None\n",
    "        self.gameover = False\n",
    "        self.player_turn = 1\n",
    "    \n",
    "    def current_board_state(self):\n",
    "        # need to return a copy or bad stuff happens\n",
    "        return copy.copy(self.state)\n",
    "    \n",
    "    def game_history(self, player, move, cuad, rotatation):\n",
    "        self.history.append((boardstate_to_ideal_key(self.state), ideal_state(self.state), player, move, cuad, rotatation))\n",
    "        #return self.history\n",
    "\n",
    "    def find_winner(self, board_state):\n",
    "        player1_win = False\n",
    "        player_min1_win = False\n",
    "        diagonal1 = board_state.diagonal()\n",
    "        diagonal2 = np.fliplr(board_state).diagonal()\n",
    "        winning_slices =  np.vstack([board_state[1:,:].T, board_state[:-1,:].T, # all columns\n",
    "                              board_state[:,1:], board_state[:,:-1], # all rows\n",
    "                              diagonal1[1:], diagonal1[:-1], # diagonal 1\n",
    "                              diagonal2[1:],diagonal2[1:], # diagonal 2\n",
    "                              board_state.diagonal(offset=1), board_state.diagonal(offset=-1), # diagonal offsets \n",
    "                              np.fliplr(board_state).diagonal(offset=1), np.fliplr(board_state).diagonal(offset=-1)] ) # diagonal offsets\n",
    "        sums = np.dot(winning_slices, np.array([1,1,1,1,1]))\n",
    "        if 5 in sums: player1_win = True\n",
    "        if -5 in sums: player_min1_win = True\n",
    "        if player1_win == True or player_min1_win == True:\n",
    "           # print(\"Player 1 winner?\", player1_win, \"Player -1 winner?\", player_min1_win)\n",
    "            self.gameover = True\n",
    "            if player1_win == True:\n",
    "                self.winner = 1\n",
    "            elif player_min1_win ==True:\n",
    "                self.winner = -1\n",
    "            self.history.append(self.winner)\n",
    "        return \"Win\"\n",
    "\n",
    "    def check_gameover(self):\n",
    "        if not 0 in self.state:\n",
    "              self.gameover = True\n",
    "              #print(\"The game board is full!\")\n",
    "        \n",
    "    def full_move(self, move, cuad, direction, player, dtype=np.int):\n",
    "        if player != self.player_turn:\n",
    "            print( \"error, wrong player turn. No move taken.\")\n",
    "            return 'Error, wrong player turn.'\n",
    "        self.state = fullmove(self.state,move, cuad, direction, player)\n",
    "\n",
    "\n",
    "        self.game_history(move, player, cuad, direction)\n",
    "        self.find_winner(self.state) #return in find_winner if a winner is found\n",
    "        self.check_gameover() #return in check_gameover\n",
    "        if player == 1:\n",
    "            self.player_turn = -1\n",
    "        else:\n",
    "            self.player_turn = 1\n",
    "        #print('Successful Move')\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class q_table:\n",
    "\n",
    "    def __init__(self,length=0, games_played=0):\n",
    "        \"\"\"Initializes the class reservation\"\"\"\n",
    "        self.time = datetime.now()\n",
    "        self.length = length\n",
    "        self.q_dict = {}\n",
    "        self.games_played = games_played\n",
    "\n",
    "  #def time(self):\n",
    "    #self.time = time\n",
    "\n",
    "    def length(self):\n",
    "        self.length += 1\n",
    "    #self.length = length  \n",
    "    \n",
    "    def get_q_value(self, boardstate):\n",
    "        return self.q_dict.get(boardstate, (0, 0))\n",
    "    \n",
    "    def update_q_value(self, boardstate, new_val, update_function = None):\n",
    "        q_val, n = self.get_q_value(boardstate) \n",
    "        if update_function:\n",
    "            #print('using custom function')\n",
    "            self.q_dict[boardstate] = update_function(q_val, n, new_val)\n",
    "        else:\n",
    "            self.q_dict[boardstate] = [new_val, n+1]\n",
    "        return self.q_dict[boardstate]\n",
    "    \n",
    "    def update_post_game(self, history, update_fn):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "\n",
    "    def update_post_game2(self, history, update_fn, decay_reward = .9):\n",
    "        winner = history[-1]\n",
    "        \n",
    "        for boardposition in history[-2::-1]:\n",
    "            key = boardposition[0]\n",
    "            #print(key, winner)\n",
    "            self.update_q_value(key, winner, update_fn)\n",
    "            winner *= decay_reward\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "\n",
    "    def __init__(self,list_density=None,lr=0.02):\n",
    "        self.model = self.build_model(list_density,lr)\n",
    "\n",
    "    def build_model(self,den,lr):\n",
    "        model = Sequential()\n",
    "        #model.add(tf.keras.Input(shape=(6,6,2,)))\n",
    "        #for layer in den:\n",
    "        model.add(Conv2D(\n",
    "                            filters = 64, \n",
    "                            kernel_size = (3,3),\n",
    "                            strides = (3,3),\n",
    "                            padding = 'valid',\n",
    "                            activation = relu,\n",
    "                            input_shape = (6,6,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation = relu))\n",
    "        model.add(Dense(128, activation = relu))\n",
    "\n",
    "\n",
    "        model.add(Dense(1, activation=linear))\n",
    "        \n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def update_model(self,states_batch, q_batch, epochs = 1):\n",
    "\n",
    "        self.model.fit(states_batch,q_batch,epochs = epochs, verbose=1)\n",
    "\n",
    "    def predict_model(self, state_batch):\n",
    "        return self.model.predict_on_batch(state_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, 2)\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jgaustad/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05547169]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,1,1,-1,1,1],[0,0,0,0,0,0],[0,0,0,1,1,0],[0,0,1,-1,-1,0],[0,0,1,-1,1,0],[-1,-1,1,0,0,0]])\n",
    "x = boardstate_to_cnn_input(x)\n",
    "print(x.shape)\n",
    "out= c.model.predict(x.reshape(1,6,6,2))\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_agent:\n",
    "    \n",
    "    def __init__(self, nn, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.nn = nn\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            #print(possible_boardstate)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            cnn_input = boardstate_to_cnn_input(possible_boardstate)\n",
    "            #print(cnn_input)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append((cnn_input, move))\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        t0 = time.time()\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        #print(\"get possible boardstates\", time.time()-t0)\n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0][1] # capture the move needed\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "           # t0 = time.time()\n",
    "            nn_input_boardstate = []\n",
    "            move = []\n",
    "            for k,v in next_possible_boardstates.items():\n",
    "                nn_input_boardstate.append(v[0][0])\n",
    "                #print(v[0][0].shape)\n",
    "                move.append(v[0][1])\n",
    "          \n",
    "            #print(nn_input_boardstate, type(nn_input_boardstate))\n",
    "            #nn_input_batch = np.concatenate(nn_input_boardstate)\n",
    "            nn_input_batch = np.array(nn_input_boardstate)\n",
    "            #print(nn_input_batch, type(nn_input_batch))\n",
    "            #print(nn_input_boardstate)\n",
    "            q_values = self.nn.predict_model(nn_input_batch)\n",
    "            #print(\"get q_values boardstates\", time.time()-t0)\n",
    "\n",
    "            #print(q_values, type(q_values))\n",
    "            #print(\"----\")\n",
    "            q_values *= self.player\n",
    "            #q_values_list = [x*self.player for x in list(q_values)]\n",
    "            #print(q_values_list)\n",
    "            # get random index of a max value\n",
    "            #print('length fo q_values_list = ', len(q_values_list))\n",
    "            max_q = np.argmax(q_values)\n",
    "            #index_of_all_max = [i for i in range(len(q_values)) if q_values[i] == max_q]\n",
    "            #random_max_q_index = random.choice(index_of_all_max)\n",
    "            #print(\"MAX VALUE:\", q_values_list[random_max_q_index])\n",
    "            mv_to_take = move[max_q]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        #if self.epsilon > self.epsilon_min:\n",
    "        #    self.epsilon *= self.epsilon_decay \n",
    "        #else:\n",
    "        #    self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_agent_multigame:\n",
    "    \n",
    "    def __init__(self, nn, player = 1, epsilon = 1, epsilon_decay = .99995, epsilon_min = .5):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.nn = nn\n",
    "        self.player = player\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "            \n",
    "    def get_avail_moves(self,boardstate):\n",
    "        \"\"\"\n",
    "        This method creates a list with available spaces in the board and combination of quadrant and rotation\n",
    "        The input is the board state (6x6) numpy array\n",
    "        \"\"\"\n",
    "        x = np.where(boardstate == 0)\n",
    "        #print(x)\n",
    "        available_positions_for_placement = list(zip(x[0], x[1]))\n",
    "        \n",
    "        # all available positions (p), quadrants(q), rotations(r)\n",
    "        available_moves = [(p,q,r) for p in available_positions_for_placement for q in [1,2,3,4] for r in [-1,1]]\n",
    "        #print(len(available_moves))\n",
    "        return available_moves\n",
    "    \n",
    "    def get_possible_next_boardstates(self, boardstate):\n",
    "        next_possible_boardstates = defaultdict(list)\n",
    "        for move in self.get_avail_moves(boardstate):\n",
    "            possible_boardstate = fullmove(boardstate,*move, self.player)\n",
    "            #print(possible_boardstate)\n",
    "            key = boardstate_to_ideal_key(possible_boardstate)\n",
    "            cnn_input = boardstate_to_cnn_input(possible_boardstate)\n",
    "            #print(cnn_input)\n",
    "            #print(key)\n",
    "            next_possible_boardstates[key].append((cnn_input, move))\n",
    "            \n",
    "        return next_possible_boardstates\n",
    "    \n",
    "    def make_move(self, game):\n",
    "        \n",
    "        # get the current boardstate from the pentago class\n",
    "        boardstate = game.current_board_state()\n",
    "        \n",
    "        # get possible next possible boardstates\n",
    "        next_possible_boardstates = self.get_possible_next_boardstates(boardstate)\n",
    "        key_list = list(next_possible_boardstates.keys())\n",
    "        \n",
    "        # determine if to take random move\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            random_bs = random.choice(key_list)\n",
    "            random_mv = next_possible_boardstates[random_bs][0][1] # capture the move needed\n",
    "            \n",
    "            game.full_move(*random_mv,self.player)\n",
    "            \n",
    "        else:\n",
    "            #print(\"not random\", self.player)\n",
    "            nn_input_boardstate = []\n",
    "            move = []\n",
    "            for k,v in next_possible_boardstates.items():\n",
    "                nn_input_boardstate.append(v[0][0])\n",
    "                #print(v[0][0].shape)\n",
    "                move.append(v[0][1])\n",
    "            \n",
    "            \n",
    "            #print(nn_input_boardstate, type(nn_input_boardstate))\n",
    "            #nn_input_batch = np.concatenate(nn_input_boardstate)\n",
    "            nn_input_batch = np.array(nn_input_boardstate)\n",
    "            #print(nn_input_batch, type(nn_input_batch))\n",
    "            #print(nn_input_boardstate)\n",
    "            q_values = self.nn.predict_model(nn_input_batch)\n",
    "            #print(q_values, type(q_values))\n",
    "            #print(\"----\")\n",
    "            q_values *= self.player\n",
    "            #q_values_list = [x*self.player for x in list(q_values)]\n",
    "            #print(q_values_list)\n",
    "            # get random index of a max value\n",
    "            #print('length fo q_values_list = ', len(q_values_list))\n",
    "            max_q = np.argmax(q_values)\n",
    "            #index_of_all_max = [i for i in range(len(q_values)) if q_values[i] == max_q]\n",
    "            #random_max_q_index = random.choice(index_of_all_max)\n",
    "            #print(\"MAX VALUE:\", q_values_list[random_max_q_index])\n",
    "            mv_to_take = move[max_q]\n",
    "            game.full_move(*mv_to_take, self.player)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4,2,1,4])\n",
    "np.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn_1 = cnn_model([72,144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = 0, epsilon_decay = .999, epsilon = 0)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_2, epsilon_min = 0, epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9728119373321533\n",
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('[000000000000000000000000000000000100]',\n",
       "  array([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0]]),\n",
       "  (0, 0),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000100000000000200]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  0]]),\n",
       "  (0, 0),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000201000000000100]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  1],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  0,  0]]),\n",
       "  (0, 0),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000102000000000201]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  0, -1],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  1]]),\n",
       "  (0, 0),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000201000100000102]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  1],\n",
       "         [ 0,  0,  0,  1,  0,  0],\n",
       "         [ 0,  0,  0,  1,  0, -1]]),\n",
       "  (0, 1),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000112000200000201]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  1, -1],\n",
       "         [ 0,  0,  0, -1,  0,  0],\n",
       "         [ 0,  0,  0, -1,  0,  1]]),\n",
       "  (0, 1),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000211000200000112]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1,  1,  1],\n",
       "         [ 0,  0,  0, -1,  0,  0],\n",
       "         [ 0,  0,  0,  1,  1, -1]]),\n",
       "  (0, 1),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000122000101000221]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1, -1, -1],\n",
       "         [ 0,  0,  0,  1,  0,  1],\n",
       "         [ 0,  0,  0, -1, -1,  1]]),\n",
       "  (0, 1),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000221000101001122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1, -1,  1],\n",
       "         [ 0,  0,  0,  1,  0,  1],\n",
       "         [ 0,  0,  1,  1, -1, -1]]),\n",
       "  (0, 3),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000112000202021211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  1, -1],\n",
       "         [ 0,  0,  0, -1,  0, -1],\n",
       "         [ 0, -1,  1, -1,  1,  1]]),\n",
       "  (0, 4),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000221000101121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1, -1,  1],\n",
       "         [ 0,  0,  0,  1,  0,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (0, 5),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000112000222121211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  1, -1],\n",
       "         [ 0,  0,  0, -1, -1, -1],\n",
       "         [ 1, -1,  1, -1,  1,  1]]),\n",
       "  (1, 1),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000221001121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1, -1,  1],\n",
       "         [ 0,  0,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (1, 3),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000112021222121211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1,  1, -1],\n",
       "         [ 0, -1,  1, -1, -1, -1],\n",
       "         [ 1, -1,  1, -1,  1,  1]]),\n",
       "  (1, 4),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000000221121121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, -1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (1, 5),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000002112121222121211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0, -1,  1,  1, -1],\n",
       "         [ 1, -1,  1, -1, -1, -1],\n",
       "         [ 1, -1,  1, -1,  1,  1]]),\n",
       "  (2, 3),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000012221121121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  1, -1, -1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (2, 4),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000000212112121222121211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [-1,  1, -1,  1,  1, -1],\n",
       "         [ 1, -1,  1, -1, -1, -1],\n",
       "         [ 1, -1,  1, -1,  1,  1]]),\n",
       "  (2, 5),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000001212221121121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  1],\n",
       "         [-1,  1, -1, -1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (3, 0),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000021212112121222121211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, -1,  1],\n",
       "         [-1,  1, -1,  1,  1, -1],\n",
       "         [ 1, -1,  1, -1, -1, -1],\n",
       "         [ 1, -1,  1, -1,  1,  1]]),\n",
       "  (3, 1),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000000121212221121121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1, -1,  1],\n",
       "         [-1,  1, -1, -1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (3, 2),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000002121212112121222121211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0, -1,  1, -1,  1],\n",
       "         [-1,  1, -1,  1,  1, -1],\n",
       "         [ 1, -1,  1, -1, -1, -1],\n",
       "         [ 1, -1,  1, -1,  1,  1]]),\n",
       "  (3, 3),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000012121212221121121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  1, -1,  1, -1,  1],\n",
       "         [-1,  1, -1, -1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (3, 4),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000000121212211212222121112121]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0],\n",
       "         [ 1, -1,  1, -1,  1, -1],\n",
       "         [-1,  1,  1, -1,  1, -1],\n",
       "         [-1, -1, -1,  1, -1,  1],\n",
       "         [ 1,  1, -1,  1, -1,  1]]),\n",
       "  (3, 5),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000001212121212221121121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  1],\n",
       "         [-1,  1, -1,  1, -1,  1],\n",
       "         [-1,  1, -1, -1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (4, 0),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000021212121212112121222121211]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, -1,  1],\n",
       "         [-1,  1, -1,  1, -1,  1],\n",
       "         [-1,  1, -1,  1,  1, -1],\n",
       "         [ 1, -1,  1, -1, -1, -1],\n",
       "         [ 1, -1,  1, -1,  1,  1]]),\n",
       "  (4, 1),\n",
       "  -1,\n",
       "  1,\n",
       "  -1),\n",
       " ('[000000000121212121212221121121121122]',\n",
       "  array([[ 0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  1, -1,  1],\n",
       "         [-1,  1, -1,  1, -1,  1],\n",
       "         [-1,  1, -1, -1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1,  1],\n",
       "         [ 1, -1,  1,  1, -1, -1]]),\n",
       "  (4, 2),\n",
       "  1,\n",
       "  1,\n",
       "  -1),\n",
       " -1]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "g = pentago()\n",
    "while g.gameover == False:\n",
    "    agent1.make_move(g)\n",
    "    if g.gameover == True: break\n",
    "    agent2.make_move(g)\n",
    "    #break\n",
    "\n",
    "#bs = agent1.get_possible_next_boardstates(g.current_board_state())\n",
    "    \n",
    "print(time.time()-t0)\n",
    "print(len(g.history))\n",
    "g.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> 28 28\n",
      "(28, 6, 6, 2) (28,)\n",
      "Epoch 1/1\n",
      "28/28 [==============================] - 1s 34ms/step - loss: 0.1720\n"
     ]
    }
   ],
   "source": [
    "g.history[:2]\n",
    "def reward_func(history, decay_factor = .9):\n",
    "    winner = history[-1]\n",
    "    nn_inputs = []\n",
    "    rewards = []\n",
    "    for boardposition in history[-2::-1]:\n",
    "        nn_inputs.append(boardposition[1])\n",
    "        rewards.append(winner)\n",
    "        winner *= decay_factor\n",
    "    return nn_inputs, rewards\n",
    "a = reward_func(g.history)\n",
    "print(type(a[0]), type(a[1]), len(a[0]), len(a[1]))\n",
    "x = np.array([boardstate_to_cnn_input(bs) for bs in a[0]])\n",
    "Y = np.array(a[1])\n",
    "print(x.shape, Y.shape)\n",
    "\n",
    "nn_1.update_model(x,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def little_sim(agents):\n",
    "    agent1, agent2 = agents\n",
    "    g = pentago()\n",
    "    while g.gameover == False:\n",
    "        agent1.make_move(g)\n",
    "        if g.gameover ==True: break\n",
    "        agent2.make_move(g)\n",
    "    #print('gameover.')\n",
    "    return g\n",
    "#little_sim((agent1,agent2))\n",
    "#if __name__ == '__main__':\n",
    "#    with Pool(1) as p:\n",
    "#        game_returns = p.map(little_sim, [(agent1,agent2)]*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '2', 0.0), (1, '9', 1607207032.787425), (2, '2', 3214414065.575404), (3, '4', 4821621098.363877), (4, '6', 6428828131.152436), (5, '4', 8036035163.939341), (6, '8', 9643242196.727467), (7, '9', 11250449229.516958), (8, '1', 12857656262.305271), (9, '6', 14464863295.093979)]\n"
     ]
    }
   ],
   "source": [
    "def test(x):\n",
    "    return x, str(time.time())[-1], x*time.time()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(6) as p:\n",
    "        returns = p.map(test, [x for x in range(10)])\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_sim_parallel_nn(agent1, agent2, n_steps = 1, games_per_step = 32, nn_to_update = [], parallel_threads = 6):\n",
    "    game_times = []\n",
    "    evals = []\n",
    "    eval_times = []\n",
    "    epsilons = []\n",
    "    nn_update_times = []\n",
    "    winner_list = []\n",
    "    \n",
    "    for n in range(n_steps):\n",
    "        print('game_step', n, end = ' ')\n",
    "        game_start = time.time()\n",
    "        \n",
    "        #if __name__ == '__main__':\n",
    "        #    with Pool(parallel_threads) as p:\n",
    "        #        game_returns = p.map(little_sim, [(agent1,agent2)]*games_per_step)\n",
    "        game_returns = [little_sim((agent1,agent2)) for x in range(games_per_step)] #comment our parallelilization if needed.\n",
    "        \n",
    "        #games_in_progress = [pentago() for g in games_per_step]\n",
    "        #finished_games = []\n",
    "        #while \n",
    "            \n",
    "        game_times.append(time.time()-game_start)\n",
    "            \n",
    "        player1_winner = 0\n",
    "        player2_winner = 0\n",
    "        # check for winner and create update batch\n",
    "        nn_input_batch = []\n",
    "        rewards_for_batch = []\n",
    "        for game in game_returns:\n",
    "            if game.winner:\n",
    "                if game.winner == 1: player1_winner += 1\n",
    "                else: player2_winner += 1\n",
    "                \n",
    "                # accumulate rewards and inputs for training\n",
    "                boardstates, rewards = reward_func(game.history)\n",
    "                nn_input_batch += [boardstate_to_cnn_input(bs) for bs in boardstates] # add nn_inputs to training list (x)\n",
    "                rewards_for_batch += rewards # add rewards to training list (Y)\n",
    "        \n",
    "        # evaluate the model\n",
    "        t0 = time.time()\n",
    "        for nn in nn_to_update:\n",
    "            evals.append(nn.model.evaluate(np.array(nn_input_batch), np.array(rewards_for_batch)))\n",
    "        eval_times.append(time.time() - t0)\n",
    "\n",
    "        \n",
    "        # train the neural network\n",
    "        t0 = time.time()\n",
    "        print(f\"updating with {len(nn_input_batch)}, {len(rewards_for_batch)} training batch.\")\n",
    "        for nn in nn_to_update:\n",
    "            nn.update_model(np.array(nn_input_batch), np.array(rewards_for_batch))\n",
    "        nn_update_times.append(time.time() - t0)\n",
    "\n",
    "\n",
    "        print(\"player 1 wins:\", player1_winner)\n",
    "        print(\"player 2 wins:\", player2_winner)\n",
    "        print(\"simulations took\", game_times[-1], \"seconds.\")\n",
    "        print(\"neural network update time:\", nn_update_times[-1], \"seconds.\")\n",
    "        print(\"eval score on batch:\", evals[-1])\n",
    "        print(\"epsilons agent1 and agent2:\", agent1.epsilon, agent2.epsilon)\n",
    "        epsilons.append((agent1.epsilon, agent2.epsilon))\n",
    "        if agent1.epsilon > agent1.epsilon_min: agent1.epsilon *= agent1.epsilon_decay\n",
    "        else: agent1.epsilon == agent1.epsilon_min\n",
    "        if agent2.epsilon > agent2.epsilon_min: agent2.epsilon *= agent2.epsilon_decay\n",
    "        else: agent2.epsilon == agent2.epsilon_min\n",
    "\n",
    "    # end of simulation runs, save q_table(s) to disk\n",
    "    nn_num = 1\n",
    "    time_str = str(datetime.now())[:19].replace(':','_')\n",
    "    for nn in nn_to_update:\n",
    "        with open(f'CNN_{nn_num}_'+time_str+'.pickle', 'wb') as file:\n",
    "            pickle.dump(nn, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        nn_num += 1\n",
    "    \n",
    "    return epsilons, nn_update_times, player1_winner, player2_winner, evals, eval_times\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Note you will overwrite this q_table and agents if you run this cell again.    Verify you won't lose your data!\n",
    "with open('decay_q_table1_2020-11-29 12_09_00.pickle', 'rb') as file:\n",
    "    qtable1 =  pickle.load(file)\n",
    "nn_1 = cnn_model()\n",
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = .02, epsilon = 1, epsilon_decay = .99)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_1, epsilon_min = .15, epsilon = 1, epsilon_decay = .995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 1s 974us/step\n",
      "updating with 797, 797 training batch.\n",
      "Epoch 1/1\n",
      "797/797 [==============================] - 1s 2ms/step - loss: 0.3802\n",
      "player 1 wins: 16\n",
      "player 2 wins: 13\n",
      "simulations took 56.79386496543884 seconds.\n",
      "neural network update time: 1.7375359535217285 seconds.\n",
      "eval score on batch: 0.20059990673472022\n",
      "epsilons agent1 and agent2: 1 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(1, 1)],\n",
       " [1.7375359535217285],\n",
       " 16,\n",
       " 13,\n",
       " [0.20059990673472022],\n",
       " [0.7785100936889648])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_sim_parallel_nn(agent1, agent2, n_steps=1, games_per_step=32, nn_to_update=[nn_1], parallel_threads=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 2, 2, 64)          1216      \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 100,033\n",
      "Trainable params: 100,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn_2 = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = nn_agent(player = 1,  nn = nn_1, epsilon_min = .1, epsilon = .3, epsilon_decay = .995)\n",
    "agent2 = nn_agent(player = -1,  nn = nn_2, epsilon_min = .1, epsilon = .3, epsilon_decay = .995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = str(datetime.now())[:19].replace(':','_')\n",
    "with open(f'outputs_'+time_str+'.pickle', 'wb') as file:\n",
    "    pickle.dump(outputs, file, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/760 [==============================] - 0s 45us/step\n",
      "760/760 [==============================] - 0s 48us/step\n",
      "updating with 760, 760 training batch.\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 202us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 201us/step - loss: 0.1998\n",
      "player 1 wins: 19\n",
      "player 2 wins: 11\n",
      "simulations took 53.884236097335815 seconds.\n",
      "neural network update time: 0.31261706352233887 seconds.\n",
      "eval score on batch: 0.20240403511806537\n",
      "epsilons agent1 and agent2: 0.26599605315039226 0.26599605315039226\n",
      "792/792 [==============================] - 0s 42us/step\n",
      "792/792 [==============================] - 0s 37us/step\n",
      "updating with 792, 792 training batch.\n",
      "Epoch 1/1\n",
      "792/792 [==============================] - 0s 154us/step - loss: 0.1903\n",
      "Epoch 1/1\n",
      "792/792 [==============================] - 0s 153us/step - loss: 0.1905\n",
      "player 1 wins: 19\n",
      "player 2 wins: 11\n",
      "simulations took 56.347805976867676 seconds.\n",
      "neural network update time: 0.24923396110534668 seconds.\n",
      "eval score on batch: 0.19175122256832894\n",
      "epsilons agent1 and agent2: 0.2646660728846403 0.2646660728846403\n",
      "828/828 [==============================] - 0s 47us/step\n",
      "828/828 [==============================] - 0s 46us/step\n",
      "updating with 828, 828 training batch.\n",
      "Epoch 1/1\n",
      "828/828 [==============================] - 0s 205us/step - loss: 0.1954\n",
      "Epoch 1/1\n",
      "828/828 [==============================] - 0s 186us/step - loss: 0.1958\n",
      "player 1 wins: 17\n",
      "player 2 wins: 14\n",
      "simulations took 56.988773822784424 seconds.\n",
      "neural network update time: 0.3297312259674072 seconds.\n",
      "eval score on batch: 0.19794656501875985\n",
      "epsilons agent1 and agent2: 0.26334274252021705 0.26334274252021705\n",
      "711/711 [==============================] - 0s 43us/step\n",
      "711/711 [==============================] - 0s 42us/step\n",
      "updating with 711, 711 training batch.\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 194us/step - loss: 0.1892\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 184us/step - loss: 0.1930\n",
      "player 1 wins: 21\n",
      "player 2 wins: 8\n",
      "simulations took 56.52450394630432 seconds.\n",
      "neural network update time: 0.2745990753173828 seconds.\n",
      "eval score on batch: 0.20823139068834307\n",
      "epsilons agent1 and agent2: 0.26202602880761594 0.26202602880761594\n",
      "736/736 [==============================] - 0s 40us/step\n",
      "736/736 [==============================] - 0s 39us/step\n",
      "updating with 736, 736 training batch.\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 149us/step - loss: 0.1992\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 158us/step - loss: 0.2011\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 56.750751972198486 seconds.\n",
      "neural network update time: 0.23167085647583008 seconds.\n",
      "eval score on batch: 0.21469927903102792\n",
      "epsilons agent1 and agent2: 0.26071589866357786 0.26071589866357786\n",
      "815/815 [==============================] - 0s 40us/step\n",
      "815/815 [==============================] - 0s 40us/step\n",
      "updating with 815, 815 training batch.\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 0s 156us/step - loss: 0.1641\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 0s 169us/step - loss: 0.1655\n",
      "player 1 wins: 23\n",
      "player 2 wins: 7\n",
      "simulations took 57.2339289188385 seconds.\n",
      "neural network update time: 0.2705209255218506 seconds.\n",
      "eval score on batch: 0.1774097137724512\n",
      "epsilons agent1 and agent2: 0.25941231917026 0.25941231917026\n",
      "739/739 [==============================] - 0s 45us/step\n",
      "739/739 [==============================] - 0s 41us/step\n",
      "updating with 739, 739 training batch.\n",
      "Epoch 1/1\n",
      "739/739 [==============================] - 0s 159us/step - loss: 0.2189\n",
      "Epoch 1/1\n",
      "739/739 [==============================] - 0s 166us/step - loss: 0.2230\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 55.83563709259033 seconds.\n",
      "neural network update time: 0.2459728717803955 seconds.\n",
      "eval score on batch: 0.256094873616918\n",
      "epsilons agent1 and agent2: 0.2581152575744087 0.2581152575744087\n",
      "700/700 [==============================] - 0s 41us/step\n",
      "700/700 [==============================] - 0s 38us/step\n",
      "updating with 700, 700 training batch.\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.2011\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 164us/step - loss: 0.1881\n",
      "player 1 wins: 20\n",
      "player 2 wins: 7\n",
      "simulations took 58.00838613510132 seconds.\n",
      "neural network update time: 0.23593401908874512 seconds.\n",
      "eval score on batch: 0.22174031138420106\n",
      "epsilons agent1 and agent2: 0.2568246812865366 0.2568246812865366\n",
      "817/817 [==============================] - 0s 42us/step\n",
      "817/817 [==============================] - 0s 37us/step\n",
      "updating with 817, 817 training batch.\n",
      "Epoch 1/1\n",
      "817/817 [==============================] - 0s 157us/step - loss: 0.2286\n",
      "Epoch 1/1\n",
      "817/817 [==============================] - 0s 156us/step - loss: 0.2234\n",
      "player 1 wins: 10\n",
      "player 2 wins: 21\n",
      "simulations took 56.370535373687744 seconds.\n",
      "neural network update time: 0.2612590789794922 seconds.\n",
      "eval score on batch: 0.30271509597066565\n",
      "epsilons agent1 and agent2: 0.25554055788010394 0.25554055788010394\n",
      "750/750 [==============================] - 0s 38us/step\n",
      "750/750 [==============================] - 0s 39us/step\n",
      "updating with 750, 750 training batch.\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 165us/step - loss: 0.2178\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 163us/step - loss: 0.2118\n",
      "player 1 wins: 18\n",
      "player 2 wins: 10\n",
      "simulations took 57.853320837020874 seconds.\n",
      "neural network update time: 0.251209020614624 seconds.\n",
      "eval score on batch: 0.28113920700550077\n",
      "epsilons agent1 and agent2: 0.2542628550907034 0.2542628550907034\n",
      "757/757 [==============================] - 0s 41us/step\n",
      "757/757 [==============================] - 0s 38us/step\n",
      "updating with 757, 757 training batch.\n",
      "Epoch 1/1\n",
      "757/757 [==============================] - 0s 155us/step - loss: 0.1733\n",
      "Epoch 1/1\n",
      "757/757 [==============================] - 0s 155us/step - loss: 0.1723\n",
      "player 1 wins: 20\n",
      "player 2 wins: 8\n",
      "simulations took 58.81599020957947 seconds.\n",
      "neural network update time: 0.23992586135864258 seconds.\n",
      "eval score on batch: 0.17307565660096577\n",
      "epsilons agent1 and agent2: 0.2529915408152499 0.2529915408152499\n",
      "633/633 [==============================] - 0s 42us/step\n",
      "633/633 [==============================] - 0s 39us/step\n",
      "updating with 633, 633 training batch.\n",
      "Epoch 1/1\n",
      "633/633 [==============================] - 0s 157us/step - loss: 0.1895\n",
      "Epoch 1/1\n",
      "633/633 [==============================] - 0s 154us/step - loss: 0.1923\n",
      "player 1 wins: 21\n",
      "player 2 wins: 7\n",
      "simulations took 53.0956928730011 seconds.\n",
      "neural network update time: 0.20234394073486328 seconds.\n",
      "eval score on batch: 0.19349348766355529\n",
      "epsilons agent1 and agent2: 0.25172658311117363 0.25172658311117363\n",
      "767/767 [==============================] - 0s 41us/step\n",
      "767/767 [==============================] - 0s 37us/step\n",
      "updating with 767, 767 training batch.\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 153us/step - loss: 0.2156\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 152us/step - loss: 0.2148\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 56.747875928878784 seconds.\n",
      "neural network update time: 0.2390727996826172 seconds.\n",
      "eval score on batch: 0.2571119191962861\n",
      "epsilons agent1 and agent2: 0.25046795019561774 0.25046795019561774\n",
      "775/775 [==============================] - 0s 40us/step\n",
      "775/775 [==============================] - 0s 39us/step\n",
      "updating with 775, 775 training batch.\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 163us/step - loss: 0.2089\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 159us/step - loss: 0.2073\n",
      "player 1 wins: 17\n",
      "player 2 wins: 13\n",
      "simulations took 56.50285482406616 seconds.\n",
      "neural network update time: 0.25497984886169434 seconds.\n",
      "eval score on batch: 0.2200448429007684\n",
      "epsilons agent1 and agent2: 0.24921561044463966 0.24921561044463966\n",
      "786/786 [==============================] - 0s 39us/step\n",
      "786/786 [==============================] - 0s 40us/step\n",
      "updating with 786, 786 training batch.\n",
      "Epoch 1/1\n",
      "786/786 [==============================] - 0s 166us/step - loss: 0.1846\n",
      "Epoch 1/1\n",
      "786/786 [==============================] - 0s 175us/step - loss: 0.1840\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 58.79544186592102 seconds.\n",
      "neural network update time: 0.2738819122314453 seconds.\n",
      "eval score on batch: 0.18464234947172437\n",
      "epsilons agent1 and agent2: 0.24796953239241645 0.24796953239241645\n",
      "803/803 [==============================] - 0s 42us/step\n",
      "803/803 [==============================] - 0s 36us/step\n",
      "updating with 803, 803 training batch.\n",
      "Epoch 1/1\n",
      "803/803 [==============================] - 0s 165us/step - loss: 0.1859\n",
      "Epoch 1/1\n",
      "803/803 [==============================] - 0s 169us/step - loss: 0.1864\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 59.73406505584717 seconds.\n",
      "neural network update time: 0.27343320846557617 seconds.\n",
      "eval score on batch: 0.1870092716619516\n",
      "epsilons agent1 and agent2: 0.24672968473045437 0.24672968473045437\n",
      "793/793 [==============================] - 0s 43us/step\n",
      "793/793 [==============================] - 0s 38us/step\n",
      "updating with 793, 793 training batch.\n",
      "Epoch 1/1\n",
      "793/793 [==============================] - 0s 154us/step - loss: 0.1982\n",
      "Epoch 1/1\n",
      "793/793 [==============================] - 0s 154us/step - loss: 0.1981\n",
      "player 1 wins: 19\n",
      "player 2 wins: 12\n",
      "simulations took 55.42839598655701 seconds.\n",
      "neural network update time: 0.24939703941345215 seconds.\n",
      "eval score on batch: 0.19810361652852307\n",
      "epsilons agent1 and agent2: 0.24549603630680208 0.24549603630680208\n",
      "827/827 [==============================] - 0s 40us/step\n",
      "827/827 [==============================] - 0s 38us/step\n",
      "updating with 827, 827 training batch.\n",
      "Epoch 1/1\n",
      "827/827 [==============================] - 0s 153us/step - loss: 0.2010\n",
      "Epoch 1/1\n",
      "827/827 [==============================] - 0s 156us/step - loss: 0.2041\n",
      "player 1 wins: 17\n",
      "player 2 wins: 15\n",
      "simulations took 54.64769697189331 seconds.\n",
      "neural network update time: 0.2610208988189697 seconds.\n",
      "eval score on batch: 0.20410978166159002\n",
      "epsilons agent1 and agent2: 0.24426855612526807 0.24426855612526807\n",
      "710/710 [==============================] - 0s 44us/step\n",
      "710/710 [==============================] - 0s 37us/step\n",
      "updating with 710, 710 training batch.\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 161us/step - loss: 0.1986\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 158us/step - loss: 0.1987\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 55.71107006072998 seconds.\n",
      "neural network update time: 0.23148179054260254 seconds.\n",
      "eval score on batch: 0.19799068823037014\n",
      "epsilons agent1 and agent2: 0.24304721334464174 0.24304721334464174\n",
      "744/744 [==============================] - 0s 41us/step\n",
      "744/744 [==============================] - 0s 39us/step\n",
      "updating with 744, 744 training batch.\n",
      "Epoch 1/1\n",
      "744/744 [==============================] - 0s 157us/step - loss: 0.2046\n",
      "Epoch 1/1\n",
      "744/744 [==============================] - 0s 158us/step - loss: 0.2032\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 55.020519971847534 seconds.\n",
      "neural network update time: 0.24022889137268066 seconds.\n",
      "eval score on batch: 0.20387195597743235\n",
      "epsilons agent1 and agent2: 0.24183197727791853 0.24183197727791853\n",
      "60 outputs loaded.\n",
      "733/733 [==============================] - 0s 48us/step\n",
      "733/733 [==============================] - 0s 45us/step\n",
      "updating with 733, 733 training batch.\n",
      "Epoch 1/1\n",
      "733/733 [==============================] - 0s 173us/step - loss: 0.1973\n",
      "Epoch 1/1\n",
      "733/733 [==============================] - 0s 175us/step - loss: 0.1979\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 55.690897941589355 seconds.\n",
      "neural network update time: 0.2604060173034668 seconds.\n",
      "eval score on batch: 0.20757551439421565\n",
      "epsilons agent1 and agent2: 0.24062281739152894 0.24062281739152894\n",
      "798/798 [==============================] - 0s 39us/step\n",
      "798/798 [==============================] - 0s 38us/step\n",
      "updating with 798, 798 training batch.\n",
      "Epoch 1/1\n",
      "798/798 [==============================] - 0s 164us/step - loss: 0.1953\n",
      "Epoch 1/1\n",
      "798/798 [==============================] - 0s 167us/step - loss: 0.1956\n",
      "player 1 wins: 17\n",
      "player 2 wins: 13\n",
      "simulations took 56.139461040496826 seconds.\n",
      "neural network update time: 0.2703690528869629 seconds.\n",
      "eval score on batch: 0.20115603641011662\n",
      "epsilons agent1 and agent2: 0.2394197033045713 0.2394197033045713\n",
      "796/796 [==============================] - 0s 42us/step\n",
      "796/796 [==============================] - 0s 36us/step\n",
      "updating with 796, 796 training batch.\n",
      "Epoch 1/1\n",
      "796/796 [==============================] - 0s 157us/step - loss: 0.2013\n",
      "Epoch 1/1\n",
      "796/796 [==============================] - 0s 173us/step - loss: 0.2025\n",
      "player 1 wins: 13\n",
      "player 2 wins: 18\n",
      "simulations took 54.97852396965027 seconds.\n",
      "neural network update time: 0.2681112289428711 seconds.\n",
      "eval score on batch: 0.20557174887788954\n",
      "epsilons agent1 and agent2: 0.23822260478804844 0.23822260478804844\n",
      "802/802 [==============================] - 0s 42us/step\n",
      "802/802 [==============================] - 0s 41us/step\n",
      "updating with 802, 802 training batch.\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 157us/step - loss: 0.1948\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 158us/step - loss: 0.1944\n",
      "player 1 wins: 13\n",
      "player 2 wins: 17\n",
      "simulations took 56.411624908447266 seconds.\n",
      "neural network update time: 0.25820016860961914 seconds.\n",
      "eval score on batch: 0.19488094873410508\n",
      "epsilons agent1 and agent2: 0.2370314917641082 0.2370314917641082\n",
      "745/745 [==============================] - 0s 42us/step\n",
      "745/745 [==============================] - 0s 39us/step\n",
      "updating with 745, 745 training batch.\n",
      "Epoch 1/1\n",
      "745/745 [==============================] - 0s 157us/step - loss: 0.2106\n",
      "Epoch 1/1\n",
      "745/745 [==============================] - 0s 168us/step - loss: 0.2041\n",
      "player 1 wins: 16\n",
      "player 2 wins: 13\n",
      "simulations took 55.71062898635864 seconds.\n",
      "neural network update time: 0.2481081485748291 seconds.\n",
      "eval score on batch: 0.2090860618940936\n",
      "epsilons agent1 and agent2: 0.23584633430528767 0.23584633430528767\n",
      "672/672 [==============================] - 0s 40us/step\n",
      "672/672 [==============================] - 0s 42us/step\n",
      "updating with 672, 672 training batch.\n",
      "Epoch 1/1\n",
      "672/672 [==============================] - 0s 165us/step - loss: 0.1847\n",
      "Epoch 1/1\n",
      "672/672 [==============================] - 0s 164us/step - loss: 0.1869\n",
      "player 1 wins: 18\n",
      "player 2 wins: 8\n",
      "simulations took 56.03374791145325 seconds.\n",
      "neural network update time: 0.22706198692321777 seconds.\n",
      "eval score on batch: 0.19034830800124578\n",
      "epsilons agent1 and agent2: 0.23466710263376123 0.23466710263376123\n",
      "829/829 [==============================] - 0s 45us/step\n",
      "829/829 [==============================] - 0s 41us/step\n",
      "updating with 829, 829 training batch.\n",
      "Epoch 1/1\n",
      "829/829 [==============================] - 0s 162us/step - loss: 0.1943\n",
      "Epoch 1/1\n",
      "829/829 [==============================] - 0s 159us/step - loss: 0.1983\n",
      "player 1 wins: 18\n",
      "player 2 wins: 13\n",
      "simulations took 56.38348412513733 seconds.\n",
      "neural network update time: 0.27153491973876953 seconds.\n",
      "eval score on batch: 0.20281421503701458\n",
      "epsilons agent1 and agent2: 0.23349376712059242 0.23349376712059242\n",
      "737/737 [==============================] - 0s 42us/step\n",
      "737/737 [==============================] - 0s 42us/step\n",
      "updating with 737, 737 training batch.\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 158us/step - loss: 0.1884\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 161us/step - loss: 0.1845\n",
      "player 1 wins: 19\n",
      "player 2 wins: 9\n",
      "simulations took 56.34041094779968 seconds.\n",
      "neural network update time: 0.24015116691589355 seconds.\n",
      "eval score on batch: 0.18839578331247372\n",
      "epsilons agent1 and agent2: 0.23232629828498946 0.23232629828498946\n",
      "708/708 [==============================] - 0s 40us/step\n",
      "708/708 [==============================] - 0s 41us/step\n",
      "updating with 708, 708 training batch.\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 156us/step - loss: 0.1841\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 175us/step - loss: 0.1828\n",
      "player 1 wins: 20\n",
      "player 2 wins: 8\n",
      "simulations took 54.79735207557678 seconds.\n",
      "neural network update time: 0.23911070823669434 seconds.\n",
      "eval score on batch: 0.18247044797839418\n",
      "epsilons agent1 and agent2: 0.2311646667935645 0.2311646667935645\n",
      "844/844 [==============================] - 0s 42us/step\n",
      "844/844 [==============================] - 0s 39us/step\n",
      "updating with 844, 844 training batch.\n",
      "Epoch 1/1\n",
      "844/844 [==============================] - 0s 154us/step - loss: 0.1739\n",
      "Epoch 1/1\n",
      "844/844 [==============================] - 0s 159us/step - loss: 0.1726\n",
      "player 1 wins: 22\n",
      "player 2 wins: 9\n",
      "simulations took 56.815396785736084 seconds.\n",
      "neural network update time: 0.26987504959106445 seconds.\n",
      "eval score on batch: 0.17231280961894913\n",
      "epsilons agent1 and agent2: 0.23000884345959668 0.23000884345959668\n",
      "640/640 [==============================] - 0s 39us/step\n",
      "640/640 [==============================] - 0s 39us/step\n",
      "updating with 640, 640 training batch.\n",
      "Epoch 1/1\n",
      "640/640 [==============================] - 0s 152us/step - loss: 0.1592\n",
      "Epoch 1/1\n",
      "640/640 [==============================] - 0s 152us/step - loss: 0.1580\n",
      "player 1 wins: 19\n",
      "player 2 wins: 5\n",
      "simulations took 57.14509892463684 seconds.\n",
      "neural network update time: 0.19896197319030762 seconds.\n",
      "eval score on batch: 0.16065970435738564\n",
      "epsilons agent1 and agent2: 0.2288587992422987 0.2288587992422987\n",
      "784/784 [==============================] - 0s 39us/step\n",
      "784/784 [==============================] - 0s 41us/step\n",
      "updating with 784, 784 training batch.\n",
      "Epoch 1/1\n",
      "784/784 [==============================] - 0s 161us/step - loss: 0.2011\n",
      "Epoch 1/1\n",
      "784/784 [==============================] - 0s 162us/step - loss: 0.2001\n",
      "player 1 wins: 18\n",
      "player 2 wins: 12\n",
      "simulations took 55.88668203353882 seconds.\n",
      "neural network update time: 0.2593820095062256 seconds.\n",
      "eval score on batch: 0.21223413016723128\n",
      "epsilons agent1 and agent2: 0.22771450524608722 0.22771450524608722\n",
      "698/698 [==============================] - 0s 40us/step\n",
      "698/698 [==============================] - 0s 40us/step\n",
      "updating with 698, 698 training batch.\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 153us/step - loss: 0.1946\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 153us/step - loss: 0.1964\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 57.027323961257935 seconds.\n",
      "neural network update time: 0.21827483177185059 seconds.\n",
      "eval score on batch: 0.1986548262074547\n",
      "epsilons agent1 and agent2: 0.2265759327198568 0.2265759327198568\n",
      "768/768 [==============================] - 0s 41us/step\n",
      "768/768 [==============================] - 0s 37us/step\n",
      "updating with 768, 768 training batch.\n",
      "Epoch 1/1\n",
      "768/768 [==============================] - 0s 154us/step - loss: 0.2056\n",
      "Epoch 1/1\n",
      "768/768 [==============================] - 0s 164us/step - loss: 0.2048\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 55.161120891571045 seconds.\n",
      "neural network update time: 0.25012898445129395 seconds.\n",
      "eval score on batch: 0.21051757348080477\n",
      "epsilons agent1 and agent2: 0.2254430530562575 0.2254430530562575\n",
      "726/726 [==============================] - 0s 40us/step\n",
      "726/726 [==============================] - 0s 39us/step\n",
      "updating with 726, 726 training batch.\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 160us/step - loss: 0.1858\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 198us/step - loss: 0.1850\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 57.589699029922485 seconds.\n",
      "neural network update time: 0.26566100120544434 seconds.\n",
      "eval score on batch: 0.18474586390102535\n",
      "epsilons agent1 and agent2: 0.22431583779097622 0.22431583779097622\n",
      "762/762 [==============================] - 0s 44us/step\n",
      "762/762 [==============================] - 0s 38us/step\n",
      "updating with 762, 762 training batch.\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 164us/step - loss: 0.1852\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 178us/step - loss: 0.1852\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 57.407686948776245 seconds.\n",
      "neural network update time: 0.2670772075653076 seconds.\n",
      "eval score on batch: 0.18654156080263806\n",
      "epsilons agent1 and agent2: 0.22319425860202133 0.22319425860202133\n",
      "814/814 [==============================] - 0s 40us/step\n",
      "814/814 [==============================] - 0s 38us/step\n",
      "updating with 814, 814 training batch.\n",
      "Epoch 1/1\n",
      "814/814 [==============================] - 0s 162us/step - loss: 0.1909\n",
      "Epoch 1/1\n",
      "814/814 [==============================] - 0s 174us/step - loss: 0.1912\n",
      "player 1 wins: 18\n",
      "player 2 wins: 12\n",
      "simulations took 56.95590400695801 seconds.\n",
      "neural network update time: 0.2795078754425049 seconds.\n",
      "eval score on batch: 0.19546103857329494\n",
      "epsilons agent1 and agent2: 0.22207828730901122 0.22207828730901122\n",
      "746/746 [==============================] - 0s 43us/step\n",
      "746/746 [==============================] - 0s 40us/step\n",
      "updating with 746, 746 training batch.\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 163us/step - loss: 0.1994\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 157us/step - loss: 0.2013\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 56.509788036346436 seconds.\n",
      "neural network update time: 0.24419116973876953 seconds.\n",
      "eval score on batch: 0.20886899415034071\n",
      "epsilons agent1 and agent2: 0.22096789587246615 0.22096789587246615\n",
      "713/713 [==============================] - 0s 45us/step\n",
      "713/713 [==============================] - 0s 45us/step\n",
      "updating with 713, 713 training batch.\n",
      "Epoch 1/1\n",
      "713/713 [==============================] - 0s 183us/step - loss: 0.2057\n",
      "Epoch 1/1\n",
      "713/713 [==============================] - 0s 184us/step - loss: 0.2066\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 55.0246160030365 seconds.\n",
      "neural network update time: 0.26776599884033203 seconds.\n",
      "eval score on batch: 0.20861395741035693\n",
      "epsilons agent1 and agent2: 0.21986305639310383 0.21986305639310383\n",
      "707/707 [==============================] - 0s 43us/step\n",
      "707/707 [==============================] - 0s 40us/step\n",
      "updating with 707, 707 training batch.\n",
      "Epoch 1/1\n",
      "707/707 [==============================] - 0s 156us/step - loss: 0.1899\n",
      "Epoch 1/1\n",
      "707/707 [==============================] - 0s 158us/step - loss: 0.1898\n",
      "player 1 wins: 18\n",
      "player 2 wins: 9\n",
      "simulations took 56.0182089805603 seconds.\n",
      "neural network update time: 0.22780585289001465 seconds.\n",
      "eval score on batch: 0.19202966400049362\n",
      "epsilons agent1 and agent2: 0.2187637411111383 0.2187637411111383\n",
      "61 outputs loaded.\n",
      "687/687 [==============================] - 0s 43us/step\n",
      "687/687 [==============================] - 0s 40us/step\n",
      "updating with 687, 687 training batch.\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 155us/step - loss: 0.2138\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 168us/step - loss: 0.2141\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 53.75655913352966 seconds.\n",
      "neural network update time: 0.2268679141998291 seconds.\n",
      "eval score on batch: 0.21944605290152275\n",
      "epsilons agent1 and agent2: 0.2176699224055826 0.2176699224055826\n",
      "832/832 [==============================] - 0s 39us/step\n",
      "832/832 [==============================] - 0s 38us/step\n",
      "updating with 832, 832 training batch.\n",
      "Epoch 1/1\n",
      "832/832 [==============================] - 0s 151us/step - loss: 0.1864\n",
      "Epoch 1/1\n",
      "832/832 [==============================] - 0s 152us/step - loss: 0.1872\n",
      "player 1 wins: 18\n",
      "player 2 wins: 12\n",
      "simulations took 56.71291494369507 seconds.\n",
      "neural network update time: 0.2582740783691406 seconds.\n",
      "eval score on batch: 0.18914918945385858\n",
      "epsilons agent1 and agent2: 0.2165815727935547 0.2165815727935547\n",
      "711/711 [==============================] - 0s 47us/step\n",
      "711/711 [==============================] - 0s 38us/step\n",
      "updating with 711, 711 training batch.\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 156us/step - loss: 0.2023\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 158us/step - loss: 0.2004\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 55.425989866256714 seconds.\n",
      "neural network update time: 0.22853398323059082 seconds.\n",
      "eval score on batch: 0.19999129983233668\n",
      "epsilons agent1 and agent2: 0.21549866492958691 0.21549866492958691\n",
      "779/779 [==============================] - 0s 42us/step\n",
      "779/779 [==============================] - 0s 39us/step\n",
      "updating with 779, 779 training batch.\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 163us/step - loss: 0.1948\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 168us/step - loss: 0.1963\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 56.692625999450684 seconds.\n",
      "neural network update time: 0.26384520530700684 seconds.\n",
      "eval score on batch: 0.19798188677609962\n",
      "epsilons agent1 and agent2: 0.21442117160493898 0.21442117160493898\n",
      "720/720 [==============================] - 0s 40us/step\n",
      "720/720 [==============================] - 0s 45us/step\n",
      "updating with 720, 720 training batch.\n",
      "Epoch 1/1\n",
      "720/720 [==============================] - 0s 187us/step - loss: 0.2041\n",
      "Epoch 1/1\n",
      "720/720 [==============================] - 0s 178us/step - loss: 0.2016\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 55.14496088027954 seconds.\n",
      "neural network update time: 0.26932692527770996 seconds.\n",
      "eval score on batch: 0.20211789674229091\n",
      "epsilons agent1 and agent2: 0.21334906574691428 0.21334906574691428\n",
      "731/731 [==============================] - 0s 42us/step\n",
      "731/731 [==============================] - 0s 37us/step\n",
      "updating with 731, 731 training batch.\n",
      "Epoch 1/1\n",
      "731/731 [==============================] - 0s 155us/step - loss: 0.1812\n",
      "Epoch 1/1\n",
      "731/731 [==============================] - 0s 152us/step - loss: 0.1826\n",
      "player 1 wins: 18\n",
      "player 2 wins: 9\n",
      "simulations took 57.10273003578186 seconds.\n",
      "neural network update time: 0.23012709617614746 seconds.\n",
      "eval score on batch: 0.18343428433675282\n",
      "epsilons agent1 and agent2: 0.2122823204181797 0.2122823204181797\n",
      "665/665 [==============================] - 0s 44us/step\n",
      "665/665 [==============================] - 0s 37us/step\n",
      "updating with 665, 665 training batch.\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 166us/step - loss: 0.1889\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 160us/step - loss: 0.1891\n",
      "player 1 wins: 19\n",
      "player 2 wins: 8\n",
      "simulations took 56.482253074645996 seconds.\n",
      "neural network update time: 0.22170281410217285 seconds.\n",
      "eval score on batch: 0.18893289268016816\n",
      "epsilons agent1 and agent2: 0.21122090881608882 0.21122090881608882\n",
      "918/918 [==============================] - 0s 43us/step\n",
      "918/918 [==============================] - 0s 40us/step\n",
      "updating with 918, 918 training batch.\n",
      "Epoch 1/1\n",
      "918/918 [==============================] - 0s 161us/step - loss: 0.1866\n",
      "Epoch 1/1\n",
      "918/918 [==============================] - 0s 164us/step - loss: 0.1879\n",
      "player 1 wins: 16\n",
      "player 2 wins: 16\n",
      "simulations took 57.32790398597717 seconds.\n",
      "neural network update time: 0.30527424812316895 seconds.\n",
      "eval score on batch: 0.20715310591555045\n",
      "epsilons agent1 and agent2: 0.21016480427200837 0.21016480427200837\n",
      "698/698 [==============================] - 0s 40us/step\n",
      "698/698 [==============================] - 0s 38us/step\n",
      "updating with 698, 698 training batch.\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 155us/step - loss: 0.1951\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 151us/step - loss: 0.1935\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 57.95676016807556 seconds.\n",
      "neural network update time: 0.2189500331878662 seconds.\n",
      "eval score on batch: 0.19814805829507914\n",
      "epsilons agent1 and agent2: 0.20911398025064834 0.20911398025064834\n",
      "714/714 [==============================] - 0s 48us/step\n",
      "714/714 [==============================] - 0s 48us/step\n",
      "updating with 714, 714 training batch.\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 174us/step - loss: 0.1525\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 180us/step - loss: 0.1525\n",
      "player 1 wins: 21\n",
      "player 2 wins: 5\n",
      "simulations took 57.61730623245239 seconds.\n",
      "neural network update time: 0.2588181495666504 seconds.\n",
      "eval score on batch: 0.16370753815977415\n",
      "epsilons agent1 and agent2: 0.2080684103493951 0.2080684103493951\n",
      "749/749 [==============================] - 0s 45us/step\n",
      "749/749 [==============================] - 0s 39us/step\n",
      "updating with 749, 749 training batch.\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 159us/step - loss: 0.1774\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 157us/step - loss: 0.1762\n",
      "player 1 wins: 20\n",
      "player 2 wins: 8\n",
      "simulations took 55.656574010849 seconds.\n",
      "neural network update time: 0.24166393280029297 seconds.\n",
      "eval score on batch: 0.183855412619932\n",
      "epsilons agent1 and agent2: 0.20702806829764814 0.20702806829764814\n",
      "763/763 [==============================] - 0s 43us/step\n",
      "763/763 [==============================] - 0s 37us/step\n",
      "updating with 763, 763 training batch.\n",
      "Epoch 1/1\n",
      "763/763 [==============================] - 0s 154us/step - loss: 0.1452\n",
      "Epoch 1/1\n",
      "763/763 [==============================] - 0s 155us/step - loss: 0.1473\n",
      "player 1 wins: 23\n",
      "player 2 wins: 5\n",
      "simulations took 56.78840088844299 seconds.\n",
      "neural network update time: 0.24093174934387207 seconds.\n",
      "eval score on batch: 0.15345380190628072\n",
      "epsilons agent1 and agent2: 0.2059929279561599 0.2059929279561599\n",
      "555/555 [==============================] - 0s 42us/step\n",
      "555/555 [==============================] - 0s 41us/step\n",
      "updating with 555, 555 training batch.\n",
      "Epoch 1/1\n",
      "555/555 [==============================] - 0s 162us/step - loss: 0.2324\n",
      "Epoch 1/1\n",
      "555/555 [==============================] - 0s 162us/step - loss: 0.2333\n",
      "player 1 wins: 10\n",
      "player 2 wins: 12\n",
      "simulations took 57.73207998275757 seconds.\n",
      "neural network update time: 0.1842801570892334 seconds.\n",
      "eval score on batch: 0.2807072745787131\n",
      "epsilons agent1 and agent2: 0.2049629633163791 0.2049629633163791\n",
      "724/724 [==============================] - 0s 44us/step\n",
      "724/724 [==============================] - 0s 65us/step\n",
      "updating with 724, 724 training batch.\n",
      "Epoch 1/1\n",
      "724/724 [==============================] - 0s 164us/step - loss: 0.1926\n",
      "Epoch 1/1\n",
      "724/724 [==============================] - 0s 154us/step - loss: 0.1909\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 56.840702056884766 seconds.\n",
      "neural network update time: 0.23621296882629395 seconds.\n",
      "eval score on batch: 0.19311272178503855\n",
      "epsilons agent1 and agent2: 0.2039381484997972 0.2039381484997972\n",
      "763/763 [==============================] - 0s 39us/step\n",
      "763/763 [==============================] - 0s 42us/step\n",
      "updating with 763, 763 training batch.\n",
      "Epoch 1/1\n",
      "763/763 [==============================] - 0s 156us/step - loss: 0.1828\n",
      "Epoch 1/1\n",
      "763/763 [==============================] - 0s 156us/step - loss: 0.1854\n",
      "player 1 wins: 20\n",
      "player 2 wins: 9\n",
      "simulations took 55.043084144592285 seconds.\n",
      "neural network update time: 0.2430129051208496 seconds.\n",
      "eval score on batch: 0.1952051303372627\n",
      "epsilons agent1 and agent2: 0.2029184577572982 0.2029184577572982\n",
      "659/659 [==============================] - 0s 44us/step\n",
      "659/659 [==============================] - 0s 42us/step\n",
      "updating with 659, 659 training batch.\n",
      "Epoch 1/1\n",
      "659/659 [==============================] - 0s 164us/step - loss: 0.2091\n",
      "Epoch 1/1\n",
      "659/659 [==============================] - 0s 168us/step - loss: 0.2092\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 56.027132749557495 seconds.\n",
      "neural network update time: 0.22391819953918457 seconds.\n",
      "eval score on batch: 0.2239847014317563\n",
      "epsilons agent1 and agent2: 0.2019038654685117 0.2019038654685117\n",
      "776/776 [==============================] - 0s 54us/step\n",
      "776/776 [==============================] - 0s 52us/step\n",
      "updating with 776, 776 training batch.\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 163us/step - loss: 0.1967\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 159us/step - loss: 0.1954\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 55.611489057540894 seconds.\n",
      "neural network update time: 0.2554512023925781 seconds.\n",
      "eval score on batch: 0.1962033221402119\n",
      "epsilons agent1 and agent2: 0.20089434614116913 0.20089434614116913\n",
      "794/794 [==============================] - 0s 46us/step\n",
      "794/794 [==============================] - 0s 44us/step\n",
      "updating with 794, 794 training batch.\n",
      "Epoch 1/1\n",
      "794/794 [==============================] - 0s 157us/step - loss: 0.1776\n",
      "Epoch 1/1\n",
      "794/794 [==============================] - 0s 155us/step - loss: 0.1783\n",
      "player 1 wins: 18\n",
      "player 2 wins: 10\n",
      "simulations took 56.89202117919922 seconds.\n",
      "neural network update time: 0.2537961006164551 seconds.\n",
      "eval score on batch: 0.18368914809725448\n",
      "epsilons agent1 and agent2: 0.1998898744104633 0.1998898744104633\n",
      "701/701 [==============================] - 0s 41us/step\n",
      "701/701 [==============================] - 0s 40us/step\n",
      "updating with 701, 701 training batch.\n",
      "Epoch 1/1\n",
      "701/701 [==============================] - 0s 162us/step - loss: 0.1770\n",
      "Epoch 1/1\n",
      "701/701 [==============================] - 0s 158us/step - loss: 0.1770\n",
      "player 1 wins: 18\n",
      "player 2 wins: 8\n",
      "simulations took 56.40184426307678 seconds.\n",
      "neural network update time: 0.2294151782989502 seconds.\n",
      "eval score on batch: 0.17674928886574107\n",
      "epsilons agent1 and agent2: 0.19889042503841098 0.19889042503841098\n",
      "644/644 [==============================] - 0s 44us/step\n",
      "644/644 [==============================] - 0s 41us/step\n",
      "updating with 644, 644 training batch.\n",
      "Epoch 1/1\n",
      "644/644 [==============================] - 0s 165us/step - loss: 0.2188\n",
      "Epoch 1/1\n",
      "644/644 [==============================] - 0s 164us/step - loss: 0.2230\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 54.66705775260925 seconds.\n",
      "neural network update time: 0.21713805198669434 seconds.\n",
      "eval score on batch: 0.24441212864581102\n",
      "epsilons agent1 and agent2: 0.19789597291321892 0.19789597291321892\n",
      "62 outputs loaded.\n",
      "767/767 [==============================] - 0s 39us/step\n",
      "767/767 [==============================] - 0s 38us/step\n",
      "updating with 767, 767 training batch.\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 154us/step - loss: 0.1868\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 152us/step - loss: 0.1867\n",
      "player 1 wins: 11\n",
      "player 2 wins: 17\n",
      "simulations took 56.30913186073303 seconds.\n",
      "neural network update time: 0.24014997482299805 seconds.\n",
      "eval score on batch: 0.1861149743782184\n",
      "epsilons agent1 and agent2: 0.19690649304865282 0.19690649304865282\n",
      "781/781 [==============================] - 0s 46us/step\n",
      "781/781 [==============================] - 0s 38us/step\n",
      "updating with 781, 781 training batch.\n",
      "Epoch 1/1\n",
      "781/781 [==============================] - 0s 154us/step - loss: 0.1937\n",
      "Epoch 1/1\n",
      "781/781 [==============================] - 0s 155us/step - loss: 0.1934\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 55.63830304145813 seconds.\n",
      "neural network update time: 0.2490081787109375 seconds.\n",
      "eval score on batch: 0.19358384463263534\n",
      "epsilons agent1 and agent2: 0.19592196058340955 0.19592196058340955\n",
      "701/701 [==============================] - 0s 51us/step\n",
      "701/701 [==============================] - 0s 39us/step\n",
      "updating with 701, 701 training batch.\n",
      "Epoch 1/1\n",
      "701/701 [==============================] - 0s 156us/step - loss: 0.1906\n",
      "Epoch 1/1\n",
      "701/701 [==============================] - 0s 159us/step - loss: 0.1908\n",
      "player 1 wins: 20\n",
      "player 2 wins: 8\n",
      "simulations took 54.246026277542114 seconds.\n",
      "neural network update time: 0.22598004341125488 seconds.\n",
      "eval score on batch: 0.21823495958058878\n",
      "epsilons agent1 and agent2: 0.1949423507804925 0.1949423507804925\n",
      "704/704 [==============================] - 0s 40us/step\n",
      "704/704 [==============================] - 0s 38us/step\n",
      "updating with 704, 704 training batch.\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 151us/step - loss: 0.2096\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 154us/step - loss: 0.2112\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 56.111685037612915 seconds.\n",
      "neural network update time: 0.2199099063873291 seconds.\n",
      "eval score on batch: 0.23269260065122085\n",
      "epsilons agent1 and agent2: 0.19396763902659003 0.19396763902659003\n",
      "743/743 [==============================] - 0s 40us/step\n",
      "743/743 [==============================] - 0s 38us/step\n",
      "updating with 743, 743 training batch.\n",
      "Epoch 1/1\n",
      "743/743 [==============================] - 0s 172us/step - loss: 0.1886\n",
      "Epoch 1/1\n",
      "743/743 [==============================] - 0s 164us/step - loss: 0.1886\n",
      "player 1 wins: 19\n",
      "player 2 wins: 9\n",
      "simulations took 57.716235876083374 seconds.\n",
      "neural network update time: 0.25540828704833984 seconds.\n",
      "eval score on batch: 0.20049679936455064\n",
      "epsilons agent1 and agent2: 0.1929978008314571 0.1929978008314571\n",
      "695/695 [==============================] - 0s 42us/step\n",
      "695/695 [==============================] - 0s 45us/step\n",
      "updating with 695, 695 training batch.\n",
      "Epoch 1/1\n",
      "695/695 [==============================] - 0s 155us/step - loss: 0.2152\n",
      "Epoch 1/1\n",
      "695/695 [==============================] - 0s 150us/step - loss: 0.2145\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 55.449191093444824 seconds.\n",
      "neural network update time: 0.2178959846496582 seconds.\n",
      "eval score on batch: 0.22624920896462186\n",
      "epsilons agent1 and agent2: 0.1920328118272998 0.1920328118272998\n",
      "770/770 [==============================] - 0s 71us/step\n",
      "770/770 [==============================] - 0s 73us/step\n",
      "updating with 770, 770 training batch.\n",
      "Epoch 1/1\n",
      "770/770 [==============================] - 0s 320us/step - loss: 0.1976\n",
      "Epoch 1/1\n",
      "770/770 [==============================] - 0s 240us/step - loss: 0.1979\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 60.23843717575073 seconds.\n",
      "neural network update time: 0.4402010440826416 seconds.\n",
      "eval score on batch: 0.19773138628474304\n",
      "epsilons agent1 and agent2: 0.19107264776816332 0.19107264776816332\n",
      "661/661 [==============================] - 0s 42us/step\n",
      "661/661 [==============================] - 0s 37us/step\n",
      "updating with 661, 661 training batch.\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 155us/step - loss: 0.2061\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 153us/step - loss: 0.2052\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 57.20376515388489 seconds.\n",
      "neural network update time: 0.2088479995727539 seconds.\n",
      "eval score on batch: 0.2063678117995904\n",
      "epsilons agent1 and agent2: 0.1901172845293225 0.1901172845293225\n",
      "653/653 [==============================] - 0s 41us/step\n",
      "653/653 [==============================] - 0s 45us/step\n",
      "updating with 653, 653 training batch.\n",
      "Epoch 1/1\n",
      "653/653 [==============================] - 0s 158us/step - loss: 0.2031\n",
      "Epoch 1/1\n",
      "653/653 [==============================] - 0s 162us/step - loss: 0.2028\n",
      "player 1 wins: 17\n",
      "player 2 wins: 9\n",
      "simulations took 56.723520040512085 seconds.\n",
      "neural network update time: 0.2149057388305664 seconds.\n",
      "eval score on batch: 0.2179484780004981\n",
      "epsilons agent1 and agent2: 0.18916669810667588 0.18916669810667588\n",
      "746/746 [==============================] - 0s 43us/step\n",
      "746/746 [==============================] - 0s 38us/step\n",
      "updating with 746, 746 training batch.\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 154us/step - loss: 0.1529\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 154us/step - loss: 0.1528\n",
      "player 1 wins: 21\n",
      "player 2 wins: 6\n",
      "simulations took 59.21564602851868 seconds.\n",
      "neural network update time: 0.23557806015014648 seconds.\n",
      "eval score on batch: 0.1538255245602083\n",
      "epsilons agent1 and agent2: 0.1882208646161425 0.1882208646161425\n",
      "787/787 [==============================] - 0s 38us/step\n",
      "787/787 [==============================] - 0s 37us/step\n",
      "updating with 787, 787 training batch.\n",
      "Epoch 1/1\n",
      "787/787 [==============================] - 0s 160us/step - loss: 0.2082\n",
      "Epoch 1/1\n",
      "787/787 [==============================] - 0s 157us/step - loss: 0.2081\n",
      "player 1 wins: 12\n",
      "player 2 wins: 17\n",
      "simulations took 59.59090304374695 seconds.\n",
      "neural network update time: 0.2553749084472656 seconds.\n",
      "eval score on batch: 0.2570687267347091\n",
      "epsilons agent1 and agent2: 0.1872797602930618 0.1872797602930618\n",
      "689/689 [==============================] - 0s 43us/step\n",
      "689/689 [==============================] - 0s 52us/step\n",
      "updating with 689, 689 training batch.\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 155us/step - loss: 0.2164\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 154us/step - loss: 0.2121\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 58.014695167541504 seconds.\n",
      "neural network update time: 0.2188091278076172 seconds.\n",
      "eval score on batch: 0.23229473316366142\n",
      "epsilons agent1 and agent2: 0.18634336149159647 0.18634336149159647\n",
      "801/801 [==============================] - 0s 40us/step\n",
      "801/801 [==============================] - 0s 38us/step\n",
      "updating with 801, 801 training batch.\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s 157us/step - loss: 0.1941\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s 157us/step - loss: 0.1910\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 57.760159969329834 seconds.\n",
      "neural network update time: 0.25667595863342285 seconds.\n",
      "eval score on batch: 0.19341275962252755\n",
      "epsilons agent1 and agent2: 0.18541164468413848 0.18541164468413848\n",
      "785/785 [==============================] - 0s 43us/step\n",
      "785/785 [==============================] - 0s 39us/step\n",
      "updating with 785, 785 training batch.\n",
      "Epoch 1/1\n",
      "785/785 [==============================] - 0s 160us/step - loss: 0.1935\n",
      "Epoch 1/1\n",
      "785/785 [==============================] - 0s 174us/step - loss: 0.1929\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 58.48607516288757 seconds.\n",
      "neural network update time: 0.2679457664489746 seconds.\n",
      "eval score on batch: 0.19605636944057075\n",
      "epsilons agent1 and agent2: 0.18448458646071778 0.18448458646071778\n",
      "716/716 [==============================] - 0s 41us/step\n",
      "716/716 [==============================] - 0s 38us/step\n",
      "updating with 716, 716 training batch.\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 153us/step - loss: 0.1901\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 154us/step - loss: 0.1901\n",
      "player 1 wins: 17\n",
      "player 2 wins: 10\n",
      "simulations took 57.319380044937134 seconds.\n",
      "neural network update time: 0.22503185272216797 seconds.\n",
      "eval score on batch: 0.18939506719877433\n",
      "epsilons agent1 and agent2: 0.18356216352841417 0.18356216352841417\n",
      "700/700 [==============================] - 0s 51us/step\n",
      "700/700 [==============================] - 0s 49us/step\n",
      "updating with 700, 700 training batch.\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 170us/step - loss: 0.1786\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 157us/step - loss: 0.1772\n",
      "player 1 wins: 21\n",
      "player 2 wins: 7\n",
      "simulations took 54.98836302757263 seconds.\n",
      "neural network update time: 0.23510217666625977 seconds.\n",
      "eval score on batch: 0.18070733615330287\n",
      "epsilons agent1 and agent2: 0.1826443527107721 0.1826443527107721\n",
      "766/766 [==============================] - 0s 40us/step\n",
      "766/766 [==============================] - 0s 37us/step\n",
      "updating with 766, 766 training batch.\n",
      "Epoch 1/1\n",
      "766/766 [==============================] - 0s 151us/step - loss: 0.1973\n",
      "Epoch 1/1\n",
      "766/766 [==============================] - 0s 154us/step - loss: 0.1971\n",
      "player 1 wins: 18\n",
      "player 2 wins: 11\n",
      "simulations took 56.277381896972656 seconds.\n",
      "neural network update time: 0.23953890800476074 seconds.\n",
      "eval score on batch: 0.20334650533317586\n",
      "epsilons agent1 and agent2: 0.18173113094721824 0.18173113094721824\n",
      "818/818 [==============================] - 0s 41us/step\n",
      "818/818 [==============================] - 0s 36us/step\n",
      "updating with 818, 818 training batch.\n",
      "Epoch 1/1\n",
      "818/818 [==============================] - 0s 161us/step - loss: 0.1859\n",
      "Epoch 1/1\n",
      "818/818 [==============================] - 0s 156us/step - loss: 0.1871\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 57.6531138420105 seconds.\n",
      "neural network update time: 0.26476168632507324 seconds.\n",
      "eval score on batch: 0.1888042834036913\n",
      "epsilons agent1 and agent2: 0.18082247529248216 0.18082247529248216\n",
      "651/651 [==============================] - 0s 41us/step\n",
      "651/651 [==============================] - 0s 40us/step\n",
      "updating with 651, 651 training batch.\n",
      "Epoch 1/1\n",
      "651/651 [==============================] - 0s 162us/step - loss: 0.1978\n",
      "Epoch 1/1\n",
      "651/651 [==============================] - 0s 158us/step - loss: 0.1974\n",
      "player 1 wins: 15\n",
      "player 2 wins: 10\n",
      "simulations took 56.406688928604126 seconds.\n",
      "neural network update time: 0.21343111991882324 seconds.\n",
      "eval score on batch: 0.20004053513867104\n",
      "epsilons agent1 and agent2: 0.17991836291601976 0.17991836291601976\n",
      "756/756 [==============================] - 0s 50us/step\n",
      "756/756 [==============================] - 0s 57us/step\n",
      "updating with 756, 756 training batch.\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 211us/step - loss: 0.2036\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 189us/step - loss: 0.2040\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 56.30940294265747 seconds.\n",
      "neural network update time: 0.31055116653442383 seconds.\n",
      "eval score on batch: 0.21208368856755513\n",
      "epsilons agent1 and agent2: 0.17901877110143966 0.17901877110143966\n",
      "63 outputs loaded.\n",
      "661/661 [==============================] - 0s 40us/step\n",
      "661/661 [==============================] - 0s 41us/step\n",
      "updating with 661, 661 training batch.\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 163us/step - loss: 0.2039\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 151us/step - loss: 0.1996\n",
      "player 1 wins: 17\n",
      "player 2 wins: 9\n",
      "simulations took 54.60651087760925 seconds.\n",
      "neural network update time: 0.21315407752990723 seconds.\n",
      "eval score on batch: 0.22189246823675873\n",
      "epsilons agent1 and agent2: 0.17812367724593245 0.17812367724593245\n",
      "729/729 [==============================] - 0s 39us/step\n",
      "729/729 [==============================] - 0s 38us/step\n",
      "updating with 729, 729 training batch.\n",
      "Epoch 1/1\n",
      "729/729 [==============================] - 0s 159us/step - loss: 0.2082\n",
      "Epoch 1/1\n",
      "729/729 [==============================] - 0s 149us/step - loss: 0.2046\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 57.6430389881134 seconds.\n",
      "neural network update time: 0.2301502227783203 seconds.\n",
      "eval score on batch: 0.23098043122409304\n",
      "epsilons agent1 and agent2: 0.17723305885970278 0.17723305885970278\n",
      "786/786 [==============================] - 0s 39us/step\n",
      "786/786 [==============================] - 0s 39us/step\n",
      "updating with 786, 786 training batch.\n",
      "Epoch 1/1\n",
      "786/786 [==============================] - 0s 171us/step - loss: 0.1996\n",
      "Epoch 1/1\n",
      "786/786 [==============================] - 0s 154us/step - loss: 0.2005\n",
      "player 1 wins: 19\n",
      "player 2 wins: 11\n",
      "simulations took 57.102174043655396 seconds.\n",
      "neural network update time: 0.26115989685058594 seconds.\n",
      "eval score on batch: 0.22800710603936028\n",
      "epsilons agent1 and agent2: 0.17634689356540426 0.17634689356540426\n",
      "798/798 [==============================] - 0s 39us/step\n",
      "798/798 [==============================] - 0s 38us/step\n",
      "updating with 798, 798 training batch.\n",
      "Epoch 1/1\n",
      "798/798 [==============================] - 0s 151us/step - loss: 0.1963\n",
      "Epoch 1/1\n",
      "798/798 [==============================] - 0s 151us/step - loss: 0.1944\n",
      "player 1 wins: 10\n",
      "player 2 wins: 19\n",
      "simulations took 56.721599817276 seconds.\n",
      "neural network update time: 0.2459719181060791 seconds.\n",
      "eval score on batch: 0.24667556990955705\n",
      "epsilons agent1 and agent2: 0.17546515909757723 0.17546515909757723\n",
      "689/689 [==============================] - 0s 46us/step\n",
      "689/689 [==============================] - 0s 42us/step\n",
      "updating with 689, 689 training batch.\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 154us/step - loss: 0.2200\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 154us/step - loss: 0.2215\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 54.571717262268066 seconds.\n",
      "neural network update time: 0.21758294105529785 seconds.\n",
      "eval score on batch: 0.25783061342995145\n",
      "epsilons agent1 and agent2: 0.17458783330208935 0.17458783330208935\n",
      "658/658 [==============================] - 0s 41us/step\n",
      "658/658 [==============================] - 0s 46us/step\n",
      "updating with 658, 658 training batch.\n",
      "Epoch 1/1\n",
      "658/658 [==============================] - 0s 154us/step - loss: 0.2118\n",
      "Epoch 1/1\n",
      "658/658 [==============================] - 0s 152us/step - loss: 0.2132\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 54.00410079956055 seconds.\n",
      "neural network update time: 0.20664095878601074 seconds.\n",
      "eval score on batch: 0.227001029035603\n",
      "epsilons agent1 and agent2: 0.1737148941355789 0.1737148941355789\n",
      "614/614 [==============================] - 0s 44us/step\n",
      "614/614 [==============================] - 0s 43us/step\n",
      "updating with 614, 614 training batch.\n",
      "Epoch 1/1\n",
      "614/614 [==============================] - 0s 155us/step - loss: 0.1991\n",
      "Epoch 1/1\n",
      "614/614 [==============================] - 0s 163us/step - loss: 0.1973\n",
      "player 1 wins: 12\n",
      "player 2 wins: 11\n",
      "simulations took 56.747719049453735 seconds.\n",
      "neural network update time: 0.20074892044067383 seconds.\n",
      "eval score on batch: 0.200380377495716\n",
      "epsilons agent1 and agent2: 0.172846319664901 0.172846319664901\n",
      "720/720 [==============================] - 0s 43us/step\n",
      "720/720 [==============================] - 0s 38us/step\n",
      "updating with 720, 720 training batch.\n",
      "Epoch 1/1\n",
      "720/720 [==============================] - 0s 152us/step - loss: 0.1867\n",
      "Epoch 1/1\n",
      "720/720 [==============================] - 0s 163us/step - loss: 0.1868\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 55.5536208152771 seconds.\n",
      "neural network update time: 0.23191094398498535 seconds.\n",
      "eval score on batch: 0.18612337569809623\n",
      "epsilons agent1 and agent2: 0.1719820880665765 0.1719820880665765\n",
      "781/781 [==============================] - 0s 42us/step\n",
      "781/781 [==============================] - 0s 40us/step\n",
      "updating with 781, 781 training batch.\n",
      "Epoch 1/1\n",
      "781/781 [==============================] - 0s 153us/step - loss: 0.1922\n",
      "Epoch 1/1\n",
      "781/781 [==============================] - 0s 153us/step - loss: 0.1903\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 56.93253707885742 seconds.\n",
      "neural network update time: 0.24452877044677734 seconds.\n",
      "eval score on batch: 0.19442631356732945\n",
      "epsilons agent1 and agent2: 0.17112217762624363 0.17112217762624363\n",
      "860/860 [==============================] - 0s 47us/step\n",
      "860/860 [==============================] - 0s 41us/step\n",
      "updating with 860, 860 training batch.\n",
      "Epoch 1/1\n",
      "860/860 [==============================] - 0s 156us/step - loss: 0.1862\n",
      "Epoch 1/1\n",
      "860/860 [==============================] - 0s 153us/step - loss: 0.1818\n",
      "player 1 wins: 22\n",
      "player 2 wins: 10\n",
      "simulations took 55.13242173194885 seconds.\n",
      "neural network update time: 0.2713930606842041 seconds.\n",
      "eval score on batch: 0.20518414031627566\n",
      "epsilons agent1 and agent2: 0.1702665667381124 0.1702665667381124\n",
      "745/745 [==============================] - 0s 44us/step\n",
      "745/745 [==============================] - 0s 36us/step\n",
      "updating with 745, 745 training batch.\n",
      "Epoch 1/1\n",
      "745/745 [==============================] - 0s 157us/step - loss: 0.1983\n",
      "Epoch 1/1\n",
      "745/745 [==============================] - 0s 158us/step - loss: 0.2068\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 56.14601707458496 seconds.\n",
      "neural network update time: 0.23939204216003418 seconds.\n",
      "eval score on batch: 0.22845024292626037\n",
      "epsilons agent1 and agent2: 0.16941523390442184 0.16941523390442184\n",
      "652/652 [==============================] - 0s 40us/step\n",
      "652/652 [==============================] - 0s 39us/step\n",
      "updating with 652, 652 training batch.\n",
      "Epoch 1/1\n",
      "652/652 [==============================] - 0s 157us/step - loss: 0.2019\n",
      "Epoch 1/1\n",
      "652/652 [==============================] - 0s 155us/step - loss: 0.2016\n",
      "player 1 wins: 13\n",
      "player 2 wins: 12\n",
      "simulations took 55.61240482330322 seconds.\n",
      "neural network update time: 0.20883703231811523 seconds.\n",
      "eval score on batch: 0.2015576256107699\n",
      "epsilons agent1 and agent2: 0.16856815773489972 0.16856815773489972\n",
      "813/813 [==============================] - 0s 42us/step\n",
      "813/813 [==============================] - 0s 42us/step\n",
      "updating with 813, 813 training batch.\n",
      "Epoch 1/1\n",
      "813/813 [==============================] - 0s 160us/step - loss: 0.1903\n",
      "Epoch 1/1\n",
      "813/813 [==============================] - 0s 166us/step - loss: 0.1904\n",
      "player 1 wins: 18\n",
      "player 2 wins: 12\n",
      "simulations took 55.972667932510376 seconds.\n",
      "neural network update time: 0.2709817886352539 seconds.\n",
      "eval score on batch: 0.19404394941136405\n",
      "epsilons agent1 and agent2: 0.16772531694622522 0.16772531694622522\n",
      "769/769 [==============================] - 0s 40us/step\n",
      "769/769 [==============================] - 0s 39us/step\n",
      "updating with 769, 769 training batch.\n",
      "Epoch 1/1\n",
      "769/769 [==============================] - 0s 161us/step - loss: 0.2023\n",
      "Epoch 1/1\n",
      "769/769 [==============================] - 0s 163us/step - loss: 0.2020\n",
      "player 1 wins: 17\n",
      "player 2 wins: 13\n",
      "simulations took 53.7296040058136 seconds.\n",
      "neural network update time: 0.254835844039917 seconds.\n",
      "eval score on batch: 0.2022887492365893\n",
      "epsilons agent1 and agent2: 0.1668866903614941 0.1668866903614941\n",
      "732/732 [==============================] - 0s 45us/step\n",
      "732/732 [==============================] - 0s 42us/step\n",
      "updating with 732, 732 training batch.\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 167us/step - loss: 0.1852\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 164us/step - loss: 0.1862\n",
      "player 1 wins: 17\n",
      "player 2 wins: 10\n",
      "simulations took 57.91702389717102 seconds.\n",
      "neural network update time: 0.24848723411560059 seconds.\n",
      "eval score on batch: 0.1876560349444874\n",
      "epsilons agent1 and agent2: 0.1660522569096866 0.1660522569096866\n",
      "764/764 [==============================] - 0s 38us/step\n",
      "764/764 [==============================] - 0s 40us/step\n",
      "updating with 764, 764 training batch.\n",
      "Epoch 1/1\n",
      "764/764 [==============================] - 0s 165us/step - loss: 0.1924\n",
      "Epoch 1/1\n",
      "764/764 [==============================] - 0s 167us/step - loss: 0.1926\n",
      "player 1 wins: 18\n",
      "player 2 wins: 11\n",
      "simulations took 57.183525800704956 seconds.\n",
      "neural network update time: 0.2587149143218994 seconds.\n",
      "eval score on batch: 0.19183593645145755\n",
      "epsilons agent1 and agent2: 0.16522199562513817 0.16522199562513817\n",
      "802/802 [==============================] - 0s 61us/step\n",
      "802/802 [==============================] - 0s 54us/step\n",
      "updating with 802, 802 training batch.\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 194us/step - loss: 0.1786\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 173us/step - loss: 0.1784\n",
      "player 1 wins: 19\n",
      "player 2 wins: 10\n",
      "simulations took 57.7489857673645 seconds.\n",
      "neural network update time: 0.30087876319885254 seconds.\n",
      "eval score on batch: 0.17829289935886897\n",
      "epsilons agent1 and agent2: 0.1643958856470125 0.1643958856470125\n",
      "762/762 [==============================] - 0s 40us/step\n",
      "762/762 [==============================] - 0s 46us/step\n",
      "updating with 762, 762 training batch.\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 155us/step - loss: 0.2052\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 164us/step - loss: 0.2058\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 56.764657974243164 seconds.\n",
      "neural network update time: 0.24843096733093262 seconds.\n",
      "eval score on batch: 0.21517314224105494\n",
      "epsilons agent1 and agent2: 0.16357390621877743 0.16357390621877743\n",
      "651/651 [==============================] - 0s 41us/step\n",
      "651/651 [==============================] - 0s 40us/step\n",
      "updating with 651, 651 training batch.\n",
      "Epoch 1/1\n",
      "651/651 [==============================] - 0s 156us/step - loss: 0.2028\n",
      "Epoch 1/1\n",
      "651/651 [==============================] - 0s 155us/step - loss: 0.2026\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 56.53489899635315 seconds.\n",
      "neural network update time: 0.20848488807678223 seconds.\n",
      "eval score on batch: 0.20234794950773638\n",
      "epsilons agent1 and agent2: 0.16275603668768354 0.16275603668768354\n",
      "684/684 [==============================] - 0s 46us/step\n",
      "684/684 [==============================] - 0s 54us/step\n",
      "updating with 684, 684 training batch.\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 188us/step - loss: 0.2230\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 184us/step - loss: 0.2190\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 53.54579496383667 seconds.\n",
      "neural network update time: 0.2606809139251709 seconds.\n",
      "eval score on batch: 0.23675461645014803\n",
      "epsilons agent1 and agent2: 0.16194225650424512 0.16194225650424512\n",
      "64 outputs loaded.\n",
      "623/623 [==============================] - 0s 46us/step\n",
      "623/623 [==============================] - 0s 46us/step\n",
      "updating with 623, 623 training batch.\n",
      "Epoch 1/1\n",
      "623/623 [==============================] - 0s 209us/step - loss: 0.2120\n",
      "Epoch 1/1\n",
      "623/623 [==============================] - 0s 190us/step - loss: 0.2146\n",
      "player 1 wins: 10\n",
      "player 2 wins: 14\n",
      "simulations took 55.36013078689575 seconds.\n",
      "neural network update time: 0.25479888916015625 seconds.\n",
      "eval score on batch: 0.23132642891921737\n",
      "epsilons agent1 and agent2: 0.1611325452217239 0.1611325452217239\n",
      "734/734 [==============================] - 0s 42us/step\n",
      "734/734 [==============================] - 0s 41us/step\n",
      "updating with 734, 734 training batch.\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 193us/step - loss: 0.2113\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 176us/step - loss: 0.2094\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 54.314764738082886 seconds.\n",
      "neural network update time: 0.27758097648620605 seconds.\n",
      "eval score on batch: 0.22629565910032726\n",
      "epsilons agent1 and agent2: 0.16032688249561527 0.16032688249561527\n",
      "768/768 [==============================] - 0s 44us/step\n",
      "768/768 [==============================] - 0s 50us/step\n",
      "updating with 768, 768 training batch.\n",
      "Epoch 1/1\n",
      "768/768 [==============================] - 0s 196us/step - loss: 0.1964\n",
      "Epoch 1/1\n",
      "768/768 [==============================] - 0s 184us/step - loss: 0.1962\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 56.6589150428772 seconds.\n",
      "neural network update time: 0.29718899726867676 seconds.\n",
      "eval score on batch: 0.20803865262617668\n",
      "epsilons agent1 and agent2: 0.1595252480831372 0.1595252480831372\n",
      "719/719 [==============================] - 0s 44us/step\n",
      "719/719 [==============================] - 0s 46us/step\n",
      "updating with 719, 719 training batch.\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 217us/step - loss: 0.1956\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 204us/step - loss: 0.2001\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 55.9757559299469 seconds.\n",
      "neural network update time: 0.31066179275512695 seconds.\n",
      "eval score on batch: 0.2044565041344846\n",
      "epsilons agent1 and agent2: 0.15872762184272152 0.15872762184272152\n",
      "748/748 [==============================] - 0s 42us/step\n",
      "748/748 [==============================] - 0s 53us/step\n",
      "updating with 748, 748 training batch.\n",
      "Epoch 1/1\n",
      "748/748 [==============================] - 0s 222us/step - loss: 0.2056\n",
      "Epoch 1/1\n",
      "748/748 [==============================] - 0s 186us/step - loss: 0.2049\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 54.5484881401062 seconds.\n",
      "neural network update time: 0.3114039897918701 seconds.\n",
      "eval score on batch: 0.2095424244230125\n",
      "epsilons agent1 and agent2: 0.15793398373350792 0.15793398373350792\n",
      "726/726 [==============================] - 0s 47us/step\n",
      "726/726 [==============================] - 0s 43us/step\n",
      "updating with 726, 726 training batch.\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 187us/step - loss: 0.1757\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 174us/step - loss: 0.1732\n",
      "player 1 wins: 20\n",
      "player 2 wins: 7\n",
      "simulations took 56.176822900772095 seconds.\n",
      "neural network update time: 0.2677450180053711 seconds.\n",
      "eval score on batch: 0.20000345089547233\n",
      "epsilons agent1 and agent2: 0.15714431381484037 0.15714431381484037\n",
      "692/692 [==============================] - 0s 42us/step\n",
      "692/692 [==============================] - 0s 51us/step\n",
      "updating with 692, 692 training batch.\n",
      "Epoch 1/1\n",
      "692/692 [==============================] - 0s 186us/step - loss: 0.2212\n",
      "Epoch 1/1\n",
      "692/692 [==============================] - 0s 219us/step - loss: 0.2225\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 54.819021701812744 seconds.\n",
      "neural network update time: 0.28659725189208984 seconds.\n",
      "eval score on batch: 0.25696134524193803\n",
      "epsilons agent1 and agent2: 0.15635859224576618 0.15635859224576618\n",
      "771/771 [==============================] - 0s 49us/step\n",
      "771/771 [==============================] - 0s 46us/step\n",
      "updating with 771, 771 training batch.\n",
      "Epoch 1/1\n",
      "771/771 [==============================] - 0s 198us/step - loss: 0.1988\n",
      "Epoch 1/1\n",
      "771/771 [==============================] - 0s 199us/step - loss: 0.1994\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 55.18571186065674 seconds.\n",
      "neural network update time: 0.3125791549682617 seconds.\n",
      "eval score on batch: 0.20244396834521905\n",
      "epsilons agent1 and agent2: 0.15557679928453735 0.15557679928453735\n",
      "775/775 [==============================] - 0s 43us/step\n",
      "775/775 [==============================] - 0s 41us/step\n",
      "updating with 775, 775 training batch.\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 187us/step - loss: 0.1937\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 197us/step - loss: 0.1931\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 55.638664960861206 seconds.\n",
      "neural network update time: 0.30364298820495605 seconds.\n",
      "eval score on batch: 0.19332363960543467\n",
      "epsilons agent1 and agent2: 0.15479891528811468 0.15479891528811468\n",
      "638/638 [==============================] - 0s 43us/step\n",
      "638/638 [==============================] - 0s 53us/step\n",
      "updating with 638, 638 training batch.\n",
      "Epoch 1/1\n",
      "638/638 [==============================] - 0s 227us/step - loss: 0.1785\n",
      "Epoch 1/1\n",
      "638/638 [==============================] - 0s 180us/step - loss: 0.1781\n",
      "player 1 wins: 18\n",
      "player 2 wins: 7\n",
      "simulations took 55.332791805267334 seconds.\n",
      "neural network update time: 0.2659339904785156 seconds.\n",
      "eval score on batch: 0.18542790212040786\n",
      "epsilons agent1 and agent2: 0.15402492071167412 0.15402492071167412\n",
      "668/668 [==============================] - 0s 45us/step\n",
      "668/668 [==============================] - 0s 45us/step\n",
      "updating with 668, 668 training batch.\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 0s 198us/step - loss: 0.1930\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - ETA: 0s - loss: 0.188 - 0s 192us/step - loss: 0.1894\n",
      "player 1 wins: 14\n",
      "player 2 wins: 10\n",
      "simulations took 57.684978723526 seconds.\n",
      "neural network update time: 0.2671852111816406 seconds.\n",
      "eval score on batch: 0.20679395284481392\n",
      "epsilons agent1 and agent2: 0.15325479610811574 0.15325479610811574\n",
      "806/806 [==============================] - 0s 41us/step\n",
      "806/806 [==============================] - 0s 57us/step\n",
      "updating with 806, 806 training batch.\n",
      "Epoch 1/1\n",
      "806/806 [==============================] - 0s 225us/step - loss: 0.1906\n",
      "Epoch 1/1\n",
      "806/806 [==============================] - 0s 203us/step - loss: 0.1937\n",
      "player 1 wins: 18\n",
      "player 2 wins: 12\n",
      "simulations took 55.393702030181885 seconds.\n",
      "neural network update time: 0.351409912109375 seconds.\n",
      "eval score on batch: 0.19189136050972005\n",
      "epsilons agent1 and agent2: 0.15248852212757516 0.15248852212757516\n",
      "770/770 [==============================] - 0s 45us/step\n",
      "770/770 [==============================] - 0s 43us/step\n",
      "updating with 770, 770 training batch.\n",
      "Epoch 1/1\n",
      "770/770 [==============================] - 0s 196us/step - loss: 0.1924\n",
      "Epoch 1/1\n",
      "770/770 [==============================] - 0s 176us/step - loss: 0.1927\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 55.59004211425781 seconds.\n",
      "neural network update time: 0.2923469543457031 seconds.\n",
      "eval score on batch: 0.19350184954743674\n",
      "epsilons agent1 and agent2: 0.15172607951693728 0.15172607951693728\n",
      "771/771 [==============================] - 0s 42us/step\n",
      "771/771 [==============================] - 0s 43us/step\n",
      "updating with 771, 771 training batch.\n",
      "Epoch 1/1\n",
      "771/771 [==============================] - 0s 227us/step - loss: 0.1930\n",
      "Epoch 1/1\n",
      "771/771 [==============================] - 0s 258us/step - loss: 0.1933\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 55.284467935562134 seconds.\n",
      "neural network update time: 0.380159854888916 seconds.\n",
      "eval score on batch: 0.19253564545288004\n",
      "epsilons agent1 and agent2: 0.1509674491193526 0.1509674491193526\n",
      "687/687 [==============================] - 0s 48us/step\n",
      "687/687 [==============================] - 0s 43us/step\n",
      "updating with 687, 687 training batch.\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 184us/step - loss: 0.1995\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 171us/step - loss: 0.2057\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 56.114898920059204 seconds.\n",
      "neural network update time: 0.24930214881896973 seconds.\n",
      "eval score on batch: 0.20306685328852314\n",
      "epsilons agent1 and agent2: 0.1502126118737558 0.1502126118737558\n",
      "734/734 [==============================] - 0s 46us/step\n",
      "734/734 [==============================] - 0s 41us/step\n",
      "updating with 734, 734 training batch.\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 188us/step - loss: 0.1930\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 175us/step - loss: 0.1924\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 56.363675117492676 seconds.\n",
      "neural network update time: 0.27196788787841797 seconds.\n",
      "eval score on batch: 0.19337228732310457\n",
      "epsilons agent1 and agent2: 0.14946154881438703 0.14946154881438703\n",
      "674/674 [==============================] - 0s 47us/step\n",
      "674/674 [==============================] - 0s 48us/step\n",
      "updating with 674, 674 training batch.\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 207us/step - loss: 0.1783\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 184us/step - loss: 0.1793\n",
      "player 1 wins: 17\n",
      "player 2 wins: 8\n",
      "simulations took 56.6482138633728 seconds.\n",
      "neural network update time: 0.270021915435791 seconds.\n",
      "eval score on batch: 0.18405415180473636\n",
      "epsilons agent1 and agent2: 0.14871424107031508 0.14871424107031508\n",
      "643/643 [==============================] - 0s 44us/step\n",
      "643/643 [==============================] - 0s 40us/step\n",
      "updating with 643, 643 training batch.\n",
      "Epoch 1/1\n",
      "643/643 [==============================] - 0s 190us/step - loss: 0.2065\n",
      "Epoch 1/1\n",
      "643/643 [==============================] - 0s 181us/step - loss: 0.2084\n",
      "player 1 wins: 14\n",
      "player 2 wins: 11\n",
      "simulations took 55.50897574424744 seconds.\n",
      "neural network update time: 0.2432870864868164 seconds.\n",
      "eval score on batch: 0.21267212294679408\n",
      "epsilons agent1 and agent2: 0.14797066986496352 0.14797066986496352\n",
      "694/694 [==============================] - 0s 44us/step\n",
      "694/694 [==============================] - 0s 41us/step\n",
      "updating with 694, 694 training batch.\n",
      "Epoch 1/1\n",
      "694/694 [==============================] - 0s 182us/step - loss: 0.1906\n",
      "Epoch 1/1\n",
      "694/694 [==============================] - 0s 181us/step - loss: 0.1890\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 55.66581606864929 seconds.\n",
      "neural network update time: 0.25772619247436523 seconds.\n",
      "eval score on batch: 0.19121101281742542\n",
      "epsilons agent1 and agent2: 0.14723081651563868 0.14723081651563868\n",
      "678/678 [==============================] - 0s 40us/step\n",
      "678/678 [==============================] - 0s 52us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 407us/step - loss: 0.1951\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 200us/step - loss: 0.1943\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 55.58596611022949 seconds.\n",
      "neural network update time: 0.418612003326416 seconds.\n",
      "eval score on batch: 0.1940801947056192\n",
      "epsilons agent1 and agent2: 0.1464946624330605 0.1464946624330605\n",
      "65 outputs loaded.\n",
      "671/671 [==============================] - 0s 47us/step\n",
      "671/671 [==============================] - 0s 58us/step\n",
      "updating with 671, 671 training batch.\n",
      "Epoch 1/1\n",
      "671/671 [==============================] - 0s 211us/step - loss: 0.2041\n",
      "Epoch 1/1\n",
      "671/671 [==============================] - 0s 200us/step - loss: 0.2050\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 55.10142803192139 seconds.\n",
      "neural network update time: 0.28341078758239746 seconds.\n",
      "eval score on batch: 0.2236409358980048\n",
      "epsilons agent1 and agent2: 0.1457621891208952 0.1457621891208952\n",
      "751/751 [==============================] - 0s 43us/step\n",
      "751/751 [==============================] - 0s 43us/step\n",
      "updating with 751, 751 training batch.\n",
      "Epoch 1/1\n",
      "751/751 [==============================] - 0s 194us/step - loss: 0.2028\n",
      "Epoch 1/1\n",
      "751/751 [==============================] - 0s 183us/step - loss: 0.1992\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 55.41400098800659 seconds.\n",
      "neural network update time: 0.28915905952453613 seconds.\n",
      "eval score on batch: 0.21492849968959107\n",
      "epsilons agent1 and agent2: 0.14503337817529072 0.14503337817529072\n",
      "669/669 [==============================] - 0s 47us/step\n",
      "669/669 [==============================] - 0s 46us/step\n",
      "updating with 669, 669 training batch.\n",
      "Epoch 1/1\n",
      "669/669 [==============================] - 0s 190us/step - loss: 0.1953\n",
      "Epoch 1/1\n",
      "669/669 [==============================] - 0s 182us/step - loss: 0.1963\n",
      "player 1 wins: 13\n",
      "player 2 wins: 12\n",
      "simulations took 56.25070905685425 seconds.\n",
      "neural network update time: 0.2546980381011963 seconds.\n",
      "eval score on batch: 0.19727632111587154\n",
      "epsilons agent1 and agent2: 0.14430821128441426 0.14430821128441426\n",
      "828/828 [==============================] - 0s 45us/step\n",
      "828/828 [==============================] - 0s 45us/step\n",
      "updating with 828, 828 training batch.\n",
      "Epoch 1/1\n",
      "828/828 [==============================] - 0s 202us/step - loss: 0.1754\n",
      "Epoch 1/1\n",
      "828/828 [==============================] - 0s 181us/step - loss: 0.1745\n",
      "player 1 wins: 21\n",
      "player 2 wins: 9\n",
      "simulations took 56.260863304138184 seconds.\n",
      "neural network update time: 0.323469877243042 seconds.\n",
      "eval score on batch: 0.18327777541202048\n",
      "epsilons agent1 and agent2: 0.14358667022799218 0.14358667022799218\n",
      "719/719 [==============================] - 0s 44us/step\n",
      "719/719 [==============================] - 0s 43us/step\n",
      "updating with 719, 719 training batch.\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 194us/step - loss: 0.1660\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 171us/step - loss: 0.1650\n",
      "player 1 wins: 19\n",
      "player 2 wins: 7\n",
      "simulations took 56.953036069869995 seconds.\n",
      "neural network update time: 0.2683751583099365 seconds.\n",
      "eval score on batch: 0.1650768082986569\n",
      "epsilons agent1 and agent2: 0.14286873687685223 0.14286873687685223\n",
      "593/593 [==============================] - 0s 43us/step\n",
      "593/593 [==============================] - 0s 43us/step\n",
      "updating with 593, 593 training batch.\n",
      "Epoch 1/1\n",
      "593/593 [==============================] - 0s 185us/step - loss: 0.2223\n",
      "Epoch 1/1\n",
      "593/593 [==============================] - 0s 180us/step - loss: 0.2212\n",
      "player 1 wins: 11\n",
      "player 2 wins: 13\n",
      "simulations took 54.81215000152588 seconds.\n",
      "neural network update time: 0.22279596328735352 seconds.\n",
      "eval score on batch: 0.24163114986874\n",
      "epsilons agent1 and agent2: 0.14215439319246798 0.14215439319246798\n",
      "796/796 [==============================] - 0s 43us/step\n",
      "796/796 [==============================] - 0s 43us/step\n",
      "updating with 796, 796 training batch.\n",
      "Epoch 1/1\n",
      "796/796 [==============================] - 0s 186us/step - loss: 0.2047\n",
      "Epoch 1/1\n",
      "796/796 [==============================] - 0s 185us/step - loss: 0.2063\n",
      "player 1 wins: 19\n",
      "player 2 wins: 12\n",
      "simulations took 54.7885422706604 seconds.\n",
      "neural network update time: 0.3009788990020752 seconds.\n",
      "eval score on batch: 0.21830357176874152\n",
      "epsilons agent1 and agent2: 0.14144362122650564 0.14144362122650564\n",
      "728/728 [==============================] - 0s 43us/step\n",
      "728/728 [==============================] - 0s 42us/step\n",
      "updating with 728, 728 training batch.\n",
      "Epoch 1/1\n",
      "728/728 [==============================] - 0s 196us/step - loss: 0.2159\n",
      "Epoch 1/1\n",
      "728/728 [==============================] - 0s 172us/step - loss: 0.2144\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 53.92976379394531 seconds.\n",
      "neural network update time: 0.2739689350128174 seconds.\n",
      "eval score on batch: 0.2281782001584441\n",
      "epsilons agent1 and agent2: 0.1407364031203731 0.1407364031203731\n",
      "714/714 [==============================] - 0s 44us/step\n",
      "714/714 [==============================] - 0s 43us/step\n",
      "updating with 714, 714 training batch.\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 194us/step - loss: 0.1984\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 190us/step - loss: 0.1981\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 55.59954905509949 seconds.\n",
      "neural network update time: 0.27993297576904297 seconds.\n",
      "eval score on batch: 0.20582931007177724\n",
      "epsilons agent1 and agent2: 0.14003272110477125 0.14003272110477125\n",
      "636/636 [==============================] - 0s 48us/step\n",
      "636/636 [==============================] - 0s 45us/step\n",
      "updating with 636, 636 training batch.\n",
      "Epoch 1/1\n",
      "636/636 [==============================] - 0s 189us/step - loss: 0.1981\n",
      "Epoch 1/1\n",
      "636/636 [==============================] - 0s 173us/step - loss: 0.1979\n",
      "player 1 wins: 13\n",
      "player 2 wins: 11\n",
      "simulations took 56.66777205467224 seconds.\n",
      "neural network update time: 0.23619604110717773 seconds.\n",
      "eval score on batch: 0.19682363828398147\n",
      "epsilons agent1 and agent2: 0.1393325574992474 0.1393325574992474\n",
      "666/666 [==============================] - 0s 43us/step\n",
      "666/666 [==============================] - 0s 43us/step\n",
      "updating with 666, 666 training batch.\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 204us/step - loss: 0.2028\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 187us/step - loss: 0.2021\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 55.543668031692505 seconds.\n",
      "neural network update time: 0.26737117767333984 seconds.\n",
      "eval score on batch: 0.20189395202351762\n",
      "epsilons agent1 and agent2: 0.13863589471175114 0.13863589471175114\n",
      "635/635 [==============================] - 0s 43us/step\n",
      "635/635 [==============================] - 0s 50us/step\n",
      "updating with 635, 635 training batch.\n",
      "Epoch 1/1\n",
      "635/635 [==============================] - 0s 180us/step - loss: 0.1958\n",
      "Epoch 1/1\n",
      "635/635 [==============================] - 0s 174us/step - loss: 0.1946\n",
      "player 1 wins: 16\n",
      "player 2 wins: 9\n",
      "simulations took 55.106653928756714 seconds.\n",
      "neural network update time: 0.2306349277496338 seconds.\n",
      "eval score on batch: 0.19562015758724663\n",
      "epsilons agent1 and agent2: 0.13794271523819238 0.13794271523819238\n",
      "764/764 [==============================] - 0s 42us/step\n",
      "764/764 [==============================] - 0s 44us/step\n",
      "updating with 764, 764 training batch.\n",
      "Epoch 1/1\n",
      "764/764 [==============================] - 0s 201us/step - loss: 0.2052\n",
      "Epoch 1/1\n",
      "764/764 [==============================] - 0s 181us/step - loss: 0.2041\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 54.59763693809509 seconds.\n",
      "neural network update time: 0.29882001876831055 seconds.\n",
      "eval score on batch: 0.2286362974119436\n",
      "epsilons agent1 and agent2: 0.1372530016620014 0.1372530016620014\n",
      "700/700 [==============================] - 0s 43us/step\n",
      "700/700 [==============================] - 0s 47us/step\n",
      "updating with 700, 700 training batch.\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 188us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 185us/step - loss: 0.2011\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 55.24213004112244 seconds.\n",
      "neural network update time: 0.2680981159210205 seconds.\n",
      "eval score on batch: 0.20212184940065656\n",
      "epsilons agent1 and agent2: 0.1365667366536914 0.1365667366536914\n",
      "779/779 [==============================] - 0s 47us/step\n",
      "779/779 [==============================] - 0s 42us/step\n",
      "updating with 779, 779 training batch.\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 191us/step - loss: 0.1928\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 177us/step - loss: 0.1929\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 55.82834601402283 seconds.\n",
      "neural network update time: 0.29189300537109375 seconds.\n",
      "eval score on batch: 0.19940662255871738\n",
      "epsilons agent1 and agent2: 0.13588390297042294 0.13588390297042294\n",
      "678/678 [==============================] - 0s 47us/step\n",
      "678/678 [==============================] - 0s 54us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 197us/step - loss: 0.1781\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 195us/step - loss: 0.1758\n",
      "player 1 wins: 19\n",
      "player 2 wins: 7\n",
      "simulations took 55.189876079559326 seconds.\n",
      "neural network update time: 0.27256083488464355 seconds.\n",
      "eval score on batch: 0.17941332678791344\n",
      "epsilons agent1 and agent2: 0.13520448345557082 0.13520448345557082\n",
      "600/600 [==============================] - 0s 43us/step\n",
      "600/600 [==============================] - 0s 43us/step\n",
      "updating with 600, 600 training batch.\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 188us/step - loss: 0.2104\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 172us/step - loss: 0.2075\n",
      "player 1 wins: 14\n",
      "player 2 wins: 10\n",
      "simulations took 54.403160095214844 seconds.\n",
      "neural network update time: 0.22112178802490234 seconds.\n",
      "eval score on batch: 0.21659424036741257\n",
      "epsilons agent1 and agent2: 0.13452846103829297 0.13452846103829297\n",
      "587/587 [==============================] - 0s 45us/step\n",
      "587/587 [==============================] - 0s 45us/step\n",
      "updating with 587, 587 training batch.\n",
      "Epoch 1/1\n",
      "587/587 [==============================] - 0s 199us/step - loss: 0.2060\n",
      "Epoch 1/1\n",
      "587/587 [==============================] - 0s 191us/step - loss: 0.2052\n",
      "player 1 wins: 11\n",
      "player 2 wins: 12\n",
      "simulations took 56.1530601978302 seconds.\n",
      "neural network update time: 0.23436594009399414 seconds.\n",
      "eval score on batch: 0.20647516477281097\n",
      "epsilons agent1 and agent2: 0.1338558187331015 0.1338558187331015\n",
      "646/646 [==============================] - 0s 42us/step\n",
      "646/646 [==============================] - 0s 47us/step\n",
      "updating with 646, 646 training batch.\n",
      "Epoch 1/1\n",
      "646/646 [==============================] - 0s 211us/step - loss: 0.1921\n",
      "Epoch 1/1\n",
      "646/646 [==============================] - 0s 186us/step - loss: 0.1874\n",
      "player 1 wins: 18\n",
      "player 2 wins: 7\n",
      "simulations took 55.59176206588745 seconds.\n",
      "neural network update time: 0.2629721164703369 seconds.\n",
      "eval score on batch: 0.20886055010171958\n",
      "epsilons agent1 and agent2: 0.133186539639436 0.133186539639436\n",
      "715/715 [==============================] - 0s 43us/step\n",
      "715/715 [==============================] - 0s 43us/step\n",
      "updating with 715, 715 training batch.\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 189us/step - loss: 0.1990\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 177us/step - loss: 0.2088\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 56.95763802528381 seconds.\n",
      "neural network update time: 0.267794132232666 seconds.\n",
      "eval score on batch: 0.25543600529223887\n",
      "epsilons agent1 and agent2: 0.13252060694123882 0.13252060694123882\n",
      "66 outputs loaded.\n",
      "632/632 [==============================] - 0s 43us/step\n",
      "632/632 [==============================] - 0s 41us/step\n",
      "updating with 632, 632 training batch.\n",
      "Epoch 1/1\n",
      "632/632 [==============================] - 0s 189us/step - loss: 0.2072\n",
      "Epoch 1/1\n",
      "632/632 [==============================] - 0s 173us/step - loss: 0.2055\n",
      "player 1 wins: 13\n",
      "player 2 wins: 11\n",
      "simulations took 55.98262619972229 seconds.\n",
      "neural network update time: 0.23455166816711426 seconds.\n",
      "eval score on batch: 0.22288275991059556\n",
      "epsilons agent1 and agent2: 0.13185800390653263 0.13185800390653263\n",
      "712/712 [==============================] - 0s 44us/step\n",
      "712/712 [==============================] - 0s 38us/step\n",
      "updating with 712, 712 training batch.\n",
      "Epoch 1/1\n",
      "712/712 [==============================] - 0s 194us/step - loss: 0.1954\n",
      "Epoch 1/1\n",
      "712/712 [==============================] - 0s 179us/step - loss: 0.1978\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 56.56223678588867 seconds.\n",
      "neural network update time: 0.27134108543395996 seconds.\n",
      "eval score on batch: 0.1983045447261983\n",
      "epsilons agent1 and agent2: 0.13119871388699997 0.13119871388699997\n",
      "720/720 [==============================] - 0s 44us/step\n",
      "720/720 [==============================] - 0s 44us/step\n",
      "updating with 720, 720 training batch.\n",
      "Epoch 1/1\n",
      "720/720 [==============================] - 0s 184us/step - loss: 0.1889\n",
      "Epoch 1/1\n",
      "720/720 [==============================] - 0s 185us/step - loss: 0.1891\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 56.51546907424927 seconds.\n",
      "neural network update time: 0.27158594131469727 seconds.\n",
      "eval score on batch: 0.19202767941686844\n",
      "epsilons agent1 and agent2: 0.13054272031756498 0.13054272031756498\n",
      "625/625 [==============================] - 0s 44us/step\n",
      "625/625 [==============================] - 0s 49us/step\n",
      "updating with 625, 625 training batch.\n",
      "Epoch 1/1\n",
      "625/625 [==============================] - 0s 209us/step - loss: 0.2036\n",
      "Epoch 1/1\n",
      "625/625 [==============================] - 0s 177us/step - loss: 0.2038\n",
      "player 1 wins: 9\n",
      "player 2 wins: 16\n",
      "simulations took 55.07012915611267 seconds.\n",
      "neural network update time: 0.24693608283996582 seconds.\n",
      "eval score on batch: 0.21112452427744866\n",
      "epsilons agent1 and agent2: 0.12989000671597714 0.12989000671597714\n",
      "776/776 [==============================] - 0s 43us/step\n",
      "776/776 [==============================] - 0s 46us/step\n",
      "updating with 776, 776 training batch.\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 195us/step - loss: 0.2042\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 184us/step - loss: 0.2003\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 55.09409475326538 seconds.\n",
      "neural network update time: 0.301163911819458 seconds.\n",
      "eval score on batch: 0.21108278178984358\n",
      "epsilons agent1 and agent2: 0.12924055668239726 0.12924055668239726\n",
      "648/648 [==============================] - 0s 43us/step\n",
      "648/648 [==============================] - 0s 44us/step\n",
      "updating with 648, 648 training batch.\n",
      "Epoch 1/1\n",
      "648/648 [==============================] - 0s 203us/step - loss: 0.1946\n",
      "Epoch 1/1\n",
      "648/648 [==============================] - 0s 193us/step - loss: 0.1904\n",
      "player 1 wins: 18\n",
      "player 2 wins: 8\n",
      "simulations took 54.651182651519775 seconds.\n",
      "neural network update time: 0.2623741626739502 seconds.\n",
      "eval score on batch: 0.19688380665985156\n",
      "epsilons agent1 and agent2: 0.12859435389898527 0.12859435389898527\n",
      "760/760 [==============================] - 0s 40us/step\n",
      "760/760 [==============================] - 0s 41us/step\n",
      "updating with 760, 760 training batch.\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 194us/step - loss: 0.1971\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 174us/step - loss: 0.2048\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 55.89240312576294 seconds.\n",
      "neural network update time: 0.2862091064453125 seconds.\n",
      "eval score on batch: 0.21446051613280648\n",
      "epsilons agent1 and agent2: 0.12795138212949034 0.12795138212949034\n",
      "815/815 [==============================] - 0s 42us/step\n",
      "815/815 [==============================] - 0s 42us/step\n",
      "updating with 815, 815 training batch.\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 0s 187us/step - loss: 0.1943\n",
      "Epoch 1/1\n",
      "815/815 [==============================] - 0s 177us/step - loss: 0.1931\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 55.85265493392944 seconds.\n",
      "neural network update time: 0.3026449680328369 seconds.\n",
      "eval score on batch: 0.19330723505650013\n",
      "epsilons agent1 and agent2: 0.12731162521884287 0.12731162521884287\n",
      "664/664 [==============================] - 0s 43us/step\n",
      "664/664 [==============================] - 0s 45us/step\n",
      "updating with 664, 664 training batch.\n",
      "Epoch 1/1\n",
      "664/664 [==============================] - 0s 185us/step - loss: 0.1979\n",
      "Epoch 1/1\n",
      "664/664 [==============================] - 0s 180us/step - loss: 0.1947\n",
      "player 1 wins: 17\n",
      "player 2 wins: 9\n",
      "simulations took 55.58651375770569 seconds.\n",
      "neural network update time: 0.24819302558898926 seconds.\n",
      "eval score on batch: 0.20102144077599768\n",
      "epsilons agent1 and agent2: 0.12667506709274864 0.12667506709274864\n",
      "775/775 [==============================] - 0s 43us/step\n",
      "775/775 [==============================] - 0s 43us/step\n",
      "updating with 775, 775 training batch.\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 186us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 183us/step - loss: 0.2038\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 55.14877414703369 seconds.\n",
      "neural network update time: 0.29189586639404297 seconds.\n",
      "eval score on batch: 0.21224998610304488\n",
      "epsilons agent1 and agent2: 0.1260416917572849 0.1260416917572849\n",
      "709/709 [==============================] - 0s 43us/step\n",
      "709/709 [==============================] - 0s 43us/step\n",
      "updating with 709, 709 training batch.\n",
      "Epoch 1/1\n",
      "709/709 [==============================] - 0s 193us/step - loss: 0.2004\n",
      "Epoch 1/1\n",
      "709/709 [==============================] - 0s 179us/step - loss: 0.1998\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 56.19430708885193 seconds.\n",
      "neural network update time: 0.26987600326538086 seconds.\n",
      "eval score on batch: 0.20402672258435586\n",
      "epsilons agent1 and agent2: 0.1254114832984985 0.1254114832984985\n",
      "714/714 [==============================] - 0s 45us/step\n",
      "714/714 [==============================] - 0s 42us/step\n",
      "updating with 714, 714 training batch.\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 200us/step - loss: 0.1741\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 192us/step - loss: 0.1750\n",
      "player 1 wins: 20\n",
      "player 2 wins: 7\n",
      "simulations took 55.37868595123291 seconds.\n",
      "neural network update time: 0.2863481044769287 seconds.\n",
      "eval score on batch: 0.18650558669673128\n",
      "epsilons agent1 and agent2: 0.124784425882006 0.124784425882006\n",
      "592/592 [==============================] - 0s 44us/step\n",
      "592/592 [==============================] - 0s 46us/step\n",
      "updating with 592, 592 training batch.\n",
      "Epoch 1/1\n",
      "592/592 [==============================] - 0s 203us/step - loss: 0.1748\n",
      "Epoch 1/1\n",
      "592/592 [==============================] - 0s 191us/step - loss: 0.1733\n",
      "player 1 wins: 17\n",
      "player 2 wins: 6\n",
      "simulations took 56.751213788986206 seconds.\n",
      "neural network update time: 0.2398362159729004 seconds.\n",
      "eval score on batch: 0.17420858426673994\n",
      "epsilons agent1 and agent2: 0.12416050375259596 0.12416050375259596\n",
      "668/668 [==============================] - 0s 45us/step\n",
      "668/668 [==============================] - 0s 41us/step\n",
      "updating with 668, 668 training batch.\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 0s 185us/step - loss: 0.2087\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 0s 181us/step - loss: 0.2109\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 55.18680500984192 seconds.\n",
      "neural network update time: 0.25020289421081543 seconds.\n",
      "eval score on batch: 0.2378075438702178\n",
      "epsilons agent1 and agent2: 0.12353970123383298 0.12353970123383298\n",
      "724/724 [==============================] - 0s 43us/step\n",
      "724/724 [==============================] - 0s 44us/step\n",
      "updating with 724, 724 training batch.\n",
      "Epoch 1/1\n",
      "724/724 [==============================] - 0s 197us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "724/724 [==============================] - 0s 182us/step - loss: 0.1960\n",
      "player 1 wins: 17\n",
      "player 2 wins: 10\n",
      "simulations took 56.62272095680237 seconds.\n",
      "neural network update time: 0.280689001083374 seconds.\n",
      "eval score on batch: 0.2145347085560142\n",
      "epsilons agent1 and agent2: 0.12292200272766382 0.12292200272766382\n",
      "804/804 [==============================] - 0s 43us/step\n",
      "804/804 [==============================] - 0s 43us/step\n",
      "updating with 804, 804 training batch.\n",
      "Epoch 1/1\n",
      "804/804 [==============================] - 0s 192us/step - loss: 0.1864\n",
      "Epoch 1/1\n",
      "804/804 [==============================] - 0s 185us/step - loss: 0.1861\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 57.558732986450195 seconds.\n",
      "neural network update time: 0.30843281745910645 seconds.\n",
      "eval score on batch: 0.19258372200217413\n",
      "epsilons agent1 and agent2: 0.1223073927140255 0.1223073927140255\n",
      "842/842 [==============================] - 0s 43us/step\n",
      "842/842 [==============================] - 0s 43us/step\n",
      "updating with 842, 842 training batch.\n",
      "Epoch 1/1\n",
      "842/842 [==============================] - 0s 189us/step - loss: 0.1930\n",
      "Epoch 1/1\n",
      "842/842 [==============================] - 0s 189us/step - loss: 0.1919\n",
      "player 1 wins: 14\n",
      "player 2 wins: 17\n",
      "simulations took 55.95716691017151 seconds.\n",
      "neural network update time: 0.3254389762878418 seconds.\n",
      "eval score on batch: 0.1923329567790005\n",
      "epsilons agent1 and agent2: 0.12169585575045538 0.12169585575045538\n",
      "625/625 [==============================] - 0s 48us/step\n",
      "625/625 [==============================] - 0s 43us/step\n",
      "updating with 625, 625 training batch.\n",
      "Epoch 1/1\n",
      "625/625 [==============================] - 0s 193us/step - loss: 0.1996\n",
      "Epoch 1/1\n",
      "625/625 [==============================] - 0s 192us/step - loss: 0.2010\n",
      "player 1 wins: 15\n",
      "player 2 wins: 9\n",
      "simulations took 56.11517524719238 seconds.\n",
      "neural network update time: 0.24617290496826172 seconds.\n",
      "eval score on batch: 0.2188675790667534\n",
      "epsilons agent1 and agent2: 0.1210873764717031 0.1210873764717031\n",
      "708/708 [==============================] - 0s 44us/step\n",
      "708/708 [==============================] - 0s 44us/step\n",
      "updating with 708, 708 training batch.\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 198us/step - loss: 0.1633\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 180us/step - loss: 0.1607\n",
      "player 1 wins: 21\n",
      "player 2 wins: 6\n",
      "simulations took 55.06268906593323 seconds.\n",
      "neural network update time: 0.27336907386779785 seconds.\n",
      "eval score on batch: 0.16488636353731914\n",
      "epsilons agent1 and agent2: 0.12048193958934458 0.12048193958934458\n",
      "750/750 [==============================] - 0s 49us/step\n",
      "750/750 [==============================] - 0s 57us/step\n",
      "updating with 750, 750 training batch.\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 225us/step - loss: 0.1942\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 208us/step - loss: 0.1948\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 56.08691596984863 seconds.\n",
      "neural network update time: 0.3320331573486328 seconds.\n",
      "eval score on batch: 0.20787001343568165\n",
      "epsilons agent1 and agent2: 0.11987952989139786 0.11987952989139786\n",
      "67 outputs loaded.\n",
      "633/633 [==============================] - 0s 45us/step\n",
      "633/633 [==============================] - 0s 38us/step\n",
      "updating with 633, 633 training batch.\n",
      "Epoch 1/1\n",
      "633/633 [==============================] - 0s 195us/step - loss: 0.2053\n",
      "Epoch 1/1\n",
      "633/633 [==============================] - 0s 187us/step - loss: 0.2048\n",
      "player 1 wins: 14\n",
      "player 2 wins: 11\n",
      "simulations took 55.10111379623413 seconds.\n",
      "neural network update time: 0.24726605415344238 seconds.\n",
      "eval score on batch: 0.20421692352347653\n",
      "epsilons agent1 and agent2: 0.11928013224194087 0.11928013224194087\n",
      "721/721 [==============================] - 0s 43us/step\n",
      "721/721 [==============================] - 0s 42us/step\n",
      "updating with 721, 721 training batch.\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 187us/step - loss: 0.1901\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 182us/step - loss: 0.1909\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 57.13477396965027 seconds.\n",
      "neural network update time: 0.27120327949523926 seconds.\n",
      "eval score on batch: 0.19304106947571947\n",
      "epsilons agent1 and agent2: 0.11868373158073116 0.11868373158073116\n",
      "688/688 [==============================] - 0s 43us/step\n",
      "688/688 [==============================] - 0s 43us/step\n",
      "updating with 688, 688 training batch.\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 191us/step - loss: 0.2061\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 177us/step - loss: 0.2053\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 54.73408317565918 seconds.\n",
      "neural network update time: 0.25836730003356934 seconds.\n",
      "eval score on batch: 0.20602030657924886\n",
      "epsilons agent1 and agent2: 0.1180903129228275 0.1180903129228275\n",
      "747/747 [==============================] - 0s 61us/step\n",
      "747/747 [==============================] - 0s 56us/step\n",
      "updating with 747, 747 training batch.\n",
      "Epoch 1/1\n",
      "747/747 [==============================] - 0s 205us/step - loss: 0.1894\n",
      "Epoch 1/1\n",
      "747/747 [==============================] - 0s 211us/step - loss: 0.1901\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 57.01505780220032 seconds.\n",
      "neural network update time: 0.31624913215637207 seconds.\n",
      "eval score on batch: 0.1889623686736088\n",
      "epsilons agent1 and agent2: 0.11749986135821336 0.11749986135821336\n",
      "570/570 [==============================] - 0s 46us/step\n",
      "570/570 [==============================] - 0s 38us/step\n",
      "updating with 570, 570 training batch.\n",
      "Epoch 1/1\n",
      "570/570 [==============================] - 0s 156us/step - loss: 0.2021\n",
      "Epoch 1/1\n",
      "570/570 [==============================] - 0s 157us/step - loss: 0.2045\n",
      "player 1 wins: 9\n",
      "player 2 wins: 13\n",
      "simulations took 55.9410560131073 seconds.\n",
      "neural network update time: 0.18311595916748047 seconds.\n",
      "eval score on batch: 0.20701208349905517\n",
      "epsilons agent1 and agent2: 0.11691236205142229 0.11691236205142229\n",
      "665/665 [==============================] - 0s 42us/step\n",
      "665/665 [==============================] - 0s 37us/step\n",
      "updating with 665, 665 training batch.\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 160us/step - loss: 0.2048\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 152us/step - loss: 0.2059\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 53.932318925857544 seconds.\n",
      "neural network update time: 0.21332883834838867 seconds.\n",
      "eval score on batch: 0.2037768229281992\n",
      "epsilons agent1 and agent2: 0.11632780024116518 0.11632780024116518\n",
      "835/835 [==============================] - 0s 48us/step\n",
      "835/835 [==============================] - 0s 38us/step\n",
      "updating with 835, 835 training batch.\n",
      "Epoch 1/1\n",
      "835/835 [==============================] - 0s 175us/step - loss: 0.1929\n",
      "Epoch 1/1\n",
      "835/835 [==============================] - 0s 165us/step - loss: 0.1964\n",
      "player 1 wins: 18\n",
      "player 2 wins: 13\n",
      "simulations took 54.85181474685669 seconds.\n",
      "neural network update time: 0.2902669906616211 seconds.\n",
      "eval score on batch: 0.19876481076858626\n",
      "epsilons agent1 and agent2: 0.11574616123995936 0.11574616123995936\n",
      "754/754 [==============================] - 0s 39us/step\n",
      "754/754 [==============================] - 0s 42us/step\n",
      "updating with 754, 754 training batch.\n",
      "Epoch 1/1\n",
      "754/754 [==============================] - 0s 159us/step - loss: 0.1959\n",
      "Epoch 1/1\n",
      "754/754 [==============================] - 0s 159us/step - loss: 0.1976\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 54.733177185058594 seconds.\n",
      "neural network update time: 0.24537420272827148 seconds.\n",
      "eval score on batch: 0.19569470287752722\n",
      "epsilons agent1 and agent2: 0.11516743043375956 0.11516743043375956\n",
      "785/785 [==============================] - 0s 37us/step\n",
      "785/785 [==============================] - 0s 39us/step\n",
      "updating with 785, 785 training batch.\n",
      "Epoch 1/1\n",
      "785/785 [==============================] - 0s 157us/step - loss: 0.1922\n",
      "Epoch 1/1\n",
      "785/785 [==============================] - 0s 152us/step - loss: 0.1939\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 55.101449966430664 seconds.\n",
      "neural network update time: 0.24813508987426758 seconds.\n",
      "eval score on batch: 0.19281898039470244\n",
      "epsilons agent1 and agent2: 0.11459159328159076 0.11459159328159076\n",
      "735/735 [==============================] - 0s 37us/step\n",
      "735/735 [==============================] - 0s 39us/step\n",
      "updating with 735, 735 training batch.\n",
      "Epoch 1/1\n",
      "735/735 [==============================] - 0s 163us/step - loss: 0.2007\n",
      "Epoch 1/1\n",
      "735/735 [==============================] - 0s 152us/step - loss: 0.2003\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 54.71969389915466 seconds.\n",
      "neural network update time: 0.2369532585144043 seconds.\n",
      "eval score on batch: 0.1995848626101098\n",
      "epsilons agent1 and agent2: 0.1140186353151828 0.1140186353151828\n",
      "732/732 [==============================] - 0s 41us/step\n",
      "732/732 [==============================] - 0s 37us/step\n",
      "updating with 732, 732 training batch.\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 156us/step - loss: 0.1997\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 152us/step - loss: 0.2005\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 54.59332776069641 seconds.\n",
      "neural network update time: 0.2311549186706543 seconds.\n",
      "eval score on batch: 0.19991537513302976\n",
      "epsilons agent1 and agent2: 0.11344854213860689 0.11344854213860689\n",
      "726/726 [==============================] - 0s 45us/step\n",
      "726/726 [==============================] - 0s 43us/step\n",
      "updating with 726, 726 training batch.\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 174us/step - loss: 0.1957\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 179us/step - loss: 0.1961\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 53.70455765724182 seconds.\n",
      "neural network update time: 0.26177501678466797 seconds.\n",
      "eval score on batch: 0.19766410992329442\n",
      "epsilons agent1 and agent2: 0.11288129942791385 0.11288129942791385\n",
      "671/671 [==============================] - 0s 39us/step\n",
      "671/671 [==============================] - 0s 38us/step\n",
      "updating with 671, 671 training batch.\n",
      "Epoch 1/1\n",
      "671/671 [==============================] - 0s 157us/step - loss: 0.2201\n",
      "Epoch 1/1\n",
      "671/671 [==============================] - 0s 150us/step - loss: 0.2239\n",
      "player 1 wins: 12\n",
      "player 2 wins: 16\n",
      "simulations took 52.3394250869751 seconds.\n",
      "neural network update time: 0.21200203895568848 seconds.\n",
      "eval score on batch: 0.23610347309517613\n",
      "epsilons agent1 and agent2: 0.11231689293077428 0.11231689293077428\n",
      "755/755 [==============================] - 0s 45us/step\n",
      "755/755 [==============================] - 0s 37us/step\n",
      "updating with 755, 755 training batch.\n",
      "Epoch 1/1\n",
      "755/755 [==============================] - 0s 161us/step - loss: 0.1876\n",
      "Epoch 1/1\n",
      "755/755 [==============================] - 0s 154us/step - loss: 0.1873\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 55.78079390525818 seconds.\n",
      "neural network update time: 0.24408912658691406 seconds.\n",
      "eval score on batch: 0.1879051962170429\n",
      "epsilons agent1 and agent2: 0.1117553084661204 0.1117553084661204\n",
      "713/713 [==============================] - 0s 43us/step\n",
      "713/713 [==============================] - 0s 39us/step\n",
      "updating with 713, 713 training batch.\n",
      "Epoch 1/1\n",
      "713/713 [==============================] - 0s 156us/step - loss: 0.1912\n",
      "Epoch 1/1\n",
      "713/713 [==============================] - 0s 158us/step - loss: 0.1915\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 55.807143211364746 seconds.\n",
      "neural network update time: 0.2299180030822754 seconds.\n",
      "eval score on batch: 0.19091091213191416\n",
      "epsilons agent1 and agent2: 0.1111965319237898 0.1111965319237898\n",
      "716/716 [==============================] - 0s 38us/step\n",
      "716/716 [==============================] - 0s 40us/step\n",
      "updating with 716, 716 training batch.\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 158us/step - loss: 0.1848\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 153us/step - loss: 0.1866\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 56.16688513755798 seconds.\n",
      "neural network update time: 0.2278280258178711 seconds.\n",
      "eval score on batch: 0.19116527824428495\n",
      "epsilons agent1 and agent2: 0.11064054926417084 0.11064054926417084\n",
      "681/681 [==============================] - 0s 43us/step\n",
      "681/681 [==============================] - 0s 42us/step\n",
      "updating with 681, 681 training batch.\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 155us/step - loss: 0.1969\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 159us/step - loss: 0.1963\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 55.301722049713135 seconds.\n",
      "neural network update time: 0.21892619132995605 seconds.\n",
      "eval score on batch: 0.2081928657225597\n",
      "epsilons agent1 and agent2: 0.11008734651784999 0.11008734651784999\n",
      "700/700 [==============================] - 0s 38us/step\n",
      "700/700 [==============================] - 0s 40us/step\n",
      "updating with 700, 700 training batch.\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.1876\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 149us/step - loss: 0.1895\n",
      "player 1 wins: 13\n",
      "player 2 wins: 12\n",
      "simulations took 56.44408178329468 seconds.\n",
      "neural network update time: 0.21767711639404297 seconds.\n",
      "eval score on batch: 0.19162904756409782\n",
      "epsilons agent1 and agent2: 0.10953690978526073 0.10953690978526073\n",
      "644/644 [==============================] - 0s 39us/step\n",
      "644/644 [==============================] - 0s 41us/step\n",
      "updating with 644, 644 training batch.\n",
      "Epoch 1/1\n",
      "644/644 [==============================] - 0s 161us/step - loss: 0.1980\n",
      "Epoch 1/1\n",
      "644/644 [==============================] - 0s 159us/step - loss: 0.1988\n",
      "player 1 wins: 15\n",
      "player 2 wins: 10\n",
      "simulations took 54.80111789703369 seconds.\n",
      "neural network update time: 0.2116680145263672 seconds.\n",
      "eval score on batch: 0.20003348558574266\n",
      "epsilons agent1 and agent2: 0.10898922523633443 0.10898922523633443\n",
      "631/631 [==============================] - 0s 40us/step\n",
      "631/631 [==============================] - 0s 38us/step\n",
      "updating with 631, 631 training batch.\n",
      "Epoch 1/1\n",
      "631/631 [==============================] - 0s 154us/step - loss: 0.1935\n",
      "Epoch 1/1\n",
      "631/631 [==============================] - 0s 155us/step - loss: 0.1978\n",
      "player 1 wins: 7\n",
      "player 2 wins: 17\n",
      "simulations took 55.35605597496033 seconds.\n",
      "neural network update time: 0.20225906372070312 seconds.\n",
      "eval score on batch: 0.2306029072559768\n",
      "epsilons agent1 and agent2: 0.10844427911015277 0.10844427911015277\n",
      "68 outputs loaded.\n",
      "658/658 [==============================] - 0s 42us/step\n",
      "658/658 [==============================] - 0s 39us/step\n",
      "updating with 658, 658 training batch.\n",
      "Epoch 1/1\n",
      "658/658 [==============================] - 0s 159us/step - loss: 0.2144\n",
      "Epoch 1/1\n",
      "658/658 [==============================] - 0s 158us/step - loss: 0.2097\n",
      "player 1 wins: 13\n",
      "player 2 wins: 11\n",
      "simulations took 56.5550971031189 seconds.\n",
      "neural network update time: 0.2138350009918213 seconds.\n",
      "eval score on batch: 0.25417573799840587\n",
      "epsilons agent1 and agent2: 0.107902057714602 0.107902057714602\n",
      "663/663 [==============================] - 0s 41us/step\n",
      "663/663 [==============================] - 0s 36us/step\n",
      "updating with 663, 663 training batch.\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 156us/step - loss: 0.1952\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 152us/step - loss: 0.1939\n",
      "player 1 wins: 12\n",
      "player 2 wins: 12\n",
      "simulations took 56.20223784446716 seconds.\n",
      "neural network update time: 0.20952486991882324 seconds.\n",
      "eval score on batch: 0.19765578609010484\n",
      "epsilons agent1 and agent2: 0.10736254742602899 0.10736254742602899\n",
      "691/691 [==============================] - 0s 38us/step\n",
      "691/691 [==============================] - 0s 41us/step\n",
      "updating with 691, 691 training batch.\n",
      "Epoch 1/1\n",
      "691/691 [==============================] - 0s 158us/step - loss: 0.1796\n",
      "Epoch 1/1\n",
      "691/691 [==============================] - 0s 155us/step - loss: 0.1802\n",
      "player 1 wins: 15\n",
      "player 2 wins: 9\n",
      "simulations took 57.839874029159546 seconds.\n",
      "neural network update time: 0.22184205055236816 seconds.\n",
      "eval score on batch: 0.18846880405192437\n",
      "epsilons agent1 and agent2: 0.10682573468889885 0.10682573468889885\n",
      "770/770 [==============================] - 0s 45us/step\n",
      "770/770 [==============================] - 0s 37us/step\n",
      "updating with 770, 770 training batch.\n",
      "Epoch 1/1\n",
      "770/770 [==============================] - 0s 162us/step - loss: 0.1915\n",
      "Epoch 1/1\n",
      "770/770 [==============================] - 0s 157us/step - loss: 0.1948\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 55.8493869304657 seconds.\n",
      "neural network update time: 0.2506999969482422 seconds.\n",
      "eval score on batch: 0.20492915696911998\n",
      "epsilons agent1 and agent2: 0.10629160601545436 0.10629160601545436\n",
      "875/875 [==============================] - 0s 46us/step\n",
      "875/875 [==============================] - 0s 42us/step\n",
      "updating with 875, 875 training batch.\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 0s 159us/step - loss: 0.1784\n",
      "Epoch 1/1\n",
      "875/875 [==============================] - 0s 157us/step - loss: 0.1765\n",
      "player 1 wins: 20\n",
      "player 2 wins: 10\n",
      "simulations took 57.15271210670471 seconds.\n",
      "neural network update time: 0.2830960750579834 seconds.\n",
      "eval score on batch: 0.19342885051188724\n",
      "epsilons agent1 and agent2: 0.10576014798537708 0.10576014798537708\n",
      "714/714 [==============================] - 0s 41us/step\n",
      "714/714 [==============================] - 0s 39us/step\n",
      "updating with 714, 714 training batch.\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 167us/step - loss: 0.1961\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 157us/step - loss: 0.1975\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 54.97149610519409 seconds.\n",
      "neural network update time: 0.23604893684387207 seconds.\n",
      "eval score on batch: 0.19722262709284155\n",
      "epsilons agent1 and agent2: 0.1052313472454502 0.1052313472454502\n",
      "675/675 [==============================] - 0s 42us/step\n",
      "675/675 [==============================] - 0s 40us/step\n",
      "updating with 675, 675 training batch.\n",
      "Epoch 1/1\n",
      "675/675 [==============================] - 0s 164us/step - loss: 0.1807\n",
      "Epoch 1/1\n",
      "675/675 [==============================] - 0s 156us/step - loss: 0.1807\n",
      "player 1 wins: 17\n",
      "player 2 wins: 8\n",
      "simulations took 56.275243043899536 seconds.\n",
      "neural network update time: 0.22181105613708496 seconds.\n",
      "eval score on batch: 0.18256968931191497\n",
      "epsilons agent1 and agent2: 0.10470519050922296 0.10470519050922296\n",
      "578/578 [==============================] - 0s 47us/step\n",
      "578/578 [==============================] - 0s 42us/step\n",
      "updating with 578, 578 training batch.\n",
      "Epoch 1/1\n",
      "578/578 [==============================] - 0s 176us/step - loss: 0.2056\n",
      "Epoch 1/1\n",
      "578/578 [==============================] - 0s 194us/step - loss: 0.2005\n",
      "player 1 wins: 12\n",
      "player 2 wins: 10\n",
      "simulations took 56.05258798599243 seconds.\n",
      "neural network update time: 0.22076201438903809 seconds.\n",
      "eval score on batch: 0.21083211646154265\n",
      "epsilons agent1 and agent2: 0.10418166455667685 0.10418166455667685\n",
      "791/791 [==============================] - 0s 41us/step\n",
      "791/791 [==============================] - 0s 38us/step\n",
      "updating with 791, 791 training batch.\n",
      "Epoch 1/1\n",
      "791/791 [==============================] - 0s 159us/step - loss: 0.1769\n",
      "Epoch 1/1\n",
      "791/791 [==============================] - 0s 154us/step - loss: 0.1771\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 57.12502717971802 seconds.\n",
      "neural network update time: 0.25400280952453613 seconds.\n",
      "eval score on batch: 0.17975363614303724\n",
      "epsilons agent1 and agent2: 0.10366075623389347 0.10366075623389347\n",
      "793/793 [==============================] - 0s 41us/step\n",
      "793/793 [==============================] - 0s 40us/step\n",
      "updating with 793, 793 training batch.\n",
      "Epoch 1/1\n",
      "793/793 [==============================] - 0s 156us/step - loss: 0.2052\n",
      "Epoch 1/1\n",
      "793/793 [==============================] - 0s 185us/step - loss: 0.2052\n",
      "player 1 wins: 15\n",
      "player 2 wins: 16\n",
      "simulations took 53.06902027130127 seconds.\n",
      "neural network update time: 0.2763400077819824 seconds.\n",
      "eval score on batch: 0.20750601499483534\n",
      "epsilons agent1 and agent2: 0.10314245245272399 0.10314245245272399\n",
      "764/764 [==============================] - 0s 37us/step\n",
      "764/764 [==============================] - 0s 40us/step\n",
      "updating with 764, 764 training batch.\n",
      "Epoch 1/1\n",
      "764/764 [==============================] - 0s 149us/step - loss: 0.1854\n",
      "Epoch 1/1\n",
      "764/764 [==============================] - 0s 154us/step - loss: 0.1854\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 56.545918226242065 seconds.\n",
      "neural network update time: 0.23659586906433105 seconds.\n",
      "eval score on batch: 0.18502930259205283\n",
      "epsilons agent1 and agent2: 0.10262674019046038 0.10262674019046038\n",
      "618/618 [==============================] - 0s 40us/step\n",
      "618/618 [==============================] - 0s 38us/step\n",
      "updating with 618, 618 training batch.\n",
      "Epoch 1/1\n",
      "618/618 [==============================] - 0s 158us/step - loss: 0.1904\n",
      "Epoch 1/1\n",
      "618/618 [==============================] - 0s 156us/step - loss: 0.1913\n",
      "player 1 wins: 14\n",
      "player 2 wins: 9\n",
      "simulations took 56.37028789520264 seconds.\n",
      "neural network update time: 0.19957590103149414 seconds.\n",
      "eval score on batch: 0.1937569465886042\n",
      "epsilons agent1 and agent2: 0.10211360648950807 0.10211360648950807\n",
      "646/646 [==============================] - 0s 41us/step\n",
      "646/646 [==============================] - 0s 38us/step\n",
      "updating with 646, 646 training batch.\n",
      "Epoch 1/1\n",
      "646/646 [==============================] - 0s 160us/step - loss: 0.2085\n",
      "Epoch 1/1\n",
      "646/646 [==============================] - 0s 160us/step - loss: 0.2077\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 54.97578716278076 seconds.\n",
      "neural network update time: 0.21211814880371094 seconds.\n",
      "eval score on batch: 0.21506307648938877\n",
      "epsilons agent1 and agent2: 0.10160303845706052 0.10160303845706052\n",
      "657/657 [==============================] - 0s 41us/step\n",
      "657/657 [==============================] - 0s 38us/step\n",
      "updating with 657, 657 training batch.\n",
      "Epoch 1/1\n",
      "657/657 [==============================] - 0s 159us/step - loss: 0.2068\n",
      "Epoch 1/1\n",
      "657/657 [==============================] - 0s 155us/step - loss: 0.2078\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 53.94638800621033 seconds.\n",
      "neural network update time: 0.21201610565185547 seconds.\n",
      "eval score on batch: 0.20807948366907542\n",
      "epsilons agent1 and agent2: 0.10109502326477522 0.10109502326477522\n",
      "655/655 [==============================] - 0s 41us/step\n",
      "655/655 [==============================] - 0s 42us/step\n",
      "updating with 655, 655 training batch.\n",
      "Epoch 1/1\n",
      "655/655 [==============================] - 0s 159us/step - loss: 0.2043\n",
      "Epoch 1/1\n",
      "655/655 [==============================] - 0s 156us/step - loss: 0.2049\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 53.70674777030945 seconds.\n",
      "neural network update time: 0.21228790283203125 seconds.\n",
      "eval score on batch: 0.2055806480859982\n",
      "epsilons agent1 and agent2: 0.10058954814845135 0.10058954814845135\n",
      "776/776 [==============================] - 0s 38us/step\n",
      "776/776 [==============================] - 0s 41us/step\n",
      "updating with 776, 776 training batch.\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 156us/step - loss: 0.1891\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 156us/step - loss: 0.1897\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 55.483826875686646 seconds.\n",
      "neural network update time: 0.24831104278564453 seconds.\n",
      "eval score on batch: 0.1900100608538721\n",
      "epsilons agent1 and agent2: 0.10008660040770909 0.10008660040770909\n",
      "773/773 [==============================] - 0s 42us/step\n",
      "773/773 [==============================] - 0s 43us/step\n",
      "updating with 773, 773 training batch.\n",
      "Epoch 1/1\n",
      "773/773 [==============================] - 0s 160us/step - loss: 0.2022\n",
      "Epoch 1/1\n",
      "773/773 [==============================] - 0s 159us/step - loss: 0.2022\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 54.14992833137512 seconds.\n",
      "neural network update time: 0.2533080577850342 seconds.\n",
      "eval score on batch: 0.20191319718930442\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "749/749 [==============================] - 0s 42us/step\n",
      "749/749 [==============================] - 0s 37us/step\n",
      "updating with 749, 749 training batch.\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 159us/step - loss: 0.1903\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 160us/step - loss: 0.1897\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 56.18523192405701 seconds.\n",
      "neural network update time: 0.2453601360321045 seconds.\n",
      "eval score on batch: 0.19049503026562475\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "736/736 [==============================] - 0s 41us/step\n",
      "736/736 [==============================] - 0s 36us/step\n",
      "updating with 736, 736 training batch.\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 156us/step - loss: 0.1929\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 151us/step - loss: 0.1940\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 55.909061908721924 seconds.\n",
      "neural network update time: 0.2312760353088379 seconds.\n",
      "eval score on batch: 0.19387733418008554\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "705/705 [==============================] - 0s 46us/step\n",
      "705/705 [==============================] - 0s 38us/step\n",
      "updating with 705, 705 training batch.\n",
      "Epoch 1/1\n",
      "705/705 [==============================] - 0s 160us/step - loss: 0.2016\n",
      "Epoch 1/1\n",
      "705/705 [==============================] - 0s 158us/step - loss: 0.2011\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 54.581258058547974 seconds.\n",
      "neural network update time: 0.22968101501464844 seconds.\n",
      "eval score on batch: 0.20110697198315716\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "69 outputs loaded.\n",
      "763/763 [==============================] - 0s 40us/step\n",
      "763/763 [==============================] - 0s 38us/step\n",
      "updating with 763, 763 training batch.\n",
      "Epoch 1/1\n",
      "763/763 [==============================] - 0s 156us/step - loss: 0.1939\n",
      "Epoch 1/1\n",
      "763/763 [==============================] - 0s 153us/step - loss: 0.1871\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 55.456382036209106 seconds.\n",
      "neural network update time: 0.24077486991882324 seconds.\n",
      "eval score on batch: 0.19170877159735478\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "811/811 [==============================] - 0s 40us/step\n",
      "811/811 [==============================] - 0s 43us/step\n",
      "updating with 811, 811 training batch.\n",
      "Epoch 1/1\n",
      "811/811 [==============================] - 0s 167us/step - loss: 0.1846\n",
      "Epoch 1/1\n",
      "811/811 [==============================] - 0s 173us/step - loss: 0.1826\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 56.93743395805359 seconds.\n",
      "neural network update time: 0.280792236328125 seconds.\n",
      "eval score on batch: 0.18578725776252705\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "704/704 [==============================] - 0s 41us/step\n",
      "704/704 [==============================] - 0s 36us/step\n",
      "updating with 704, 704 training batch.\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 154us/step - loss: 0.1939\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 158us/step - loss: 0.1933\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 55.390746116638184 seconds.\n",
      "neural network update time: 0.22543096542358398 seconds.\n",
      "eval score on batch: 0.19385008107532153\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "644/644 [==============================] - 0s 42us/step\n",
      "644/644 [==============================] - 0s 38us/step\n",
      "updating with 644, 644 training batch.\n",
      "Epoch 1/1\n",
      "644/644 [==============================] - 0s 163us/step - loss: 0.1908\n",
      "Epoch 1/1\n",
      "644/644 [==============================] - 0s 158us/step - loss: 0.1883\n",
      "player 1 wins: 16\n",
      "player 2 wins: 8\n",
      "simulations took 55.457635164260864 seconds.\n",
      "neural network update time: 0.21297001838684082 seconds.\n",
      "eval score on batch: 0.20267646394160962\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "760/760 [==============================] - 0s 42us/step\n",
      "760/760 [==============================] - 0s 47us/step\n",
      "updating with 760, 760 training batch.\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 153us/step - loss: 0.1939\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 156us/step - loss: 0.1933\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 56.41658973693848 seconds.\n",
      "neural network update time: 0.2402498722076416 seconds.\n",
      "eval score on batch: 0.21567234051854986\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "535/535 [==============================] - 0s 40us/step\n",
      "535/535 [==============================] - 0s 41us/step\n",
      "updating with 535, 535 training batch.\n",
      "Epoch 1/1\n",
      "535/535 [==============================] - 0s 156us/step - loss: 0.2068\n",
      "Epoch 1/1\n",
      "535/535 [==============================] - 0s 156us/step - loss: 0.2064\n",
      "player 1 wins: 12\n",
      "player 2 wins: 9\n",
      "simulations took 54.90939974784851 seconds.\n",
      "neural network update time: 0.17278265953063965 seconds.\n",
      "eval score on batch: 0.2140873056357709\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "653/653 [==============================] - 0s 44us/step\n",
      "653/653 [==============================] - 0s 38us/step\n",
      "updating with 653, 653 training batch.\n",
      "Epoch 1/1\n",
      "653/653 [==============================] - 0s 157us/step - loss: 0.2096\n",
      "Epoch 1/1\n",
      "653/653 [==============================] - 0s 154us/step - loss: 0.2109\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 53.42650818824768 seconds.\n",
      "neural network update time: 0.20941829681396484 seconds.\n",
      "eval score on batch: 0.21046824388314397\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "674/674 [==============================] - 0s 40us/step\n",
      "674/674 [==============================] - 0s 44us/step\n",
      "updating with 674, 674 training batch.\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 164us/step - loss: 0.1961\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 161us/step - loss: 0.1951\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 54.57648205757141 seconds.\n",
      "neural network update time: 0.22400689125061035 seconds.\n",
      "eval score on batch: 0.20158529910604395\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "800/800 [==============================] - 0s 37us/step\n",
      "800/800 [==============================] - 0s 40us/step\n",
      "updating with 800, 800 training batch.\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 154us/step - loss: 0.1988\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 0s 153us/step - loss: 0.1995\n",
      "player 1 wins: 18\n",
      "player 2 wins: 11\n",
      "simulations took 55.73538517951965 seconds.\n",
      "neural network update time: 0.2525498867034912 seconds.\n",
      "eval score on batch: 0.22384193152189255\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "708/708 [==============================] - 0s 44us/step\n",
      "708/708 [==============================] - 0s 42us/step\n",
      "updating with 708, 708 training batch.\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 155us/step - loss: 0.1718\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 160us/step - loss: 0.1716\n",
      "player 1 wins: 17\n",
      "player 2 wins: 8\n",
      "simulations took 56.752321004867554 seconds.\n",
      "neural network update time: 0.2281041145324707 seconds.\n",
      "eval score on batch: 0.17126514558485673\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "829/829 [==============================] - 0s 43us/step\n",
      "829/829 [==============================] - 0s 37us/step\n",
      "updating with 829, 829 training batch.\n",
      "Epoch 1/1\n",
      "829/829 [==============================] - 0s 155us/step - loss: 0.1737\n",
      "Epoch 1/1\n",
      "829/829 [==============================] - 0s 156us/step - loss: 0.1732\n",
      "player 1 wins: 19\n",
      "player 2 wins: 10\n",
      "simulations took 57.00032877922058 seconds.\n",
      "neural network update time: 0.26311802864074707 seconds.\n",
      "eval score on batch: 0.17274952989938894\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "591/591 [==============================] - 0s 40us/step\n",
      "591/591 [==============================] - 0s 41us/step\n",
      "updating with 591, 591 training batch.\n",
      "Epoch 1/1\n",
      "591/591 [==============================] - 0s 160us/step - loss: 0.1979\n",
      "Epoch 1/1\n",
      "591/591 [==============================] - 0s 159us/step - loss: 0.1965\n",
      "player 1 wins: 12\n",
      "player 2 wins: 10\n",
      "simulations took 55.88120698928833 seconds.\n",
      "neural network update time: 0.19346308708190918 seconds.\n",
      "eval score on batch: 0.19719523037649855\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "719/719 [==============================] - 0s 37us/step\n",
      "719/719 [==============================] - 0s 40us/step\n",
      "updating with 719, 719 training batch.\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 162us/step - loss: 0.2031\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 156us/step - loss: 0.2041\n",
      "player 1 wins: 12\n",
      "player 2 wins: 16\n",
      "simulations took 54.2302188873291 seconds.\n",
      "neural network update time: 0.23546385765075684 seconds.\n",
      "eval score on batch: 0.20352335155051673\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "758/758 [==============================] - 0s 38us/step\n",
      "758/758 [==============================] - 0s 42us/step\n",
      "updating with 758, 758 training batch.\n",
      "Epoch 1/1\n",
      "758/758 [==============================] - 0s 152us/step - loss: 0.1858\n",
      "Epoch 1/1\n",
      "758/758 [==============================] - 0s 156us/step - loss: 0.1858\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 56.172313928604126 seconds.\n",
      "neural network update time: 0.23987102508544922 seconds.\n",
      "eval score on batch: 0.18592458265516562\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "652/652 [==============================] - 0s 46us/step\n",
      "652/652 [==============================] - 0s 37us/step\n",
      "updating with 652, 652 training batch.\n",
      "Epoch 1/1\n",
      "652/652 [==============================] - 0s 154us/step - loss: 0.1922\n",
      "Epoch 1/1\n",
      "652/652 [==============================] - 0s 157us/step - loss: 0.1931\n",
      "player 1 wins: 9\n",
      "player 2 wins: 16\n",
      "simulations took 54.94383692741394 seconds.\n",
      "neural network update time: 0.2081770896911621 seconds.\n",
      "eval score on batch: 0.19393308439000617\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "705/705 [==============================] - 0s 50us/step\n",
      "705/705 [==============================] - 0s 46us/step\n",
      "updating with 705, 705 training batch.\n",
      "Epoch 1/1\n",
      "705/705 [==============================] - 0s 180us/step - loss: 0.2038\n",
      "Epoch 1/1\n",
      "705/705 [==============================] - 0s 173us/step - loss: 0.2032\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 54.5586678981781 seconds.\n",
      "neural network update time: 0.2544882297515869 seconds.\n",
      "eval score on batch: 0.20908651291780794\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "810/810 [==============================] - 0s 42us/step\n",
      "810/810 [==============================] - 0s 43us/step\n",
      "updating with 810, 810 training batch.\n",
      "Epoch 1/1\n",
      "810/810 [==============================] - 0s 159us/step - loss: 0.1960\n",
      "Epoch 1/1\n",
      "810/810 [==============================] - 0s 158us/step - loss: 0.1937\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 54.51311922073364 seconds.\n",
      "neural network update time: 0.26268601417541504 seconds.\n",
      "eval score on batch: 0.19302884593927933\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "767/767 [==============================] - 0s 41us/step\n",
      "767/767 [==============================] - 0s 37us/step\n",
      "updating with 767, 767 training batch.\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 156us/step - loss: 0.1980\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 152us/step - loss: 0.1997\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 54.263474225997925 seconds.\n",
      "neural network update time: 0.24142098426818848 seconds.\n",
      "eval score on batch: 0.20083459300587667\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "679/679 [==============================] - 0s 47us/step\n",
      "679/679 [==============================] - 0s 37us/step\n",
      "updating with 679, 679 training batch.\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 158us/step - loss: 0.1924\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 157us/step - loss: 0.1921\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 56.679807901382446 seconds.\n",
      "neural network update time: 0.21913599967956543 seconds.\n",
      "eval score on batch: 0.1914827550764145\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "728/728 [==============================] - 0s 39us/step\n",
      "728/728 [==============================] - 0s 39us/step\n",
      "updating with 728, 728 training batch.\n",
      "Epoch 1/1\n",
      "728/728 [==============================] - 0s 152us/step - loss: 0.2107\n",
      "Epoch 1/1\n",
      "728/728 [==============================] - 0s 157us/step - loss: 0.2100\n",
      "player 1 wins: 16\n",
      "player 2 wins: 13\n",
      "simulations took 53.228513956069946 seconds.\n",
      "neural network update time: 0.23079204559326172 seconds.\n",
      "eval score on batch: 0.21240115263959863\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "70 outputs loaded.\n",
      "802/802 [==============================] - 0s 46us/step\n",
      "802/802 [==============================] - 0s 38us/step\n",
      "updating with 802, 802 training batch.\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 158us/step - loss: 0.1849\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 158us/step - loss: 0.1836\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 56.3591251373291 seconds.\n",
      "neural network update time: 0.25995612144470215 seconds.\n",
      "eval score on batch: 0.1872669006337846\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "805/805 [==============================] - 0s 41us/step\n",
      "805/805 [==============================] - 0s 39us/step\n",
      "updating with 805, 805 training batch.\n",
      "Epoch 1/1\n",
      "805/805 [==============================] - 0s 158us/step - loss: 0.1849\n",
      "Epoch 1/1\n",
      "805/805 [==============================] - 0s 158us/step - loss: 0.1829\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 56.961028814315796 seconds.\n",
      "neural network update time: 0.2596700191497803 seconds.\n",
      "eval score on batch: 0.18662611634736434\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "717/717 [==============================] - 0s 44us/step\n",
      "717/717 [==============================] - 0s 40us/step\n",
      "updating with 717, 717 training batch.\n",
      "Epoch 1/1\n",
      "717/717 [==============================] - 0s 153us/step - loss: 0.2017\n",
      "Epoch 1/1\n",
      "717/717 [==============================] - 0s 156us/step - loss: 0.2013\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 54.049315214157104 seconds.\n",
      "neural network update time: 0.22666597366333008 seconds.\n",
      "eval score on batch: 0.20112087915429386\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "765/765 [==============================] - 0s 46us/step\n",
      "765/765 [==============================] - 0s 38us/step\n",
      "updating with 765, 765 training batch.\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 156us/step - loss: 0.1841\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 158us/step - loss: 0.1840\n",
      "player 1 wins: 10\n",
      "player 2 wins: 17\n",
      "simulations took 56.899943351745605 seconds.\n",
      "neural network update time: 0.24588799476623535 seconds.\n",
      "eval score on batch: 0.19706538553331412\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "716/716 [==============================] - 0s 45us/step\n",
      "716/716 [==============================] - 0s 40us/step\n",
      "updating with 716, 716 training batch.\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 155us/step - loss: 0.2032\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 156us/step - loss: 0.2022\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 55.37398099899292 seconds.\n",
      "neural network update time: 0.22786879539489746 seconds.\n",
      "eval score on batch: 0.2201529914024156\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "744/744 [==============================] - 0s 39us/step\n",
      "744/744 [==============================] - 0s 40us/step\n",
      "updating with 744, 744 training batch.\n",
      "Epoch 1/1\n",
      "744/744 [==============================] - 0s 161us/step - loss: 0.1973\n",
      "Epoch 1/1\n",
      "744/744 [==============================] - 0s 158us/step - loss: 0.2015\n",
      "player 1 wins: 12\n",
      "player 2 wins: 16\n",
      "simulations took 54.69838213920593 seconds.\n",
      "neural network update time: 0.24401187896728516 seconds.\n",
      "eval score on batch: 0.20861137714437258\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "678/678 [==============================] - 0s 44us/step\n",
      "678/678 [==============================] - 0s 38us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 159us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.1927\n",
      "player 1 wins: 14\n",
      "player 2 wins: 11\n",
      "simulations took 56.160155057907104 seconds.\n",
      "neural network update time: 0.21880507469177246 seconds.\n",
      "eval score on batch: 0.20146747109559873\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "700/700 [==============================] - 0s 38us/step\n",
      "700/700 [==============================] - 0s 54us/step\n",
      "updating with 700, 700 training batch.\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 160us/step - loss: 0.1938\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 167us/step - loss: 0.1941\n",
      "player 1 wins: 17\n",
      "player 2 wins: 10\n",
      "simulations took 54.66616415977478 seconds.\n",
      "neural network update time: 0.2349848747253418 seconds.\n",
      "eval score on batch: 0.1933330730029515\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "721/721 [==============================] - 0s 40us/step\n",
      "721/721 [==============================] - 0s 40us/step\n",
      "updating with 721, 721 training batch.\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 160us/step - loss: 0.1927\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 162us/step - loss: 0.1915\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 57.521838903427124 seconds.\n",
      "neural network update time: 0.23803019523620605 seconds.\n",
      "eval score on batch: 0.21755357166227784\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "638/638 [==============================] - 0s 43us/step\n",
      "638/638 [==============================] - 0s 36us/step\n",
      "updating with 638, 638 training batch.\n",
      "Epoch 1/1\n",
      "638/638 [==============================] - 0s 151us/step - loss: 0.1909\n",
      "Epoch 1/1\n",
      "638/638 [==============================] - 0s 155us/step - loss: 0.1947\n",
      "player 1 wins: 9\n",
      "player 2 wins: 15\n",
      "simulations took 55.36475110054016 seconds.\n",
      "neural network update time: 0.20068812370300293 seconds.\n",
      "eval score on batch: 0.19435792596175752\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "783/783 [==============================] - 0s 40us/step\n",
      "783/783 [==============================] - 0s 40us/step\n",
      "updating with 783, 783 training batch.\n",
      "Epoch 1/1\n",
      "783/783 [==============================] - 0s 161us/step - loss: 0.1911\n",
      "Epoch 1/1\n",
      "783/783 [==============================] - 0s 154us/step - loss: 0.1911\n",
      "player 1 wins: 19\n",
      "player 2 wins: 10\n",
      "simulations took 55.083484172821045 seconds.\n",
      "neural network update time: 0.25263214111328125 seconds.\n",
      "eval score on batch: 0.21088116439975238\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "663/663 [==============================] - 0s 45us/step\n",
      "663/663 [==============================] - 0s 38us/step\n",
      "updating with 663, 663 training batch.\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 156us/step - loss: 0.2066\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 164us/step - loss: 0.2027\n",
      "player 1 wins: 13\n",
      "player 2 wins: 12\n",
      "simulations took 55.292888879776 seconds.\n",
      "neural network update time: 0.21876287460327148 seconds.\n",
      "eval score on batch: 0.2234298598083076\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "694/694 [==============================] - 0s 44us/step\n",
      "694/694 [==============================] - 0s 40us/step\n",
      "updating with 694, 694 training batch.\n",
      "Epoch 1/1\n",
      "694/694 [==============================] - 0s 161us/step - loss: 0.2092\n",
      "Epoch 1/1\n",
      "694/694 [==============================] - 0s 167us/step - loss: 0.2137\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 54.507498025894165 seconds.\n",
      "neural network update time: 0.23365020751953125 seconds.\n",
      "eval score on batch: 0.2208920461300127\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "738/738 [==============================] - 0s 40us/step\n",
      "738/738 [==============================] - 0s 42us/step\n",
      "updating with 738, 738 training batch.\n",
      "Epoch 1/1\n",
      "738/738 [==============================] - 0s 157us/step - loss: 0.1936\n",
      "Epoch 1/1\n",
      "738/738 [==============================] - 0s 158us/step - loss: 0.1973\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 57.619709968566895 seconds.\n",
      "neural network update time: 0.2379741668701172 seconds.\n",
      "eval score on batch: 0.20105390959396596\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "674/674 [==============================] - 0s 48us/step\n",
      "674/674 [==============================] - 0s 36us/step\n",
      "updating with 674, 674 training batch.\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 171us/step - loss: 0.1846\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 171us/step - loss: 0.1873\n",
      "player 1 wins: 9\n",
      "player 2 wins: 16\n",
      "simulations took 56.303781032562256 seconds.\n",
      "neural network update time: 0.23699212074279785 seconds.\n",
      "eval score on batch: 0.1872324744943814\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "756/756 [==============================] - 0s 38us/step\n",
      "756/756 [==============================] - 0s 39us/step\n",
      "updating with 756, 756 training batch.\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 153us/step - loss: 0.1981\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 159us/step - loss: 0.1970\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 55.60679006576538 seconds.\n",
      "neural network update time: 0.24194931983947754 seconds.\n",
      "eval score on batch: 0.21488223047483535\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "706/706 [==============================] - 0s 37us/step\n",
      "706/706 [==============================] - 0s 40us/step\n",
      "updating with 706, 706 training batch.\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 167us/step - loss: 0.2036\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 177us/step - loss: 0.2056\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 56.763099908828735 seconds.\n",
      "neural network update time: 0.24985718727111816 seconds.\n",
      "eval score on batch: 0.21863331892672747\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "771/771 [==============================] - 0s 46us/step\n",
      "771/771 [==============================] - 0s 40us/step\n",
      "updating with 771, 771 training batch.\n",
      "Epoch 1/1\n",
      "771/771 [==============================] - 0s 169us/step - loss: 0.1837\n",
      "Epoch 1/1\n",
      "771/771 [==============================] - 0s 167us/step - loss: 0.1845\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 60.735918045043945 seconds.\n",
      "neural network update time: 0.26436614990234375 seconds.\n",
      "eval score on batch: 0.20097691875951298\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "713/713 [==============================] - 0s 45us/step\n",
      "713/713 [==============================] - 0s 38us/step\n",
      "updating with 713, 713 training batch.\n",
      "Epoch 1/1\n",
      "713/713 [==============================] - 0s 155us/step - loss: 0.1984\n",
      "Epoch 1/1\n",
      "713/713 [==============================] - 0s 156us/step - loss: 0.1953\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 56.63392996788025 seconds.\n",
      "neural network update time: 0.2272660732269287 seconds.\n",
      "eval score on batch: 0.19945062310820208\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "666/666 [==============================] - 0s 40us/step\n",
      "666/666 [==============================] - 0s 42us/step\n",
      "updating with 666, 666 training batch.\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 162us/step - loss: 0.1931\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 169us/step - loss: 0.1943\n",
      "player 1 wins: 15\n",
      "player 2 wins: 10\n",
      "simulations took 56.7800989151001 seconds.\n",
      "neural network update time: 0.2259521484375 seconds.\n",
      "eval score on batch: 0.20039190684218663\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "71 outputs loaded.\n",
      "741/741 [==============================] - 0s 43us/step\n",
      "741/741 [==============================] - 0s 39us/step\n",
      "updating with 741, 741 training batch.\n",
      "Epoch 1/1\n",
      "741/741 [==============================] - 0s 157us/step - loss: 0.1945\n",
      "Epoch 1/1\n",
      "741/741 [==============================] - 0s 159us/step - loss: 0.1927\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 55.766952991485596 seconds.\n",
      "neural network update time: 0.240692138671875 seconds.\n",
      "eval score on batch: 0.19615606265331087\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "816/816 [==============================] - 0s 41us/step\n",
      "816/816 [==============================] - 0s 38us/step\n",
      "updating with 816, 816 training batch.\n",
      "Epoch 1/1\n",
      "816/816 [==============================] - 0s 161us/step - loss: 0.1999\n",
      "Epoch 1/1\n",
      "816/816 [==============================] - 0s 158us/step - loss: 0.1986\n",
      "player 1 wins: 15\n",
      "player 2 wins: 16\n",
      "simulations took 53.67730093002319 seconds.\n",
      "neural network update time: 0.2661600112915039 seconds.\n",
      "eval score on batch: 0.19808187571299427\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "748/748 [==============================] - 0s 43us/step\n",
      "748/748 [==============================] - 0s 37us/step\n",
      "updating with 748, 748 training batch.\n",
      "Epoch 1/1\n",
      "748/748 [==============================] - 0s 158us/step - loss: 0.1866\n",
      "Epoch 1/1\n",
      "748/748 [==============================] - 0s 157us/step - loss: 0.1855\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 55.871232986450195 seconds.\n",
      "neural network update time: 0.2413339614868164 seconds.\n",
      "eval score on batch: 0.1864061704811884\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "729/729 [==============================] - 0s 49us/step\n",
      "729/729 [==============================] - 0s 36us/step\n",
      "updating with 729, 729 training batch.\n",
      "Epoch 1/1\n",
      "729/729 [==============================] - 0s 158us/step - loss: 0.1949\n",
      "Epoch 1/1\n",
      "729/729 [==============================] - 0s 178us/step - loss: 0.1961\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 55.52234697341919 seconds.\n",
      "neural network update time: 0.2496640682220459 seconds.\n",
      "eval score on batch: 0.20395552607148437\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "700/700 [==============================] - 0s 43us/step\n",
      "700/700 [==============================] - 0s 36us/step\n",
      "updating with 700, 700 training batch.\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 156us/step - loss: 0.1918\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - 0s 154us/step - loss: 0.1919\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 55.86533999443054 seconds.\n",
      "neural network update time: 0.2241830825805664 seconds.\n",
      "eval score on batch: 0.19145943880081176\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "721/721 [==============================] - 0s 43us/step\n",
      "721/721 [==============================] - 0s 39us/step\n",
      "updating with 721, 721 training batch.\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 152us/step - loss: 0.1984\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 158us/step - loss: 0.1971\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 53.960294246673584 seconds.\n",
      "neural network update time: 0.22833776473999023 seconds.\n",
      "eval score on batch: 0.19761743275616603\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "751/751 [==============================] - 0s 38us/step\n",
      "751/751 [==============================] - 0s 43us/step\n",
      "updating with 751, 751 training batch.\n",
      "Epoch 1/1\n",
      "751/751 [==============================] - 0s 172us/step - loss: 0.1995\n",
      "Epoch 1/1\n",
      "751/751 [==============================] - 0s 170us/step - loss: 0.1964\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 55.04324722290039 seconds.\n",
      "neural network update time: 0.262800931930542 seconds.\n",
      "eval score on batch: 0.206947661122771\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "784/784 [==============================] - 0s 41us/step\n",
      "784/784 [==============================] - 0s 39us/step\n",
      "updating with 784, 784 training batch.\n",
      "Epoch 1/1\n",
      "784/784 [==============================] - 0s 157us/step - loss: 0.1949\n",
      "Epoch 1/1\n",
      "784/784 [==============================] - 0s 153us/step - loss: 0.1962\n",
      "player 1 wins: 19\n",
      "player 2 wins: 11\n",
      "simulations took 53.941476821899414 seconds.\n",
      "neural network update time: 0.24886202812194824 seconds.\n",
      "eval score on batch: 0.2184876296289113\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "744/744 [==============================] - 0s 38us/step\n",
      "744/744 [==============================] - 0s 40us/step\n",
      "updating with 744, 744 training batch.\n",
      "Epoch 1/1\n",
      "744/744 [==============================] - 0s 155us/step - loss: 0.1975\n",
      "Epoch 1/1\n",
      "744/744 [==============================] - 0s 160us/step - loss: 0.2025\n",
      "player 1 wins: 10\n",
      "player 2 wins: 17\n",
      "simulations took 55.79742789268494 seconds.\n",
      "neural network update time: 0.23944902420043945 seconds.\n",
      "eval score on batch: 0.23725121396203194\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "647/647 [==============================] - 0s 38us/step\n",
      "647/647 [==============================] - 0s 42us/step\n",
      "updating with 647, 647 training batch.\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 161us/step - loss: 0.2285\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 162us/step - loss: 0.2306\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 53.58799409866333 seconds.\n",
      "neural network update time: 0.21374917030334473 seconds.\n",
      "eval score on batch: 0.2610223600079508\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "768/768 [==============================] - 0s 40us/step\n",
      "768/768 [==============================] - 0s 38us/step\n",
      "updating with 768, 768 training batch.\n",
      "Epoch 1/1\n",
      "768/768 [==============================] - 0s 155us/step - loss: 0.2085\n",
      "Epoch 1/1\n",
      "768/768 [==============================] - 0s 153us/step - loss: 0.2093\n",
      "player 1 wins: 13\n",
      "player 2 wins: 17\n",
      "simulations took 53.13646721839905 seconds.\n",
      "neural network update time: 0.24200987815856934 seconds.\n",
      "eval score on batch: 0.22912313602864742\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "723/723 [==============================] - 0s 44us/step\n",
      "723/723 [==============================] - 0s 40us/step\n",
      "updating with 723, 723 training batch.\n",
      "Epoch 1/1\n",
      "723/723 [==============================] - 0s 156us/step - loss: 0.2054\n",
      "Epoch 1/1\n",
      "723/723 [==============================] - 0s 156us/step - loss: 0.2037\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 54.56215310096741 seconds.\n",
      "neural network update time: 0.23064303398132324 seconds.\n",
      "eval score on batch: 0.21077998977973747\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "680/680 [==============================] - 0s 44us/step\n",
      "680/680 [==============================] - 0s 37us/step\n",
      "updating with 680, 680 training batch.\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 161us/step - loss: 0.2084\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 157us/step - loss: 0.2085\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 53.868571043014526 seconds.\n",
      "neural network update time: 0.22234702110290527 seconds.\n",
      "eval score on batch: 0.21058404517962653\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "719/719 [==============================] - 0s 41us/step\n",
      "719/719 [==============================] - 0s 38us/step\n",
      "updating with 719, 719 training batch.\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 155us/step - loss: 0.1978\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 155us/step - loss: 0.1962\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 54.7541389465332 seconds.\n",
      "neural network update time: 0.22825980186462402 seconds.\n",
      "eval score on batch: 0.20354787628937082\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "789/789 [==============================] - 0s 42us/step\n",
      "789/789 [==============================] - 0s 39us/step\n",
      "updating with 789, 789 training batch.\n",
      "Epoch 1/1\n",
      "789/789 [==============================] - 0s 152us/step - loss: 0.1763\n",
      "Epoch 1/1\n",
      "789/789 [==============================] - 0s 155us/step - loss: 0.1870\n",
      "player 1 wins: 8\n",
      "player 2 wins: 20\n",
      "simulations took 56.04584789276123 seconds.\n",
      "neural network update time: 0.24899697303771973 seconds.\n",
      "eval score on batch: 0.21709788867823676\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "730/730 [==============================] - 0s 41us/step\n",
      "730/730 [==============================] - 0s 47us/step\n",
      "updating with 730, 730 training batch.\n",
      "Epoch 1/1\n",
      "730/730 [==============================] - 0s 223us/step - loss: 0.2274\n",
      "Epoch 1/1\n",
      "730/730 [==============================] - 0s 225us/step - loss: 0.2153\n",
      "player 1 wins: 16\n",
      "player 2 wins: 13\n",
      "simulations took 53.199294090270996 seconds.\n",
      "neural network update time: 0.3331437110900879 seconds.\n",
      "eval score on batch: 0.26403090357780457\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "716/716 [==============================] - 0s 46us/step\n",
      "716/716 [==============================] - 0s 35us/step\n",
      "updating with 716, 716 training batch.\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 161us/step - loss: 0.1888\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 162us/step - loss: 0.1897\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 56.251782178878784 seconds.\n",
      "neural network update time: 0.2362048625946045 seconds.\n",
      "eval score on batch: 0.19055189352544993\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "838/838 [==============================] - 0s 43us/step\n",
      "838/838 [==============================] - 0s 39us/step\n",
      "updating with 838, 838 training batch.\n",
      "Epoch 1/1\n",
      "838/838 [==============================] - 0s 163us/step - loss: 0.1881\n",
      "Epoch 1/1\n",
      "838/838 [==============================] - 0s 160us/step - loss: 0.1884\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 56.55686593055725 seconds.\n",
      "neural network update time: 0.2760131359100342 seconds.\n",
      "eval score on batch: 0.18743226645241529\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "668/668 [==============================] - 0s 49us/step\n",
      "668/668 [==============================] - 0s 41us/step\n",
      "updating with 668, 668 training batch.\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 0s 177us/step - loss: 0.2025\n",
      "Epoch 1/1\n",
      "668/668 [==============================] - 0s 165us/step - loss: 0.2038\n",
      "player 1 wins: 14\n",
      "player 2 wins: 12\n",
      "simulations took 54.38922905921936 seconds.\n",
      "neural network update time: 0.23398995399475098 seconds.\n",
      "eval score on batch: 0.2021798874268275\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "503/503 [==============================] - 0s 43us/step\n",
      "503/503 [==============================] - 0s 41us/step\n",
      "updating with 503, 503 training batch.\n",
      "Epoch 1/1\n",
      "503/503 [==============================] - 0s 153us/step - loss: 0.1980\n",
      "Epoch 1/1\n",
      "503/503 [==============================] - 0s 160us/step - loss: 0.1985\n",
      "player 1 wins: 10\n",
      "player 2 wins: 9\n",
      "simulations took 56.46363282203674 seconds.\n",
      "neural network update time: 0.16217708587646484 seconds.\n",
      "eval score on batch: 0.1976082193004208\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "72 outputs loaded.\n",
      "692/692 [==============================] - 0s 40us/step\n",
      "692/692 [==============================] - 0s 39us/step\n",
      "updating with 692, 692 training batch.\n",
      "Epoch 1/1\n",
      "692/692 [==============================] - 0s 152us/step - loss: 0.1974\n",
      "Epoch 1/1\n",
      "692/692 [==============================] - 0s 154us/step - loss: 0.1966\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 55.274685859680176 seconds.\n",
      "neural network update time: 0.2163851261138916 seconds.\n",
      "eval score on batch: 0.19685530824037645\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "639/639 [==============================] - 0s 44us/step\n",
      "639/639 [==============================] - 0s 36us/step\n",
      "updating with 639, 639 training batch.\n",
      "Epoch 1/1\n",
      "639/639 [==============================] - 0s 150us/step - loss: 0.1849\n",
      "Epoch 1/1\n",
      "639/639 [==============================] - 0s 156us/step - loss: 0.1812\n",
      "player 1 wins: 7\n",
      "player 2 wins: 17\n",
      "simulations took 55.87221002578735 seconds.\n",
      "neural network update time: 0.201218843460083 seconds.\n",
      "eval score on batch: 0.19128954508494883\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "674/674 [==============================] - 0s 42us/step\n",
      "674/674 [==============================] - 0s 38us/step\n",
      "updating with 674, 674 training batch.\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 162us/step - loss: 0.1934\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 159us/step - loss: 0.1937\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 55.86642837524414 seconds.\n",
      "neural network update time: 0.22304606437683105 seconds.\n",
      "eval score on batch: 0.2015893344797791\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "580/580 [==============================] - 0s 47us/step\n",
      "580/580 [==============================] - 0s 37us/step\n",
      "updating with 580, 580 training batch.\n",
      "Epoch 1/1\n",
      "580/580 [==============================] - 0s 160us/step - loss: 0.1914\n",
      "Epoch 1/1\n",
      "580/580 [==============================] - 0s 164us/step - loss: 0.2008\n",
      "player 1 wins: 8\n",
      "player 2 wins: 14\n",
      "simulations took 55.91264319419861 seconds.\n",
      "neural network update time: 0.19306206703186035 seconds.\n",
      "eval score on batch: 0.19600948950853841\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "746/746 [==============================] - 0s 42us/step\n",
      "746/746 [==============================] - 0s 38us/step\n",
      "updating with 746, 746 training batch.\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 164us/step - loss: 0.1931\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 156us/step - loss: 0.1917\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 55.980167865753174 seconds.\n",
      "neural network update time: 0.2443399429321289 seconds.\n",
      "eval score on batch: 0.20533654545951915\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "765/765 [==============================] - 0s 39us/step\n",
      "765/765 [==============================] - 0s 35us/step\n",
      "updating with 765, 765 training batch.\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 159us/step - loss: 0.1942\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 151us/step - loss: 0.1934\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 55.85761213302612 seconds.\n",
      "neural network update time: 0.24289774894714355 seconds.\n",
      "eval score on batch: 0.19563645319221845\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "670/670 [==============================] - 0s 40us/step\n",
      "670/670 [==============================] - 0s 37us/step\n",
      "updating with 670, 670 training batch.\n",
      "Epoch 1/1\n",
      "670/670 [==============================] - 0s 151us/step - loss: 0.2020\n",
      "Epoch 1/1\n",
      "670/670 [==============================] - 0s 153us/step - loss: 0.2020\n",
      "player 1 wins: 14\n",
      "player 2 wins: 12\n",
      "simulations took 54.52586388587952 seconds.\n",
      "neural network update time: 0.2086658477783203 seconds.\n",
      "eval score on batch: 0.201639246495802\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "665/665 [==============================] - 0s 38us/step\n",
      "665/665 [==============================] - 0s 40us/step\n",
      "updating with 665, 665 training batch.\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 157us/step - loss: 0.1956\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 158us/step - loss: 0.1959\n",
      "player 1 wins: 14\n",
      "player 2 wins: 11\n",
      "simulations took 55.691978931427 seconds.\n",
      "neural network update time: 0.21571683883666992 seconds.\n",
      "eval score on batch: 0.19495297631150799\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "760/760 [==============================] - 0s 42us/step\n",
      "760/760 [==============================] - 0s 37us/step\n",
      "updating with 760, 760 training batch.\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 160us/step - loss: 0.1933\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 151us/step - loss: 0.1932\n",
      "player 1 wins: 11\n",
      "player 2 wins: 17\n",
      "simulations took 55.679407835006714 seconds.\n",
      "neural network update time: 0.24216699600219727 seconds.\n",
      "eval score on batch: 0.20148488973316395\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "715/715 [==============================] - 0s 44us/step\n",
      "715/715 [==============================] - 0s 38us/step\n",
      "updating with 715, 715 training batch.\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 158us/step - loss: 0.1958\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 159us/step - loss: 0.1991\n",
      "player 1 wins: 17\n",
      "player 2 wins: 10\n",
      "simulations took 55.597623109817505 seconds.\n",
      "neural network update time: 0.23202896118164062 seconds.\n",
      "eval score on batch: 0.22322967691229773\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "694/694 [==============================] - 0s 38us/step\n",
      "694/694 [==============================] - 0s 41us/step\n",
      "updating with 694, 694 training batch.\n",
      "Epoch 1/1\n",
      "694/694 [==============================] - 0s 153us/step - loss: 0.2057\n",
      "Epoch 1/1\n",
      "694/694 [==============================] - 0s 158us/step - loss: 0.2041\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 55.578542709350586 seconds.\n",
      "neural network update time: 0.22057485580444336 seconds.\n",
      "eval score on batch: 0.2174129366058781\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "759/759 [==============================] - 0s 37us/step\n",
      "759/759 [==============================] - 0s 39us/step\n",
      "updating with 759, 759 training batch.\n",
      "Epoch 1/1\n",
      "759/759 [==============================] - 0s 155us/step - loss: 0.1917\n",
      "Epoch 1/1\n",
      "759/759 [==============================] - 0s 157us/step - loss: 0.1907\n",
      "player 1 wins: 12\n",
      "player 2 wins: 16\n",
      "simulations took 55.84432601928711 seconds.\n",
      "neural network update time: 0.24240589141845703 seconds.\n",
      "eval score on batch: 0.1909634356088789\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "708/708 [==============================] - 0s 41us/step\n",
      "708/708 [==============================] - 0s 39us/step\n",
      "updating with 708, 708 training batch.\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 157us/step - loss: 0.1897\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 158us/step - loss: 0.1896\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 56.15262198448181 seconds.\n",
      "neural network update time: 0.22884607315063477 seconds.\n",
      "eval score on batch: 0.18935199769208533\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "766/766 [==============================] - 0s 46us/step\n",
      "766/766 [==============================] - 0s 43us/step\n",
      "updating with 766, 766 training batch.\n",
      "Epoch 1/1\n",
      "766/766 [==============================] - 0s 188us/step - loss: 0.1926\n",
      "Epoch 1/1\n",
      "766/766 [==============================] - 0s 176us/step - loss: 0.1912\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 55.40357708930969 seconds.\n",
      "neural network update time: 0.28658294677734375 seconds.\n",
      "eval score on batch: 0.20087765873100988\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "711/711 [==============================] - 0s 41us/step\n",
      "711/711 [==============================] - 0s 39us/step\n",
      "updating with 711, 711 training batch.\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 156us/step - loss: 0.1755\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 161us/step - loss: 0.1746\n",
      "player 1 wins: 18\n",
      "player 2 wins: 8\n",
      "simulations took 55.67711114883423 seconds.\n",
      "neural network update time: 0.2308809757232666 seconds.\n",
      "eval score on batch: 0.17634584659682037\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "801/801 [==============================] - 0s 44us/step\n",
      "801/801 [==============================] - 0s 44us/step\n",
      "updating with 801, 801 training batch.\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s 158us/step - loss: 0.2053\n",
      "Epoch 1/1\n",
      "801/801 [==============================] - 0s 156us/step - loss: 0.2036\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 55.17285490036011 seconds.\n",
      "neural network update time: 0.2585768699645996 seconds.\n",
      "eval score on batch: 0.2159772146892584\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "652/652 [==============================] - 0s 43us/step\n",
      "652/652 [==============================] - 0s 39us/step\n",
      "updating with 652, 652 training batch.\n",
      "Epoch 1/1\n",
      "652/652 [==============================] - 0s 154us/step - loss: 0.1963\n",
      "Epoch 1/1\n",
      "652/652 [==============================] - 0s 159us/step - loss: 0.1954\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 55.323280811309814 seconds.\n",
      "neural network update time: 0.20975089073181152 seconds.\n",
      "eval score on batch: 0.19630119350790246\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "600/600 [==============================] - 0s 38us/step\n",
      "600/600 [==============================] - 0s 63us/step\n",
      "updating with 600, 600 training batch.\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 184us/step - loss: 0.2009\n",
      "Epoch 1/1\n",
      "600/600 [==============================] - 0s 171us/step - loss: 0.1936\n",
      "player 1 wins: 17\n",
      "player 2 wins: 6\n",
      "simulations took 55.55482792854309 seconds.\n",
      "neural network update time: 0.21827077865600586 seconds.\n",
      "eval score on batch: 0.24056784749031068\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "697/697 [==============================] - 0s 45us/step\n",
      "697/697 [==============================] - 0s 40us/step\n",
      "updating with 697, 697 training batch.\n",
      "Epoch 1/1\n",
      "697/697 [==============================] - 0s 157us/step - loss: 0.2053\n",
      "Epoch 1/1\n",
      "697/697 [==============================] - 0s 153us/step - loss: 0.2081\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 54.37923002243042 seconds.\n",
      "neural network update time: 0.22144699096679688 seconds.\n",
      "eval score on batch: 0.2218839366475342\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "730/730 [==============================] - 0s 39us/step\n",
      "730/730 [==============================] - 0s 39us/step\n",
      "updating with 730, 730 training batch.\n",
      "Epoch 1/1\n",
      "730/730 [==============================] - 0s 152us/step - loss: 0.1946\n",
      "Epoch 1/1\n",
      "730/730 [==============================] - 0s 157us/step - loss: 0.1934\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 55.49326515197754 seconds.\n",
      "neural network update time: 0.2306978702545166 seconds.\n",
      "eval score on batch: 0.1948575684060789\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "73 outputs loaded.\n",
      "734/734 [==============================] - 0s 42us/step\n",
      "734/734 [==============================] - 0s 41us/step\n",
      "updating with 734, 734 training batch.\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 153us/step - loss: 0.1860\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 152us/step - loss: 0.1869\n",
      "player 1 wins: 18\n",
      "player 2 wins: 9\n",
      "simulations took 55.510164976119995 seconds.\n",
      "neural network update time: 0.22884368896484375 seconds.\n",
      "eval score on batch: 0.20066354239019452\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "710/710 [==============================] - 0s 42us/step\n",
      "710/710 [==============================] - 0s 40us/step\n",
      "updating with 710, 710 training batch.\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 161us/step - loss: 0.2084\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 165us/step - loss: 0.2064\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 53.38743495941162 seconds.\n",
      "neural network update time: 0.23659300804138184 seconds.\n",
      "eval score on batch: 0.2112775348218709\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "619/619 [==============================] - 0s 44us/step\n",
      "619/619 [==============================] - 0s 40us/step\n",
      "updating with 619, 619 training batch.\n",
      "Epoch 1/1\n",
      "619/619 [==============================] - 0s 156us/step - loss: 0.1945\n",
      "Epoch 1/1\n",
      "619/619 [==============================] - 0s 159us/step - loss: 0.1954\n",
      "player 1 wins: 11\n",
      "player 2 wins: 12\n",
      "simulations took 55.82903432846069 seconds.\n",
      "neural network update time: 0.20056915283203125 seconds.\n",
      "eval score on batch: 0.19749778284979397\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "703/703 [==============================] - 0s 43us/step\n",
      "703/703 [==============================] - 0s 39us/step\n",
      "updating with 703, 703 training batch.\n",
      "Epoch 1/1\n",
      "703/703 [==============================] - 0s 153us/step - loss: 0.1996\n",
      "Epoch 1/1\n",
      "703/703 [==============================] - 0s 155us/step - loss: 0.2000\n",
      "player 1 wins: 10\n",
      "player 2 wins: 18\n",
      "simulations took 54.12029695510864 seconds.\n",
      "neural network update time: 0.22185134887695312 seconds.\n",
      "eval score on batch: 0.2033838072465799\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "802/802 [==============================] - 0s 44us/step\n",
      "802/802 [==============================] - 0s 43us/step\n",
      "updating with 802, 802 training batch.\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 156us/step - loss: 0.2031\n",
      "Epoch 1/1\n",
      "802/802 [==============================] - 0s 159us/step - loss: 0.2018\n",
      "player 1 wins: 13\n",
      "player 2 wins: 18\n",
      "simulations took 54.238303899765015 seconds.\n",
      "neural network update time: 0.259523868560791 seconds.\n",
      "eval score on batch: 0.20504816978219942\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "650/650 [==============================] - 0s 43us/step\n",
      "650/650 [==============================] - 0s 39us/step\n",
      "updating with 650, 650 training batch.\n",
      "Epoch 1/1\n",
      "650/650 [==============================] - 0s 159us/step - loss: 0.1935\n",
      "Epoch 1/1\n",
      "650/650 [==============================] - 0s 164us/step - loss: 0.1930\n",
      "player 1 wins: 11\n",
      "player 2 wins: 13\n",
      "simulations took 56.13257598876953 seconds.\n",
      "neural network update time: 0.21510672569274902 seconds.\n",
      "eval score on batch: 0.19248715746288117\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "687/687 [==============================] - 0s 41us/step\n",
      "687/687 [==============================] - 0s 37us/step\n",
      "updating with 687, 687 training batch.\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 153us/step - loss: 0.1987\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 160us/step - loss: 0.1982\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 55.263312101364136 seconds.\n",
      "neural network update time: 0.22073101997375488 seconds.\n",
      "eval score on batch: 0.19814711947433292\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "566/566 [==============================] - 0s 43us/step\n",
      "566/566 [==============================] - 0s 38us/step\n",
      "updating with 566, 566 training batch.\n",
      "Epoch 1/1\n",
      "566/566 [==============================] - 0s 155us/step - loss: 0.1910\n",
      "Epoch 1/1\n",
      "566/566 [==============================] - 0s 157us/step - loss: 0.1910\n",
      "player 1 wins: 8\n",
      "player 2 wins: 13\n",
      "simulations took 56.2254159450531 seconds.\n",
      "neural network update time: 0.18190407752990723 seconds.\n",
      "eval score on batch: 0.19505574272504544\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "718/718 [==============================] - 0s 50us/step\n",
      "718/718 [==============================] - 0s 36us/step\n",
      "updating with 718, 718 training batch.\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 166us/step - loss: 0.2024\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 186us/step - loss: 0.2012\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 54.786125898361206 seconds.\n",
      "neural network update time: 0.2585110664367676 seconds.\n",
      "eval score on batch: 0.20687893568473273\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "714/714 [==============================] - 0s 41us/step\n",
      "714/714 [==============================] - 0s 37us/step\n",
      "updating with 714, 714 training batch.\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 155us/step - loss: 0.2051\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 155us/step - loss: 0.2060\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 53.747268199920654 seconds.\n",
      "neural network update time: 0.22672724723815918 seconds.\n",
      "eval score on batch: 0.2070140843657928\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "688/688 [==============================] - 0s 38us/step\n",
      "688/688 [==============================] - 0s 42us/step\n",
      "updating with 688, 688 training batch.\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 152us/step - loss: 0.1882\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 160us/step - loss: 0.1885\n",
      "player 1 wins: 9\n",
      "player 2 wins: 17\n",
      "simulations took 54.765743255615234 seconds.\n",
      "neural network update time: 0.22027063369750977 seconds.\n",
      "eval score on batch: 0.19110313802957535\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "645/645 [==============================] - 0s 40us/step\n",
      "645/645 [==============================] - 0s 41us/step\n",
      "updating with 645, 645 training batch.\n",
      "Epoch 1/1\n",
      "645/645 [==============================] - 0s 158us/step - loss: 0.2008\n",
      "Epoch 1/1\n",
      "645/645 [==============================] - 0s 174us/step - loss: 0.2027\n",
      "player 1 wins: 15\n",
      "player 2 wins: 9\n",
      "simulations took 56.01377511024475 seconds.\n",
      "neural network update time: 0.21974897384643555 seconds.\n",
      "eval score on batch: 0.22767739648201554\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "663/663 [==============================] - 0s 37us/step\n",
      "663/663 [==============================] - 0s 41us/step\n",
      "updating with 663, 663 training batch.\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 153us/step - loss: 0.2066\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 157us/step - loss: 0.2090\n",
      "player 1 wins: 8\n",
      "player 2 wins: 17\n",
      "simulations took 55.15020799636841 seconds.\n",
      "neural network update time: 0.21056485176086426 seconds.\n",
      "eval score on batch: 0.24778653688020835\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "631/631 [==============================] - 0s 45us/step\n",
      "631/631 [==============================] - 0s 36us/step\n",
      "updating with 631, 631 training batch.\n",
      "Epoch 1/1\n",
      "631/631 [==============================] - 0s 154us/step - loss: 0.2194\n",
      "Epoch 1/1\n",
      "631/631 [==============================] - 0s 159us/step - loss: 0.2158\n",
      "player 1 wins: 13\n",
      "player 2 wins: 11\n",
      "simulations took 54.886844873428345 seconds.\n",
      "neural network update time: 0.2026970386505127 seconds.\n",
      "eval score on batch: 0.24719782338939641\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "667/667 [==============================] - 0s 47us/step\n",
      "667/667 [==============================] - 0s 37us/step\n",
      "updating with 667, 667 training batch.\n",
      "Epoch 1/1\n",
      "667/667 [==============================] - 0s 153us/step - loss: 0.1860\n",
      "Epoch 1/1\n",
      "667/667 [==============================] - 0s 158us/step - loss: 0.1857\n",
      "player 1 wins: 14\n",
      "player 2 wins: 10\n",
      "simulations took 56.71530032157898 seconds.\n",
      "neural network update time: 0.21324706077575684 seconds.\n",
      "eval score on batch: 0.18526250627757548\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "606/606 [==============================] - 0s 44us/step\n",
      "606/606 [==============================] - 0s 37us/step\n",
      "updating with 606, 606 training batch.\n",
      "Epoch 1/1\n",
      "606/606 [==============================] - 0s 150us/step - loss: 0.2003\n",
      "Epoch 1/1\n",
      "606/606 [==============================] - 0s 157us/step - loss: 0.2014\n",
      "player 1 wins: 11\n",
      "player 2 wins: 12\n",
      "simulations took 55.280335903167725 seconds.\n",
      "neural network update time: 0.1913602352142334 seconds.\n",
      "eval score on batch: 0.20747275439032628\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "629/629 [==============================] - 0s 39us/step\n",
      "629/629 [==============================] - 0s 41us/step\n",
      "updating with 629, 629 training batch.\n",
      "Epoch 1/1\n",
      "629/629 [==============================] - 0s 158us/step - loss: 0.2083\n",
      "Epoch 1/1\n",
      "629/629 [==============================] - 0s 158us/step - loss: 0.2084\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 54.520833015441895 seconds.\n",
      "neural network update time: 0.2042560577392578 seconds.\n",
      "eval score on batch: 0.20871394697357626\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "750/750 [==============================] - 0s 43us/step\n",
      "750/750 [==============================] - 0s 37us/step\n",
      "updating with 750, 750 training batch.\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 153us/step - loss: 0.1925\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 157us/step - loss: 0.1916\n",
      "player 1 wins: 11\n",
      "player 2 wins: 17\n",
      "simulations took 55.076361894607544 seconds.\n",
      "neural network update time: 0.2383110523223877 seconds.\n",
      "eval score on batch: 0.19470089595516524\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "746/746 [==============================] - 0s 39us/step\n",
      "746/746 [==============================] - 0s 41us/step\n",
      "updating with 746, 746 training batch.\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 159us/step - loss: 0.1965\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 161us/step - loss: 0.1958\n",
      "player 1 wins: 11\n",
      "player 2 wins: 18\n",
      "simulations took 54.03996920585632 seconds.\n",
      "neural network update time: 0.24396204948425293 seconds.\n",
      "eval score on batch: 0.19561694872917623\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "799/799 [==============================] - 0s 42us/step\n",
      "799/799 [==============================] - 0s 37us/step\n",
      "updating with 799, 799 training batch.\n",
      "Epoch 1/1\n",
      "799/799 [==============================] - 0s 155us/step - loss: 0.1842\n",
      "Epoch 1/1\n",
      "799/799 [==============================] - 0s 155us/step - loss: 0.1843\n",
      "player 1 wins: 23\n",
      "player 2 wins: 7\n",
      "simulations took 54.42783284187317 seconds.\n",
      "neural network update time: 0.25336480140686035 seconds.\n",
      "eval score on batch: 0.24044206660142978\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "74 outputs loaded.\n",
      "690/690 [==============================] - 0s 39us/step\n",
      "690/690 [==============================] - 0s 39us/step\n",
      "updating with 690, 690 training batch.\n",
      "Epoch 1/1\n",
      "690/690 [==============================] - 0s 156us/step - loss: 0.2105\n",
      "Epoch 1/1\n",
      "690/690 [==============================] - 0s 157us/step - loss: 0.2155\n",
      "player 1 wins: 13\n",
      "player 2 wins: 12\n",
      "simulations took 56.27257704734802 seconds.\n",
      "neural network update time: 0.2211921215057373 seconds.\n",
      "eval score on batch: 0.2611227869177642\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "749/749 [==============================] - 0s 44us/step\n",
      "749/749 [==============================] - 0s 40us/step\n",
      "updating with 749, 749 training batch.\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 156us/step - loss: 0.1835\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 159us/step - loss: 0.1804\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 57.33814215660095 seconds.\n",
      "neural network update time: 0.24074721336364746 seconds.\n",
      "eval score on batch: 0.192929826796214\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "699/699 [==============================] - 0s 46us/step\n",
      "699/699 [==============================] - 0s 37us/step\n",
      "updating with 699, 699 training batch.\n",
      "Epoch 1/1\n",
      "699/699 [==============================] - 0s 151us/step - loss: 0.1995\n",
      "Epoch 1/1\n",
      "699/699 [==============================] - 0s 157us/step - loss: 0.1968\n",
      "player 1 wins: 9\n",
      "player 2 wins: 17\n",
      "simulations took 54.93240189552307 seconds.\n",
      "neural network update time: 0.221085786819458 seconds.\n",
      "eval score on batch: 0.22473400976183758\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "761/761 [==============================] - 0s 40us/step\n",
      "761/761 [==============================] - 0s 39us/step\n",
      "updating with 761, 761 training batch.\n",
      "Epoch 1/1\n",
      "761/761 [==============================] - 0s 152us/step - loss: 0.1990\n",
      "Epoch 1/1\n",
      "761/761 [==============================] - 0s 155us/step - loss: 0.2001\n",
      "player 1 wins: 12\n",
      "player 2 wins: 17\n",
      "simulations took 54.81277513504028 seconds.\n",
      "neural network update time: 0.23938775062561035 seconds.\n",
      "eval score on batch: 0.2040259379124986\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "782/782 [==============================] - 0s 42us/step\n",
      "782/782 [==============================] - 0s 37us/step\n",
      "updating with 782, 782 training batch.\n",
      "Epoch 1/1\n",
      "782/782 [==============================] - 0s 157us/step - loss: 0.1942\n",
      "Epoch 1/1\n",
      "782/782 [==============================] - 0s 156us/step - loss: 0.1938\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 54.88608193397522 seconds.\n",
      "neural network update time: 0.25005292892456055 seconds.\n",
      "eval score on batch: 0.19345506475023602\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "743/743 [==============================] - 0s 38us/step\n",
      "743/743 [==============================] - 0s 40us/step\n",
      "updating with 743, 743 training batch.\n",
      "Epoch 1/1\n",
      "743/743 [==============================] - 0s 157us/step - loss: 0.1905\n",
      "Epoch 1/1\n",
      "743/743 [==============================] - 0s 160us/step - loss: 0.1913\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 55.599570989608765 seconds.\n",
      "neural network update time: 0.24148321151733398 seconds.\n",
      "eval score on batch: 0.19052054693052498\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "791/791 [==============================] - 0s 37us/step\n",
      "791/791 [==============================] - 0s 42us/step\n",
      "updating with 791, 791 training batch.\n",
      "Epoch 1/1\n",
      "791/791 [==============================] - 0s 153us/step - loss: 0.2047\n",
      "Epoch 1/1\n",
      "791/791 [==============================] - 0s 162us/step - loss: 0.2055\n",
      "player 1 wins: 16\n",
      "player 2 wins: 15\n",
      "simulations took 53.50528883934021 seconds.\n",
      "neural network update time: 0.25387001037597656 seconds.\n",
      "eval score on batch: 0.20481436625217217\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "625/625 [==============================] - 0s 41us/step\n",
      "625/625 [==============================] - 0s 39us/step\n",
      "updating with 625, 625 training batch.\n",
      "Epoch 1/1\n",
      "625/625 [==============================] - 0s 159us/step - loss: 0.1753\n",
      "Epoch 1/1\n",
      "625/625 [==============================] - 0s 160us/step - loss: 0.1723\n",
      "player 1 wins: 7\n",
      "player 2 wins: 15\n",
      "simulations took 56.58536171913147 seconds.\n",
      "neural network update time: 0.20434904098510742 seconds.\n",
      "eval score on batch: 0.18716477422714234\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "737/737 [==============================] - 0s 42us/step\n",
      "737/737 [==============================] - 0s 39us/step\n",
      "updating with 737, 737 training batch.\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 158us/step - loss: 0.1903\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 161us/step - loss: 0.1934\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 56.0541467666626 seconds.\n",
      "neural network update time: 0.24167299270629883 seconds.\n",
      "eval score on batch: 0.19968750195393245\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "585/585 [==============================] - 0s 45us/step\n",
      "585/585 [==============================] - 0s 39us/step\n",
      "updating with 585, 585 training batch.\n",
      "Epoch 1/1\n",
      "585/585 [==============================] - 0s 159us/step - loss: 0.1886\n",
      "Epoch 1/1\n",
      "585/585 [==============================] - 0s 162us/step - loss: 0.1860\n",
      "player 1 wins: 9\n",
      "player 2 wins: 12\n",
      "simulations took 57.150413036346436 seconds.\n",
      "neural network update time: 0.19256591796875 seconds.\n",
      "eval score on batch: 0.18566470663978632\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "822/822 [==============================] - 0s 41us/step\n",
      "822/822 [==============================] - 0s 38us/step\n",
      "updating with 822, 822 training batch.\n",
      "Epoch 1/1\n",
      "822/822 [==============================] - 0s 163us/step - loss: 0.1959\n",
      "Epoch 1/1\n",
      "822/822 [==============================] - 0s 154us/step - loss: 0.1958\n",
      "player 1 wins: 12\n",
      "player 2 wins: 20\n",
      "simulations took 53.1969690322876 seconds.\n",
      "neural network update time: 0.2662808895111084 seconds.\n",
      "eval score on batch: 0.19751240084641172\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "723/723 [==============================] - 0s 40us/step\n",
      "723/723 [==============================] - 0s 40us/step\n",
      "updating with 723, 723 training batch.\n",
      "Epoch 1/1\n",
      "723/723 [==============================] - 0s 155us/step - loss: 0.1949\n",
      "Epoch 1/1\n",
      "723/723 [==============================] - 0s 160us/step - loss: 0.1942\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 56.339377880096436 seconds.\n",
      "neural network update time: 0.2333660125732422 seconds.\n",
      "eval score on batch: 0.21618287250873625\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "734/734 [==============================] - 0s 40us/step\n",
      "734/734 [==============================] - 0s 38us/step\n",
      "updating with 734, 734 training batch.\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 154us/step - loss: 0.1888\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 170us/step - loss: 0.1891\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 55.730642318725586 seconds.\n",
      "neural network update time: 0.24430108070373535 seconds.\n",
      "eval score on batch: 0.19012120674676403\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "680/680 [==============================] - 0s 38us/step\n",
      "680/680 [==============================] - 0s 45us/step\n",
      "updating with 680, 680 training batch.\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 156us/step - loss: 0.1930\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 157us/step - loss: 0.1940\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 56.41755294799805 seconds.\n",
      "neural network update time: 0.21798491477966309 seconds.\n",
      "eval score on batch: 0.19911633290569572\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "810/810 [==============================] - 0s 41us/step\n",
      "810/810 [==============================] - 0s 39us/step\n",
      "updating with 810, 810 training batch.\n",
      "Epoch 1/1\n",
      "810/810 [==============================] - 0s 159us/step - loss: 0.1892\n",
      "Epoch 1/1\n",
      "810/810 [==============================] - 0s 155us/step - loss: 0.1875\n",
      "player 1 wins: 18\n",
      "player 2 wins: 11\n",
      "simulations took 55.95368313789368 seconds.\n",
      "neural network update time: 0.2596769332885742 seconds.\n",
      "eval score on batch: 0.20058936927881504\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "776/776 [==============================] - 0s 37us/step\n",
      "776/776 [==============================] - 0s 39us/step\n",
      "updating with 776, 776 training batch.\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 152us/step - loss: 0.2032\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 160us/step - loss: 0.1963\n",
      "player 1 wins: 11\n",
      "player 2 wins: 18\n",
      "simulations took 54.91724109649658 seconds.\n",
      "neural network update time: 0.24758195877075195 seconds.\n",
      "eval score on batch: 0.22284225451270329\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "665/665 [==============================] - 0s 46us/step\n",
      "665/665 [==============================] - 0s 43us/step\n",
      "updating with 665, 665 training batch.\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 155us/step - loss: 0.2053\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 154us/step - loss: 0.2076\n",
      "player 1 wins: 15\n",
      "player 2 wins: 10\n",
      "simulations took 55.14809226989746 seconds.\n",
      "neural network update time: 0.21095800399780273 seconds.\n",
      "eval score on batch: 0.23417192442076548\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "710/710 [==============================] - 0s 41us/step\n",
      "710/710 [==============================] - 0s 37us/step\n",
      "updating with 710, 710 training batch.\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 158us/step - loss: 0.2041\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 158us/step - loss: 0.2055\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 54.637962102890015 seconds.\n",
      "neural network update time: 0.23012924194335938 seconds.\n",
      "eval score on batch: 0.21185272991867132\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "779/779 [==============================] - 0s 43us/step\n",
      "779/779 [==============================] - 0s 40us/step\n",
      "updating with 779, 779 training batch.\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 161us/step - loss: 0.1913\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 157us/step - loss: 0.1886\n",
      "player 1 wins: 18\n",
      "player 2 wins: 11\n",
      "simulations took 55.30154800415039 seconds.\n",
      "neural network update time: 0.25453710556030273 seconds.\n",
      "eval score on batch: 0.19534751718842325\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "616/616 [==============================] - 0s 45us/step\n",
      "616/616 [==============================] - 0s 36us/step\n",
      "updating with 616, 616 training batch.\n",
      "Epoch 1/1\n",
      "616/616 [==============================] - 0s 156us/step - loss: 0.1970\n",
      "Epoch 1/1\n",
      "616/616 [==============================] - 0s 161us/step - loss: 0.1962\n",
      "player 1 wins: 15\n",
      "player 2 wins: 9\n",
      "simulations took 55.149495124816895 seconds.\n",
      "neural network update time: 0.20023488998413086 seconds.\n",
      "eval score on batch: 0.1963754261468912\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "75 outputs loaded.\n",
      "666/666 [==============================] - 0s 45us/step\n",
      "666/666 [==============================] - 0s 38us/step\n",
      "updating with 666, 666 training batch.\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 155us/step - loss: 0.2008\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 155us/step - loss: 0.1996\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 55.769871950149536 seconds.\n",
      "neural network update time: 0.212263822555542 seconds.\n",
      "eval score on batch: 0.21097001954689398\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "755/755 [==============================] - 0s 38us/step\n",
      "755/755 [==============================] - 0s 42us/step\n",
      "updating with 755, 755 training batch.\n",
      "Epoch 1/1\n",
      "755/755 [==============================] - 0s 152us/step - loss: 0.1891\n",
      "Epoch 1/1\n",
      "755/755 [==============================] - 0s 159us/step - loss: 0.1891\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 56.21613526344299 seconds.\n",
      "neural network update time: 0.2399458885192871 seconds.\n",
      "eval score on batch: 0.1902327273411072\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "843/843 [==============================] - 0s 41us/step\n",
      "843/843 [==============================] - 0s 38us/step\n",
      "updating with 843, 843 training batch.\n",
      "Epoch 1/1\n",
      "843/843 [==============================] - 0s 154us/step - loss: 0.1806\n",
      "Epoch 1/1\n",
      "843/843 [==============================] - 0s 158us/step - loss: 0.1803\n",
      "player 1 wins: 19\n",
      "player 2 wins: 11\n",
      "simulations took 56.279454946517944 seconds.\n",
      "neural network update time: 0.2679760456085205 seconds.\n",
      "eval score on batch: 0.18576538610611273\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "797/797 [==============================] - 0s 42us/step\n",
      "797/797 [==============================] - 0s 41us/step\n",
      "updating with 797, 797 training batch.\n",
      "Epoch 1/1\n",
      "797/797 [==============================] - 0s 152us/step - loss: 0.2003\n",
      "Epoch 1/1\n",
      "797/797 [==============================] - 0s 155us/step - loss: 0.2023\n",
      "player 1 wins: 12\n",
      "player 2 wins: 18\n",
      "simulations took 54.70553207397461 seconds.\n",
      "neural network update time: 0.25075697898864746 seconds.\n",
      "eval score on batch: 0.23334647891774232\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "705/705 [==============================] - 0s 42us/step\n",
      "705/705 [==============================] - 0s 39us/step\n",
      "updating with 705, 705 training batch.\n",
      "Epoch 1/1\n",
      "705/705 [==============================] - 0s 159us/step - loss: 0.1873\n",
      "Epoch 1/1\n",
      "705/705 [==============================] - 0s 160us/step - loss: 0.1856\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 56.38934898376465 seconds.\n",
      "neural network update time: 0.2307291030883789 seconds.\n",
      "eval score on batch: 0.19041609614823304\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "687/687 [==============================] - 0s 43us/step\n",
      "687/687 [==============================] - 0s 36us/step\n",
      "updating with 687, 687 training batch.\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 155us/step - loss: 0.1916\n",
      "Epoch 1/1\n",
      "687/687 [==============================] - 0s 157us/step - loss: 0.1939\n",
      "player 1 wins: 9\n",
      "player 2 wins: 18\n",
      "simulations took 53.777578830718994 seconds.\n",
      "neural network update time: 0.21996188163757324 seconds.\n",
      "eval score on batch: 0.20138827922823127\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "686/686 [==============================] - 0s 42us/step\n",
      "686/686 [==============================] - 0s 40us/step\n",
      "updating with 686, 686 training batch.\n",
      "Epoch 1/1\n",
      "686/686 [==============================] - 0s 159us/step - loss: 0.1903\n",
      "Epoch 1/1\n",
      "686/686 [==============================] - 0s 159us/step - loss: 0.1900\n",
      "player 1 wins: 9\n",
      "player 2 wins: 18\n",
      "simulations took 53.96833300590515 seconds.\n",
      "neural network update time: 0.22368216514587402 seconds.\n",
      "eval score on batch: 0.1910737750022713\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "684/684 [==============================] - 0s 40us/step\n",
      "684/684 [==============================] - 0s 39us/step\n",
      "updating with 684, 684 training batch.\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 156us/step - loss: 0.2040\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 157us/step - loss: 0.2019\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 54.990036964416504 seconds.\n",
      "neural network update time: 0.21925997734069824 seconds.\n",
      "eval score on batch: 0.225281232477803\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "714/714 [==============================] - 0s 43us/step\n",
      "714/714 [==============================] - 0s 37us/step\n",
      "updating with 714, 714 training batch.\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 156us/step - loss: 0.1991\n",
      "Epoch 1/1\n",
      "714/714 [==============================] - 0s 158us/step - loss: 0.1945\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 56.13034796714783 seconds.\n",
      "neural network update time: 0.22980403900146484 seconds.\n",
      "eval score on batch: 0.20246593476206595\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "666/666 [==============================] - 0s 40us/step\n",
      "666/666 [==============================] - 0s 39us/step\n",
      "updating with 666, 666 training batch.\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 155us/step - loss: 0.1937\n",
      "Epoch 1/1\n",
      "666/666 [==============================] - 0s 157us/step - loss: 0.1923\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 55.748143911361694 seconds.\n",
      "neural network update time: 0.2130906581878662 seconds.\n",
      "eval score on batch: 0.19225782091105664\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "624/624 [==============================] - 0s 46us/step\n",
      "624/624 [==============================] - 0s 38us/step\n",
      "updating with 624, 624 training batch.\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 153us/step - loss: 0.2089\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 158us/step - loss: 0.2100\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 53.41313982009888 seconds.\n",
      "neural network update time: 0.2003939151763916 seconds.\n",
      "eval score on batch: 0.21420916685691246\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "684/684 [==============================] - 0s 39us/step\n",
      "684/684 [==============================] - 0s 42us/step\n",
      "updating with 684, 684 training batch.\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 156us/step - loss: 0.1997\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 159us/step - loss: 0.1987\n",
      "player 1 wins: 14\n",
      "player 2 wins: 12\n",
      "simulations took 54.683581829071045 seconds.\n",
      "neural network update time: 0.22111010551452637 seconds.\n",
      "eval score on batch: 0.19831542915811665\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "699/699 [==============================] - 0s 43us/step\n",
      "699/699 [==============================] - 0s 38us/step\n",
      "updating with 699, 699 training batch.\n",
      "Epoch 1/1\n",
      "699/699 [==============================] - 0s 161us/step - loss: 0.1988\n",
      "Epoch 1/1\n",
      "699/699 [==============================] - 0s 155us/step - loss: 0.2030\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 54.3290159702301 seconds.\n",
      "neural network update time: 0.22633981704711914 seconds.\n",
      "eval score on batch: 0.20568965669609446\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "624/624 [==============================] - 0s 44us/step\n",
      "624/624 [==============================] - 0s 37us/step\n",
      "updating with 624, 624 training batch.\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 151us/step - loss: 0.1915\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 157us/step - loss: 0.1917\n",
      "player 1 wins: 10\n",
      "player 2 wins: 13\n",
      "simulations took 56.82036113739014 seconds.\n",
      "neural network update time: 0.19701409339904785 seconds.\n",
      "eval score on batch: 0.19205226911566195\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "750/750 [==============================] - 0s 39us/step\n",
      "750/750 [==============================] - 0s 41us/step\n",
      "updating with 750, 750 training batch.\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 154us/step - loss: 0.2020\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 160us/step - loss: 0.2027\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 53.99097013473511 seconds.\n",
      "neural network update time: 0.24100923538208008 seconds.\n",
      "eval score on batch: 0.20281790202111005\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "767/767 [==============================] - 0s 42us/step\n",
      "767/767 [==============================] - 0s 41us/step\n",
      "updating with 767, 767 training batch.\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 149us/step - loss: 0.1984\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 156us/step - loss: 0.1975\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 54.30477428436279 seconds.\n",
      "neural network update time: 0.2393507957458496 seconds.\n",
      "eval score on batch: 0.20052028833032431\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "710/710 [==============================] - 0s 39us/step\n",
      "710/710 [==============================] - 0s 38us/step\n",
      "updating with 710, 710 training batch.\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 154us/step - loss: 0.1895\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 162us/step - loss: 0.1898\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 55.97956681251526 seconds.\n",
      "neural network update time: 0.22963714599609375 seconds.\n",
      "eval score on batch: 0.1894295905519959\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "799/799 [==============================] - 0s 37us/step\n",
      "799/799 [==============================] - 0s 38us/step\n",
      "updating with 799, 799 training batch.\n",
      "Epoch 1/1\n",
      "799/799 [==============================] - 0s 149us/step - loss: 0.2022\n",
      "Epoch 1/1\n",
      "799/799 [==============================] - 0s 158us/step - loss: 0.2037\n",
      "player 1 wins: 17\n",
      "player 2 wins: 14\n",
      "simulations took 53.01530122756958 seconds.\n",
      "neural network update time: 0.25052595138549805 seconds.\n",
      "eval score on batch: 0.20846519815981462\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "707/707 [==============================] - 0s 39us/step\n",
      "707/707 [==============================] - 0s 41us/step\n",
      "updating with 707, 707 training batch.\n",
      "Epoch 1/1\n",
      "707/707 [==============================] - 0s 154us/step - loss: 0.1906\n",
      "Epoch 1/1\n",
      "707/707 [==============================] - 0s 160us/step - loss: 0.1900\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 55.81237983703613 seconds.\n",
      "neural network update time: 0.2272028923034668 seconds.\n",
      "eval score on batch: 0.1896975907511454\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "728/728 [==============================] - 0s 41us/step\n",
      "728/728 [==============================] - 0s 37us/step\n",
      "updating with 728, 728 training batch.\n",
      "Epoch 1/1\n",
      "728/728 [==============================] - 0s 154us/step - loss: 0.1888\n",
      "Epoch 1/1\n",
      "728/728 [==============================] - 0s 156us/step - loss: 0.1864\n",
      "player 1 wins: 19\n",
      "player 2 wins: 9\n",
      "simulations took 54.108911752700806 seconds.\n",
      "neural network update time: 0.23086190223693848 seconds.\n",
      "eval score on batch: 0.18922336727053254\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "76 outputs loaded.\n",
      "721/721 [==============================] - 0s 39us/step\n",
      "721/721 [==============================] - 0s 39us/step\n",
      "updating with 721, 721 training batch.\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 155us/step - loss: 0.1987\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 158us/step - loss: 0.2012\n",
      "player 1 wins: 15\n",
      "player 2 wins: 12\n",
      "simulations took 54.853675842285156 seconds.\n",
      "neural network update time: 0.23133373260498047 seconds.\n",
      "eval score on batch: 0.2043950846509158\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "747/747 [==============================] - 0s 45us/step\n",
      "747/747 [==============================] - 0s 46us/step\n",
      "updating with 747, 747 training batch.\n",
      "Epoch 1/1\n",
      "747/747 [==============================] - 0s 160us/step - loss: 0.1908\n",
      "Epoch 1/1\n",
      "747/747 [==============================] - 0s 172us/step - loss: 0.1903\n",
      "player 1 wins: 18\n",
      "player 2 wins: 10\n",
      "simulations took 54.90723705291748 seconds.\n",
      "neural network update time: 0.2534620761871338 seconds.\n",
      "eval score on batch: 0.19518975238284592\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "719/719 [==============================] - 0s 38us/step\n",
      "719/719 [==============================] - 0s 42us/step\n",
      "updating with 719, 719 training batch.\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 152us/step - loss: 0.2042\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 154us/step - loss: 0.1989\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 55.417742013931274 seconds.\n",
      "neural network update time: 0.2253861427307129 seconds.\n",
      "eval score on batch: 0.22648147955764816\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "766/766 [==============================] - 0s 39us/step\n",
      "766/766 [==============================] - 0s 37us/step\n",
      "updating with 766, 766 training batch.\n",
      "Epoch 1/1\n",
      "766/766 [==============================] - 0s 151us/step - loss: 0.1913\n",
      "Epoch 1/1\n",
      "766/766 [==============================] - 0s 155us/step - loss: 0.1963\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 55.26175308227539 seconds.\n",
      "neural network update time: 0.23946094512939453 seconds.\n",
      "eval score on batch: 0.20775851506165985\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "678/678 [==============================] - 0s 43us/step\n",
      "678/678 [==============================] - 0s 38us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 158us/step - loss: 0.1862\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 160us/step - loss: 0.1822\n",
      "player 1 wins: 7\n",
      "player 2 wins: 18\n",
      "simulations took 56.15411615371704 seconds.\n",
      "neural network update time: 0.2208559513092041 seconds.\n",
      "eval score on batch: 0.2038636094613061\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "686/686 [==============================] - 0s 39us/step\n",
      "686/686 [==============================] - 0s 39us/step\n",
      "updating with 686, 686 training batch.\n",
      "Epoch 1/1\n",
      "686/686 [==============================] - 0s 151us/step - loss: 0.2061\n",
      "Epoch 1/1\n",
      "686/686 [==============================] - 0s 159us/step - loss: 0.2057\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 55.53738808631897 seconds.\n",
      "neural network update time: 0.21858882904052734 seconds.\n",
      "eval score on batch: 0.21704210881849767\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "680/680 [==============================] - 0s 38us/step\n",
      "680/680 [==============================] - 0s 43us/step\n",
      "updating with 680, 680 training batch.\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 156us/step - loss: 0.1846\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 161us/step - loss: 0.1848\n",
      "player 1 wins: 17\n",
      "player 2 wins: 8\n",
      "simulations took 55.90383791923523 seconds.\n",
      "neural network update time: 0.2205660343170166 seconds.\n",
      "eval score on batch: 0.1977386030532858\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "661/661 [==============================] - 0s 45us/step\n",
      "661/661 [==============================] - 0s 43us/step\n",
      "updating with 661, 661 training batch.\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 183us/step - loss: 0.1802\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 214us/step - loss: 0.1786\n",
      "player 1 wins: 16\n",
      "player 2 wins: 8\n",
      "simulations took 57.12369704246521 seconds.\n",
      "neural network update time: 0.2678089141845703 seconds.\n",
      "eval score on batch: 0.18048983246493808\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "688/688 [==============================] - 0s 43us/step\n",
      "688/688 [==============================] - 0s 39us/step\n",
      "updating with 688, 688 training batch.\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 158us/step - loss: 0.1959\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 159us/step - loss: 0.2005\n",
      "player 1 wins: 9\n",
      "player 2 wins: 17\n",
      "simulations took 56.54246401786804 seconds.\n",
      "neural network update time: 0.2230699062347412 seconds.\n",
      "eval score on batch: 0.22627362842823184\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "706/706 [==============================] - 0s 45us/step\n",
      "706/706 [==============================] - 0s 41us/step\n",
      "updating with 706, 706 training batch.\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 162us/step - loss: 0.1946\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 160us/step - loss: 0.1928\n",
      "player 1 wins: 10\n",
      "player 2 wins: 17\n",
      "simulations took 55.3482129573822 seconds.\n",
      "neural network update time: 0.2333979606628418 seconds.\n",
      "eval score on batch: 0.1968348364137303\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "665/665 [==============================] - 0s 44us/step\n",
      "665/665 [==============================] - 0s 37us/step\n",
      "updating with 665, 665 training batch.\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 162us/step - loss: 0.2021\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 170us/step - loss: 0.2010\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 55.86672401428223 seconds.\n",
      "neural network update time: 0.22655630111694336 seconds.\n",
      "eval score on batch: 0.2008225713905535\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "698/698 [==============================] - 0s 43us/step\n",
      "698/698 [==============================] - 0s 40us/step\n",
      "updating with 698, 698 training batch.\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 180us/step - loss: 0.1857\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 174us/step - loss: 0.1846\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 60.3215811252594 seconds.\n",
      "neural network update time: 0.25272488594055176 seconds.\n",
      "eval score on batch: 0.18318296512081836\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "608/608 [==============================] - 0s 45us/step\n",
      "608/608 [==============================] - 0s 42us/step\n",
      "updating with 608, 608 training batch.\n",
      "Epoch 1/1\n",
      "608/608 [==============================] - 0s 154us/step - loss: 0.1984\n",
      "Epoch 1/1\n",
      "608/608 [==============================] - 0s 156us/step - loss: 0.2012\n",
      "player 1 wins: 14\n",
      "player 2 wins: 9\n",
      "simulations took 56.17343616485596 seconds.\n",
      "neural network update time: 0.19316482543945312 seconds.\n",
      "eval score on batch: 0.216929040457073\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "709/709 [==============================] - 0s 44us/step\n",
      "709/709 [==============================] - 0s 37us/step\n",
      "updating with 709, 709 training batch.\n",
      "Epoch 1/1\n",
      "709/709 [==============================] - 0s 154us/step - loss: 0.1959\n",
      "Epoch 1/1\n",
      "709/709 [==============================] - 0s 161us/step - loss: 0.1978\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 56.24150323867798 seconds.\n",
      "neural network update time: 0.22806000709533691 seconds.\n",
      "eval score on batch: 0.20265246950857058\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "757/757 [==============================] - 0s 44us/step\n",
      "757/757 [==============================] - 0s 43us/step\n",
      "updating with 757, 757 training batch.\n",
      "Epoch 1/1\n",
      "757/757 [==============================] - 0s 154us/step - loss: 0.1920\n",
      "Epoch 1/1\n",
      "757/757 [==============================] - 0s 159us/step - loss: 0.1931\n",
      "player 1 wins: 12\n",
      "player 2 wins: 16\n",
      "simulations took 55.4936089515686 seconds.\n",
      "neural network update time: 0.24278807640075684 seconds.\n",
      "eval score on batch: 0.1912774326405248\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "691/691 [==============================] - 0s 43us/step\n",
      "691/691 [==============================] - 0s 36us/step\n",
      "updating with 691, 691 training batch.\n",
      "Epoch 1/1\n",
      "691/691 [==============================] - 0s 153us/step - loss: 0.1933\n",
      "Epoch 1/1\n",
      "691/691 [==============================] - 0s 159us/step - loss: 0.1937\n",
      "player 1 wins: 17\n",
      "player 2 wins: 9\n",
      "simulations took 55.46248006820679 seconds.\n",
      "neural network update time: 0.2205817699432373 seconds.\n",
      "eval score on batch: 0.2095750415395789\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "680/680 [==============================] - 0s 43us/step\n",
      "680/680 [==============================] - 0s 41us/step\n",
      "updating with 680, 680 training batch.\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 155us/step - loss: 0.1887\n",
      "Epoch 1/1\n",
      "680/680 [==============================] - 0s 157us/step - loss: 0.1903\n",
      "player 1 wins: 15\n",
      "player 2 wins: 10\n",
      "simulations took 56.456989765167236 seconds.\n",
      "neural network update time: 0.21793103218078613 seconds.\n",
      "eval score on batch: 0.19646576752557474\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "649/649 [==============================] - 0s 43us/step\n",
      "649/649 [==============================] - 0s 39us/step\n",
      "updating with 649, 649 training batch.\n",
      "Epoch 1/1\n",
      "649/649 [==============================] - 0s 159us/step - loss: 0.1977\n",
      "Epoch 1/1\n",
      "649/649 [==============================] - 0s 167us/step - loss: 0.1962\n",
      "player 1 wins: 9\n",
      "player 2 wins: 16\n",
      "simulations took 55.43477916717529 seconds.\n",
      "neural network update time: 0.21640801429748535 seconds.\n",
      "eval score on batch: 0.2083718044272924\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "748/748 [==============================] - 0s 44us/step\n",
      "748/748 [==============================] - 0s 39us/step\n",
      "updating with 748, 748 training batch.\n",
      "Epoch 1/1\n",
      "748/748 [==============================] - 0s 155us/step - loss: 0.1921\n",
      "Epoch 1/1\n",
      "748/748 [==============================] - 0s 161us/step - loss: 0.1924\n",
      "player 1 wins: 11\n",
      "player 2 wins: 17\n",
      "simulations took 56.0307822227478 seconds.\n",
      "neural network update time: 0.24167394638061523 seconds.\n",
      "eval score on batch: 0.19406952549464362\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "681/681 [==============================] - 0s 42us/step\n",
      "681/681 [==============================] - 0s 39us/step\n",
      "updating with 681, 681 training batch.\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 159us/step - loss: 0.1873\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 159us/step - loss: 0.1876\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 55.37315893173218 seconds.\n",
      "neural network update time: 0.2215421199798584 seconds.\n",
      "eval score on batch: 0.1870165897421156\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "77 outputs loaded.\n",
      "736/736 [==============================] - 0s 40us/step\n",
      "736/736 [==============================] - 0s 36us/step\n",
      "updating with 736, 736 training batch.\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 151us/step - loss: 0.1880\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 155us/step - loss: 0.1865\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 56.151771068573 seconds.\n",
      "neural network update time: 0.2306671142578125 seconds.\n",
      "eval score on batch: 0.1905583263091419\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "689/689 [==============================] - 0s 42us/step\n",
      "689/689 [==============================] - 0s 41us/step\n",
      "updating with 689, 689 training batch.\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 159us/step - loss: 0.1908\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 156us/step - loss: 0.1905\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 56.13698124885559 seconds.\n",
      "neural network update time: 0.2228851318359375 seconds.\n",
      "eval score on batch: 0.19106022923298083\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "688/688 [==============================] - 0s 43us/step\n",
      "688/688 [==============================] - 0s 39us/step\n",
      "updating with 688, 688 training batch.\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 161us/step - loss: 0.1834\n",
      "Epoch 1/1\n",
      "688/688 [==============================] - 0s 156us/step - loss: 0.1869\n",
      "player 1 wins: 16\n",
      "player 2 wins: 9\n",
      "simulations took 56.0943067073822 seconds.\n",
      "neural network update time: 0.22394704818725586 seconds.\n",
      "eval score on batch: 0.1952112823248256\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "721/721 [==============================] - 0s 44us/step\n",
      "721/721 [==============================] - 0s 42us/step\n",
      "updating with 721, 721 training batch.\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 158us/step - loss: 0.1952\n",
      "Epoch 1/1\n",
      "721/721 [==============================] - 0s 157us/step - loss: 0.1968\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 56.671249866485596 seconds.\n",
      "neural network update time: 0.2338390350341797 seconds.\n",
      "eval score on batch: 0.2104085355211726\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "725/725 [==============================] - 0s 40us/step\n",
      "725/725 [==============================] - 0s 39us/step\n",
      "updating with 725, 725 training batch.\n",
      "Epoch 1/1\n",
      "725/725 [==============================] - 0s 156us/step - loss: 0.1880\n",
      "Epoch 1/1\n",
      "725/725 [==============================] - 0s 157us/step - loss: 0.1878\n",
      "player 1 wins: 10\n",
      "player 2 wins: 17\n",
      "simulations took 55.39606499671936 seconds.\n",
      "neural network update time: 0.2331838607788086 seconds.\n",
      "eval score on batch: 0.18869768533213385\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "567/567 [==============================] - 0s 50us/step\n",
      "567/567 [==============================] - 0s 38us/step\n",
      "updating with 567, 567 training batch.\n",
      "Epoch 1/1\n",
      "567/567 [==============================] - 0s 158us/step - loss: 0.1967\n",
      "Epoch 1/1\n",
      "567/567 [==============================] - 0s 159us/step - loss: 0.1947\n",
      "player 1 wins: 14\n",
      "player 2 wins: 7\n",
      "simulations took 56.27881908416748 seconds.\n",
      "neural network update time: 0.18492484092712402 seconds.\n",
      "eval score on batch: 0.2283382279111805\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "648/648 [==============================] - 0s 49us/step\n",
      "648/648 [==============================] - 0s 51us/step\n",
      "updating with 648, 648 training batch.\n",
      "Epoch 1/1\n",
      "648/648 [==============================] - 0s 164us/step - loss: 0.1893\n",
      "Epoch 1/1\n",
      "648/648 [==============================] - 0s 171us/step - loss: 0.1902\n",
      "player 1 wins: 13\n",
      "player 2 wins: 10\n",
      "simulations took 56.42570781707764 seconds.\n",
      "neural network update time: 0.2223219871520996 seconds.\n",
      "eval score on batch: 0.19703421776217442\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "605/605 [==============================] - 0s 38us/step\n",
      "605/605 [==============================] - 0s 42us/step\n",
      "updating with 605, 605 training batch.\n",
      "Epoch 1/1\n",
      "605/605 [==============================] - 0s 164us/step - loss: 0.1870\n",
      "Epoch 1/1\n",
      "605/605 [==============================] - 0s 167us/step - loss: 0.1865\n",
      "player 1 wins: 14\n",
      "player 2 wins: 8\n",
      "simulations took 60.439491748809814 seconds.\n",
      "neural network update time: 0.20558476448059082 seconds.\n",
      "eval score on batch: 0.1860117343339053\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "575/575 [==============================] - 0s 40us/step\n",
      "575/575 [==============================] - 0s 39us/step\n",
      "updating with 575, 575 training batch.\n",
      "Epoch 1/1\n",
      "575/575 [==============================] - 0s 149us/step - loss: 0.2174\n",
      "Epoch 1/1\n",
      "575/575 [==============================] - 0s 152us/step - loss: 0.2157\n",
      "player 1 wins: 9\n",
      "player 2 wins: 14\n",
      "simulations took 56.62691831588745 seconds.\n",
      "neural network update time: 0.17840886116027832 seconds.\n",
      "eval score on batch: 0.2296728544909021\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "674/674 [==============================] - 0s 43us/step\n",
      "674/674 [==============================] - 0s 38us/step\n",
      "updating with 674, 674 training batch.\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 159us/step - loss: 0.1917\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 157us/step - loss: 0.1919\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 55.86558508872986 seconds.\n",
      "neural network update time: 0.21851396560668945 seconds.\n",
      "eval score on batch: 0.1897530321003137\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "762/762 [==============================] - 0s 42us/step\n",
      "762/762 [==============================] - 0s 42us/step\n",
      "updating with 762, 762 training batch.\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 153us/step - loss: 0.1796\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 153us/step - loss: 0.1811\n",
      "player 1 wins: 20\n",
      "player 2 wins: 8\n",
      "simulations took 55.235127210617065 seconds.\n",
      "neural network update time: 0.23973298072814941 seconds.\n",
      "eval score on batch: 0.2154735548524406\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "823/823 [==============================] - 0s 40us/step\n",
      "823/823 [==============================] - 0s 39us/step\n",
      "updating with 823, 823 training batch.\n",
      "Epoch 1/1\n",
      "823/823 [==============================] - 0s 154us/step - loss: 0.2000\n",
      "Epoch 1/1\n",
      "823/823 [==============================] - 0s 156us/step - loss: 0.1989\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 54.959867000579834 seconds.\n",
      "neural network update time: 0.2603938579559326 seconds.\n",
      "eval score on batch: 0.2267285452958735\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "783/783 [==============================] - 0s 41us/step\n",
      "783/783 [==============================] - 0s 38us/step\n",
      "updating with 783, 783 training batch.\n",
      "Epoch 1/1\n",
      "783/783 [==============================] - 0s 160us/step - loss: 0.1939\n",
      "Epoch 1/1\n",
      "783/783 [==============================] - 0s 179us/step - loss: 0.1942\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 54.745338678359985 seconds.\n",
      "neural network update time: 0.2718842029571533 seconds.\n",
      "eval score on batch: 0.19522835167736233\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "681/681 [==============================] - 0s 46us/step\n",
      "681/681 [==============================] - 0s 39us/step\n",
      "updating with 681, 681 training batch.\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 167us/step - loss: 0.1939\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 173us/step - loss: 0.1920\n",
      "player 1 wins: 13\n",
      "player 2 wins: 12\n",
      "simulations took 56.91409087181091 seconds.\n",
      "neural network update time: 0.23651719093322754 seconds.\n",
      "eval score on batch: 0.19181487586338308\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "737/737 [==============================] - 0s 43us/step\n",
      "737/737 [==============================] - 0s 43us/step\n",
      "updating with 737, 737 training batch.\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 156us/step - loss: 0.1998\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 161us/step - loss: 0.1993\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 56.98064303398132 seconds.\n",
      "neural network update time: 0.2389390468597412 seconds.\n",
      "eval score on batch: 0.20012668036038164\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "706/706 [==============================] - 0s 40us/step\n",
      "706/706 [==============================] - 0s 44us/step\n",
      "updating with 706, 706 training batch.\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 165us/step - loss: 0.1987\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 175us/step - loss: 0.2026\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 56.83268594741821 seconds.\n",
      "neural network update time: 0.24465084075927734 seconds.\n",
      "eval score on batch: 0.1987440539471193\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "698/698 [==============================] - 0s 48us/step\n",
      "698/698 [==============================] - 0s 42us/step\n",
      "updating with 698, 698 training batch.\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 168us/step - loss: 0.1992\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 169us/step - loss: 0.1968\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 57.47992420196533 seconds.\n",
      "neural network update time: 0.24053502082824707 seconds.\n",
      "eval score on batch: 0.2070247070452887\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "708/708 [==============================] - 0s 45us/step\n",
      "708/708 [==============================] - 0s 43us/step\n",
      "updating with 708, 708 training batch.\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 166us/step - loss: 0.2058\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 180us/step - loss: 0.2087\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 55.089531898498535 seconds.\n",
      "neural network update time: 0.25018811225891113 seconds.\n",
      "eval score on batch: 0.22231831589166476\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "708/708 [==============================] - 0s 44us/step\n",
      "708/708 [==============================] - 0s 38us/step\n",
      "updating with 708, 708 training batch.\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 159us/step - loss: 0.1981\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 164us/step - loss: 0.1982\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 54.39104199409485 seconds.\n",
      "neural network update time: 0.2335958480834961 seconds.\n",
      "eval score on batch: 0.1984369987358818\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "620/620 [==============================] - 0s 40us/step\n",
      "620/620 [==============================] - 0s 40us/step\n",
      "updating with 620, 620 training batch.\n",
      "Epoch 1/1\n",
      "620/620 [==============================] - 0s 158us/step - loss: 0.1854\n",
      "Epoch 1/1\n",
      "620/620 [==============================] - 0s 162us/step - loss: 0.1860\n",
      "player 1 wins: 13\n",
      "player 2 wins: 9\n",
      "simulations took 57.08486294746399 seconds.\n",
      "neural network update time: 0.20350289344787598 seconds.\n",
      "eval score on batch: 0.1891271924329621\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "78 outputs loaded.\n",
      "613/613 [==============================] - 0s 44us/step\n",
      "613/613 [==============================] - 0s 44us/step\n",
      "updating with 613, 613 training batch.\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 164us/step - loss: 0.1978\n",
      "Epoch 1/1\n",
      "613/613 [==============================] - 0s 162us/step - loss: 0.1965\n",
      "player 1 wins: 12\n",
      "player 2 wins: 11\n",
      "simulations took 56.10847997665405 seconds.\n",
      "neural network update time: 0.20580720901489258 seconds.\n",
      "eval score on batch: 0.1997880091431868\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "794/794 [==============================] - 0s 42us/step\n",
      "794/794 [==============================] - 0s 38us/step\n",
      "updating with 794, 794 training batch.\n",
      "Epoch 1/1\n",
      "794/794 [==============================] - 0s 151us/step - loss: 0.1972\n",
      "Epoch 1/1\n",
      "794/794 [==============================] - 0s 156us/step - loss: 0.1972\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 54.43515062332153 seconds.\n",
      "neural network update time: 0.24918484687805176 seconds.\n",
      "eval score on batch: 0.19740977196459206\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "825/825 [==============================] - 0s 40us/step\n",
      "825/825 [==============================] - 0s 37us/step\n",
      "updating with 825, 825 training batch.\n",
      "Epoch 1/1\n",
      "825/825 [==============================] - 0s 158us/step - loss: 0.1833\n",
      "Epoch 1/1\n",
      "825/825 [==============================] - 0s 156us/step - loss: 0.1844\n",
      "player 1 wins: 12\n",
      "player 2 wins: 17\n",
      "simulations took 56.31244397163391 seconds.\n",
      "neural network update time: 0.26401686668395996 seconds.\n",
      "eval score on batch: 0.18896721322428098\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "736/736 [==============================] - 0s 45us/step\n",
      "736/736 [==============================] - 0s 41us/step\n",
      "updating with 736, 736 training batch.\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 157us/step - loss: 0.2023\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 160us/step - loss: 0.2003\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 54.7188880443573 seconds.\n",
      "neural network update time: 0.2388169765472412 seconds.\n",
      "eval score on batch: 0.20745001636121585\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "678/678 [==============================] - 0s 42us/step\n",
      "678/678 [==============================] - 0s 37us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.1943\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 162us/step - loss: 0.1978\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 55.010627031326294 seconds.\n",
      "neural network update time: 0.22130513191223145 seconds.\n",
      "eval score on batch: 0.20441494168058466\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "803/803 [==============================] - 0s 41us/step\n",
      "803/803 [==============================] - 0s 41us/step\n",
      "updating with 803, 803 training batch.\n",
      "Epoch 1/1\n",
      "803/803 [==============================] - 0s 156us/step - loss: 0.1973\n",
      "Epoch 1/1\n",
      "803/803 [==============================] - 0s 162us/step - loss: 0.1922\n",
      "player 1 wins: 14\n",
      "player 2 wins: 15\n",
      "simulations took 55.649654150009155 seconds.\n",
      "neural network update time: 0.2608208656311035 seconds.\n",
      "eval score on batch: 0.19682301016226364\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "710/710 [==============================] - 0s 48us/step\n",
      "710/710 [==============================] - 0s 38us/step\n",
      "updating with 710, 710 training batch.\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 156us/step - loss: 0.1803\n",
      "Epoch 1/1\n",
      "710/710 [==============================] - 0s 160us/step - loss: 0.1763\n",
      "player 1 wins: 16\n",
      "player 2 wins: 9\n",
      "simulations took 56.71949481964111 seconds.\n",
      "neural network update time: 0.22956514358520508 seconds.\n",
      "eval score on batch: 0.18223331073121968\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "693/693 [==============================] - 0s 42us/step\n",
      "693/693 [==============================] - 0s 37us/step\n",
      "updating with 693, 693 training batch.\n",
      "Epoch 1/1\n",
      "693/693 [==============================] - 0s 153us/step - loss: 0.1971\n",
      "Epoch 1/1\n",
      "693/693 [==============================] - 0s 160us/step - loss: 0.2025\n",
      "player 1 wins: 9\n",
      "player 2 wins: 17\n",
      "simulations took 55.34324502944946 seconds.\n",
      "neural network update time: 0.22239208221435547 seconds.\n",
      "eval score on batch: 0.2399975448674798\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "686/686 [==============================] - 0s 44us/step\n",
      "686/686 [==============================] - 0s 37us/step\n",
      "updating with 686, 686 training batch.\n",
      "Epoch 1/1\n",
      "686/686 [==============================] - 0s 158us/step - loss: 0.1925\n",
      "Epoch 1/1\n",
      "686/686 [==============================] - 0s 160us/step - loss: 0.1963\n",
      "player 1 wins: 13\n",
      "player 2 wins: 11\n",
      "simulations took 56.84878206253052 seconds.\n",
      "neural network update time: 0.22337818145751953 seconds.\n",
      "eval score on batch: 0.2239628570594176\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "765/765 [==============================] - 0s 41us/step\n",
      "765/765 [==============================] - 0s 45us/step\n",
      "updating with 765, 765 training batch.\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 154us/step - loss: 0.1964\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 155us/step - loss: 0.1963\n",
      "player 1 wins: 10\n",
      "player 2 wins: 19\n",
      "simulations took 54.707942962646484 seconds.\n",
      "neural network update time: 0.24232912063598633 seconds.\n",
      "eval score on batch: 0.21864731755911135\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "633/633 [==============================] - 0s 43us/step\n",
      "633/633 [==============================] - 0s 34us/step\n",
      "updating with 633, 633 training batch.\n",
      "Epoch 1/1\n",
      "633/633 [==============================] - 0s 150us/step - loss: 0.2133\n",
      "Epoch 1/1\n",
      "633/633 [==============================] - 0s 154us/step - loss: 0.2101\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 54.250683307647705 seconds.\n",
      "neural network update time: 0.19775819778442383 seconds.\n",
      "eval score on batch: 0.220750755726067\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "670/670 [==============================] - 0s 37us/step\n",
      "670/670 [==============================] - 0s 41us/step\n",
      "updating with 670, 670 training batch.\n",
      "Epoch 1/1\n",
      "670/670 [==============================] - 0s 150us/step - loss: 0.1944\n",
      "Epoch 1/1\n",
      "670/670 [==============================] - 0s 157us/step - loss: 0.1956\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 56.16077208518982 seconds.\n",
      "neural network update time: 0.2108631134033203 seconds.\n",
      "eval score on batch: 0.19525466163211794\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "775/775 [==============================] - 0s 38us/step\n",
      "775/775 [==============================] - 0s 41us/step\n",
      "updating with 775, 775 training batch.\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 154us/step - loss: 0.1867\n",
      "Epoch 1/1\n",
      "775/775 [==============================] - 0s 160us/step - loss: 0.1893\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 56.10429906845093 seconds.\n",
      "neural network update time: 0.24814891815185547 seconds.\n",
      "eval score on batch: 0.20018203397672021\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "746/746 [==============================] - 0s 39us/step\n",
      "746/746 [==============================] - 0s 41us/step\n",
      "updating with 746, 746 training batch.\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 157us/step - loss: 0.1997\n",
      "Epoch 1/1\n",
      "746/746 [==============================] - 0s 161us/step - loss: 0.2001\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 55.00411295890808 seconds.\n",
      "neural network update time: 0.24229884147644043 seconds.\n",
      "eval score on batch: 0.2085371353730919\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "734/734 [==============================] - 0s 42us/step\n",
      "734/734 [==============================] - 0s 38us/step\n",
      "updating with 734, 734 training batch.\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 153us/step - loss: 0.2069\n",
      "Epoch 1/1\n",
      "734/734 [==============================] - 0s 156us/step - loss: 0.1985\n",
      "player 1 wins: 19\n",
      "player 2 wins: 10\n",
      "simulations took 53.430715799331665 seconds.\n",
      "neural network update time: 0.23195791244506836 seconds.\n",
      "eval score on batch: 0.21286016191058976\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "718/718 [==============================] - 0s 43us/step\n",
      "718/718 [==============================] - 0s 38us/step\n",
      "updating with 718, 718 training batch.\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 156us/step - loss: 0.2005\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 158us/step - loss: 0.2064\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 54.95157194137573 seconds.\n",
      "neural network update time: 0.2313070297241211 seconds.\n",
      "eval score on batch: 0.22218124448090876\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "684/684 [==============================] - 0s 40us/step\n",
      "684/684 [==============================] - 0s 41us/step\n",
      "updating with 684, 684 training batch.\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 155us/step - loss: 0.1932\n",
      "Epoch 1/1\n",
      "684/684 [==============================] - 0s 161us/step - loss: 0.1912\n",
      "player 1 wins: 15\n",
      "player 2 wins: 10\n",
      "simulations took 55.81651830673218 seconds.\n",
      "neural network update time: 0.22175073623657227 seconds.\n",
      "eval score on batch: 0.20224377239526015\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "777/777 [==============================] - 0s 48us/step\n",
      "777/777 [==============================] - 0s 37us/step\n",
      "updating with 777, 777 training batch.\n",
      "Epoch 1/1\n",
      "777/777 [==============================] - 0s 160us/step - loss: 0.1966\n",
      "Epoch 1/1\n",
      "777/777 [==============================] - 0s 162us/step - loss: 0.1958\n",
      "player 1 wins: 16\n",
      "player 2 wins: 13\n",
      "simulations took 54.637070178985596 seconds.\n",
      "neural network update time: 0.2564668655395508 seconds.\n",
      "eval score on batch: 0.20081093864685626\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "693/693 [==============================] - 0s 40us/step\n",
      "693/693 [==============================] - 0s 45us/step\n",
      "updating with 693, 693 training batch.\n",
      "Epoch 1/1\n",
      "693/693 [==============================] - 0s 157us/step - loss: 0.1968\n",
      "Epoch 1/1\n",
      "693/693 [==============================] - 0s 161us/step - loss: 0.1964\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 55.191380977630615 seconds.\n",
      "neural network update time: 0.22654390335083008 seconds.\n",
      "eval score on batch: 0.19602058021537153\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "742/742 [==============================] - 0s 43us/step\n",
      "742/742 [==============================] - 0s 37us/step\n",
      "updating with 742, 742 training batch.\n",
      "Epoch 1/1\n",
      "742/742 [==============================] - 0s 159us/step - loss: 0.1935\n",
      "Epoch 1/1\n",
      "742/742 [==============================] - 0s 160us/step - loss: 0.1932\n",
      "player 1 wins: 11\n",
      "player 2 wins: 17\n",
      "simulations took 54.756404876708984 seconds.\n",
      "neural network update time: 0.24224162101745605 seconds.\n",
      "eval score on batch: 0.19587530591901683\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "79 outputs loaded.\n",
      "646/646 [==============================] - 0s 38us/step\n",
      "646/646 [==============================] - 0s 41us/step\n",
      "updating with 646, 646 training batch.\n",
      "Epoch 1/1\n",
      "646/646 [==============================] - 0s 155us/step - loss: 0.2013\n",
      "Epoch 1/1\n",
      "646/646 [==============================] - 0s 162us/step - loss: 0.2026\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 54.498111963272095 seconds.\n",
      "neural network update time: 0.2094581127166748 seconds.\n",
      "eval score on batch: 0.2029639058299478\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "762/762 [==============================] - 0s 42us/step\n",
      "762/762 [==============================] - 0s 41us/step\n",
      "updating with 762, 762 training batch.\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 153us/step - loss: 0.1921\n",
      "Epoch 1/1\n",
      "762/762 [==============================] - 0s 159us/step - loss: 0.1926\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 55.40451908111572 seconds.\n",
      "neural network update time: 0.2435610294342041 seconds.\n",
      "eval score on batch: 0.19316983903486898\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "660/660 [==============================] - 0s 47us/step\n",
      "660/660 [==============================] - 0s 43us/step\n",
      "updating with 660, 660 training batch.\n",
      "Epoch 1/1\n",
      "660/660 [==============================] - 0s 158us/step - loss: 0.1972\n",
      "Epoch 1/1\n",
      "660/660 [==============================] - 0s 158us/step - loss: 0.1995\n",
      "player 1 wins: 8\n",
      "player 2 wins: 18\n",
      "simulations took 53.64081907272339 seconds.\n",
      "neural network update time: 0.21592116355895996 seconds.\n",
      "eval score on batch: 0.21582359621922176\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "756/756 [==============================] - 0s 46us/step\n",
      "756/756 [==============================] - 0s 41us/step\n",
      "updating with 756, 756 training batch.\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 154us/step - loss: 0.1822\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 157us/step - loss: 0.1812\n",
      "player 1 wins: 9\n",
      "player 2 wins: 19\n",
      "simulations took 54.77936100959778 seconds.\n",
      "neural network update time: 0.24042391777038574 seconds.\n",
      "eval score on batch: 0.18488838650266487\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "681/681 [==============================] - 0s 44us/step\n",
      "681/681 [==============================] - 0s 37us/step\n",
      "updating with 681, 681 training batch.\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 162us/step - loss: 0.2030\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 162us/step - loss: 0.2022\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 53.52021312713623 seconds.\n",
      "neural network update time: 0.22528481483459473 seconds.\n",
      "eval score on batch: 0.2024792052706958\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "811/811 [==============================] - 0s 44us/step\n",
      "811/811 [==============================] - 0s 38us/step\n",
      "updating with 811, 811 training batch.\n",
      "Epoch 1/1\n",
      "811/811 [==============================] - 0s 159us/step - loss: 0.1961\n",
      "Epoch 1/1\n",
      "811/811 [==============================] - 0s 159us/step - loss: 0.1941\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 54.764665842056274 seconds.\n",
      "neural network update time: 0.26361083984375 seconds.\n",
      "eval score on batch: 0.20000156011023593\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "663/663 [==============================] - 0s 39us/step\n",
      "663/663 [==============================] - 0s 38us/step\n",
      "updating with 663, 663 training batch.\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 155us/step - loss: 0.1995\n",
      "Epoch 1/1\n",
      "663/663 [==============================] - 0s 154us/step - loss: 0.1971\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 54.40691828727722 seconds.\n",
      "neural network update time: 0.2103416919708252 seconds.\n",
      "eval score on batch: 0.19841396033224476\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "765/765 [==============================] - 0s 39us/step\n",
      "765/765 [==============================] - 0s 40us/step\n",
      "updating with 765, 765 training batch.\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 153us/step - loss: 0.1843\n",
      "Epoch 1/1\n",
      "765/765 [==============================] - 0s 155us/step - loss: 0.1918\n",
      "player 1 wins: 7\n",
      "player 2 wins: 22\n",
      "simulations took 53.83328104019165 seconds.\n",
      "neural network update time: 0.2418689727783203 seconds.\n",
      "eval score on batch: 0.24217479403307235\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "665/665 [==============================] - 0s 44us/step\n",
      "665/665 [==============================] - 0s 42us/step\n",
      "updating with 665, 665 training batch.\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 152us/step - loss: 0.2091\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 157us/step - loss: 0.2096\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 54.469512939453125 seconds.\n",
      "neural network update time: 0.21153783798217773 seconds.\n",
      "eval score on batch: 0.22256983455858734\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "712/712 [==============================] - 0s 39us/step\n",
      "712/712 [==============================] - 0s 39us/step\n",
      "updating with 712, 712 training batch.\n",
      "Epoch 1/1\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.1866\n",
      "Epoch 1/1\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.1861\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 55.728659868240356 seconds.\n",
      "neural network update time: 0.23067307472229004 seconds.\n",
      "eval score on batch: 0.18614641382434835\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "718/718 [==============================] - 0s 40us/step\n",
      "718/718 [==============================] - 0s 42us/step\n",
      "updating with 718, 718 training batch.\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 156us/step - loss: 0.1791\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 161us/step - loss: 0.1792\n",
      "player 1 wins: 9\n",
      "player 2 wins: 17\n",
      "simulations took 56.10872721672058 seconds.\n",
      "neural network update time: 0.23326897621154785 seconds.\n",
      "eval score on batch: 0.17987779935194093\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "786/786 [==============================] - 0s 38us/step\n",
      "786/786 [==============================] - 0s 42us/step\n",
      "updating with 786, 786 training batch.\n",
      "Epoch 1/1\n",
      "786/786 [==============================] - 0s 155us/step - loss: 0.1903\n",
      "Epoch 1/1\n",
      "786/786 [==============================] - 0s 157us/step - loss: 0.1898\n",
      "player 1 wins: 19\n",
      "player 2 wins: 9\n",
      "simulations took 55.83585500717163 seconds.\n",
      "neural network update time: 0.2510871887207031 seconds.\n",
      "eval score on batch: 0.2245120887879197\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "679/679 [==============================] - 0s 42us/step\n",
      "679/679 [==============================] - 0s 43us/step\n",
      "updating with 679, 679 training batch.\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 155us/step - loss: 0.2090\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 164us/step - loss: 0.2048\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 56.32089400291443 seconds.\n",
      "neural network update time: 0.22213315963745117 seconds.\n",
      "eval score on batch: 0.2435694553276225\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "736/736 [==============================] - 0s 42us/step\n",
      "736/736 [==============================] - 0s 39us/step\n",
      "updating with 736, 736 training batch.\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 151us/step - loss: 0.1835\n",
      "Epoch 1/1\n",
      "736/736 [==============================] - 0s 156us/step - loss: 0.1811\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 56.814701080322266 seconds.\n",
      "neural network update time: 0.2310488224029541 seconds.\n",
      "eval score on batch: 0.18244369036477545\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "719/719 [==============================] - 0s 39us/step\n",
      "719/719 [==============================] - 0s 39us/step\n",
      "updating with 719, 719 training batch.\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 159us/step - loss: 0.1971\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 173us/step - loss: 0.1975\n",
      "player 1 wins: 13\n",
      "player 2 wins: 14\n",
      "simulations took 55.28875994682312 seconds.\n",
      "neural network update time: 0.2441558837890625 seconds.\n",
      "eval score on batch: 0.19824304602138687\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "682/682 [==============================] - 0s 46us/step\n",
      "682/682 [==============================] - 0s 43us/step\n",
      "updating with 682, 682 training batch.\n",
      "Epoch 1/1\n",
      "682/682 [==============================] - 0s 159us/step - loss: 0.2059\n",
      "Epoch 1/1\n",
      "682/682 [==============================] - 0s 161us/step - loss: 0.2067\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 53.69912505149841 seconds.\n",
      "neural network update time: 0.22436189651489258 seconds.\n",
      "eval score on batch: 0.2060807017374598\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "699/699 [==============================] - 0s 42us/step\n",
      "699/699 [==============================] - 0s 38us/step\n",
      "updating with 699, 699 training batch.\n",
      "Epoch 1/1\n",
      "699/699 [==============================] - 0s 152us/step - loss: 0.1945\n",
      "Epoch 1/1\n",
      "699/699 [==============================] - 0s 162us/step - loss: 0.1942\n",
      "player 1 wins: 14\n",
      "player 2 wins: 12\n",
      "simulations took 55.3543221950531 seconds.\n",
      "neural network update time: 0.2253279685974121 seconds.\n",
      "eval score on batch: 0.19378251248760114\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "722/722 [==============================] - 0s 38us/step\n",
      "722/722 [==============================] - 0s 40us/step\n",
      "updating with 722, 722 training batch.\n",
      "Epoch 1/1\n",
      "722/722 [==============================] - 0s 155us/step - loss: 0.1956\n",
      "Epoch 1/1\n",
      "722/722 [==============================] - 0s 164us/step - loss: 0.1958\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 54.607521057128906 seconds.\n",
      "neural network update time: 0.23597002029418945 seconds.\n",
      "eval score on batch: 0.19560790648090542\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "749/749 [==============================] - 0s 39us/step\n",
      "749/749 [==============================] - 0s 39us/step\n",
      "updating with 749, 749 training batch.\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 169us/step - loss: 0.1956\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 168us/step - loss: 0.1959\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 55.51479506492615 seconds.\n",
      "neural network update time: 0.25818800926208496 seconds.\n",
      "eval score on batch: 0.1957431438494628\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "660/660 [==============================] - 0s 40us/step\n",
      "660/660 [==============================] - 0s 41us/step\n",
      "updating with 660, 660 training batch.\n",
      "Epoch 1/1\n",
      "660/660 [==============================] - 0s 154us/step - loss: 0.2057\n",
      "Epoch 1/1\n",
      "660/660 [==============================] - 0s 155us/step - loss: 0.2066\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 54.460790157318115 seconds.\n",
      "neural network update time: 0.2090139389038086 seconds.\n",
      "eval score on batch: 0.20598462962291458\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "80 outputs loaded.\n",
      "732/732 [==============================] - 0s 44us/step\n",
      "732/732 [==============================] - 0s 37us/step\n",
      "updating with 732, 732 training batch.\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 160us/step - loss: 0.1738\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 157us/step - loss: 0.1760\n",
      "player 1 wins: 18\n",
      "player 2 wins: 8\n",
      "simulations took 56.85524082183838 seconds.\n",
      "neural network update time: 0.2380390167236328 seconds.\n",
      "eval score on batch: 0.18568745019331656\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "679/679 [==============================] - 0s 54us/step\n",
      "679/679 [==============================] - 0s 38us/step\n",
      "updating with 679, 679 training batch.\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 159us/step - loss: 0.1916\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 162us/step - loss: 0.1900\n",
      "player 1 wins: 10\n",
      "player 2 wins: 13\n",
      "simulations took 58.58559513092041 seconds.\n",
      "neural network update time: 0.22287893295288086 seconds.\n",
      "eval score on batch: 0.21847506620220303\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "879/879 [==============================] - 0s 41us/step\n",
      "879/879 [==============================] - 0s 40us/step\n",
      "updating with 879, 879 training batch.\n",
      "Epoch 1/1\n",
      "879/879 [==============================] - 0s 156us/step - loss: 0.1857\n",
      "Epoch 1/1\n",
      "879/879 [==============================] - 0s 156us/step - loss: 0.1871\n",
      "player 1 wins: 17\n",
      "player 2 wins: 14\n",
      "simulations took 56.8649320602417 seconds.\n",
      "neural network update time: 0.2804141044616699 seconds.\n",
      "eval score on batch: 0.19928637275788022\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "767/767 [==============================] - 0s 42us/step\n",
      "767/767 [==============================] - 0s 37us/step\n",
      "updating with 767, 767 training batch.\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 156us/step - loss: 0.1901\n",
      "Epoch 1/1\n",
      "767/767 [==============================] - 0s 157us/step - loss: 0.1901\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 56.74853229522705 seconds.\n",
      "neural network update time: 0.24545502662658691 seconds.\n",
      "eval score on batch: 0.19020688879598693\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "732/732 [==============================] - 0s 38us/step\n",
      "732/732 [==============================] - 0s 40us/step\n",
      "updating with 732, 732 training batch.\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 151us/step - loss: 0.1980\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 155us/step - loss: 0.1984\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 54.998661279678345 seconds.\n",
      "neural network update time: 0.22986483573913574 seconds.\n",
      "eval score on batch: 0.19752858204594076\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "732/732 [==============================] - 0s 37us/step\n",
      "732/732 [==============================] - 0s 40us/step\n",
      "updating with 732, 732 training batch.\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 157us/step - loss: 0.1870\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 159us/step - loss: 0.1857\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 57.032270193099976 seconds.\n",
      "neural network update time: 0.23704123497009277 seconds.\n",
      "eval score on batch: 0.18772277415124444\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "756/756 [==============================] - 0s 42us/step\n",
      "756/756 [==============================] - 0s 37us/step\n",
      "updating with 756, 756 training batch.\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 157us/step - loss: 0.1891\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 157us/step - loss: 0.1895\n",
      "player 1 wins: 11\n",
      "player 2 wins: 17\n",
      "simulations took 56.199440240859985 seconds.\n",
      "neural network update time: 0.2431778907775879 seconds.\n",
      "eval score on batch: 0.1920496016819641\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "647/647 [==============================] - 0s 46us/step\n",
      "647/647 [==============================] - 0s 42us/step\n",
      "updating with 647, 647 training batch.\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 191us/step - loss: 0.1866\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 205us/step - loss: 0.1864\n",
      "player 1 wins: 8\n",
      "player 2 wins: 17\n",
      "simulations took 60.869812965393066 seconds.\n",
      "neural network update time: 0.2613258361816406 seconds.\n",
      "eval score on batch: 0.18687345573854586\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "732/732 [==============================] - 0s 55us/step\n",
      "732/732 [==============================] - 0s 78us/step\n",
      "updating with 732, 732 training batch.\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 282us/step - loss: 0.1703\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 296us/step - loss: 0.1709\n",
      "player 1 wins: 8\n",
      "player 2 wins: 18\n",
      "simulations took 84.91585993766785 seconds.\n",
      "neural network update time: 0.4354090690612793 seconds.\n",
      "eval score on batch: 0.17012298457283792\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "641/641 [==============================] - 0s 69us/step\n",
      "641/641 [==============================] - 0s 90us/step\n",
      "updating with 641, 641 training batch.\n",
      "Epoch 1/1\n",
      "641/641 [==============================] - 0s 299us/step - loss: 0.2020\n",
      "Epoch 1/1\n",
      "641/641 [==============================] - 0s 301us/step - loss: 0.1975\n",
      "player 1 wins: 11\n",
      "player 2 wins: 13\n",
      "simulations took 94.74506187438965 seconds.\n",
      "neural network update time: 0.39322590827941895 seconds.\n",
      "eval score on batch: 0.20458969812078892\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "647/647 [==============================] - 0s 72us/step\n",
      "647/647 [==============================] - 0s 82us/step\n",
      "updating with 647, 647 training batch.\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 299us/step - loss: 0.2025\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 278us/step - loss: 0.2021\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 93.68971490859985 seconds.\n",
      "neural network update time: 0.38303399085998535 seconds.\n",
      "eval score on batch: 0.20165588834482767\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "670/670 [==============================] - 0s 48us/step\n",
      "670/670 [==============================] - 0s 48us/step\n",
      "updating with 670, 670 training batch.\n",
      "Epoch 1/1\n",
      "670/670 [==============================] - 0s 275us/step - loss: 0.1958\n",
      "Epoch 1/1\n",
      "670/670 [==============================] - 0s 312us/step - loss: 0.1955\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 82.43471622467041 seconds.\n",
      "neural network update time: 0.4006929397583008 seconds.\n",
      "eval score on batch: 0.19524905036634474\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "829/829 [==============================] - 0s 64us/step\n",
      "829/829 [==============================] - 0s 74us/step\n",
      "updating with 829, 829 training batch.\n",
      "Epoch 1/1\n",
      "829/829 [==============================] - 0s 367us/step - loss: 0.2029\n",
      "Epoch 1/1\n",
      "829/829 [==============================] - 0s 392us/step - loss: 0.2004\n",
      "player 1 wins: 14\n",
      "player 2 wins: 18\n",
      "simulations took 73.88055896759033 seconds.\n",
      "neural network update time: 0.6384961605072021 seconds.\n",
      "eval score on batch: 0.20046268363985423\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "785/785 [==============================] - 0s 85us/step\n",
      "785/785 [==============================] - 0s 97us/step\n",
      "updating with 785, 785 training batch.\n",
      "Epoch 1/1\n",
      "785/785 [==============================] - 0s 319us/step - loss: 0.2002\n",
      "Epoch 1/1\n",
      "785/785 [==============================] - 0s 321us/step - loss: 0.2001\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 83.30416703224182 seconds.\n",
      "neural network update time: 0.5105059146881104 seconds.\n",
      "eval score on batch: 0.20154298609656512\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "711/711 [==============================] - 0s 55us/step\n",
      "711/711 [==============================] - 0s 62us/step\n",
      "updating with 711, 711 training batch.\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 296us/step - loss: 0.2032\n",
      "Epoch 1/1\n",
      "711/711 [==============================] - 0s 288us/step - loss: 0.2014\n",
      "player 1 wins: 11\n",
      "player 2 wins: 17\n",
      "simulations took 88.9612181186676 seconds.\n",
      "neural network update time: 0.4246089458465576 seconds.\n",
      "eval score on batch: 0.21000102478958424\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "615/615 [==============================] - 0s 57us/step\n",
      "615/615 [==============================] - 0s 58us/step\n",
      "updating with 615, 615 training batch.\n",
      "Epoch 1/1\n",
      "615/615 [==============================] - 0s 322us/step - loss: 0.2101\n",
      "Epoch 1/1\n",
      "615/615 [==============================] - 0s 337us/step - loss: 0.2175\n",
      "player 1 wins: 14\n",
      "player 2 wins: 10\n",
      "simulations took 82.98197603225708 seconds.\n",
      "neural network update time: 0.4151461124420166 seconds.\n",
      "eval score on batch: 0.23639500931935098\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "661/661 [==============================] - 0s 59us/step\n",
      "661/661 [==============================] - 0s 65us/step\n",
      "updating with 661, 661 training batch.\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 271us/step - loss: 0.2075\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 257us/step - loss: 0.2117\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 77.59889101982117 seconds.\n",
      "neural network update time: 0.3586390018463135 seconds.\n",
      "eval score on batch: 0.2133841461385072\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "614/614 [==============================] - 0s 55us/step\n",
      "614/614 [==============================] - 0s 65us/step\n",
      "updating with 614, 614 training batch.\n",
      "Epoch 1/1\n",
      "614/614 [==============================] - 0s 317us/step - loss: 0.1994\n",
      "Epoch 1/1\n",
      "614/614 [==============================] - 0s 311us/step - loss: 0.1930\n",
      "player 1 wins: 14\n",
      "player 2 wins: 9\n",
      "simulations took 87.0976300239563 seconds.\n",
      "neural network update time: 0.3939371109008789 seconds.\n",
      "eval score on batch: 0.2001504744372728\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "715/715 [==============================] - 0s 76us/step\n",
      "715/715 [==============================] - 0s 57us/step\n",
      "updating with 715, 715 training batch.\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 284us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 290us/step - loss: 0.2048\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 90.11536908149719 seconds.\n",
      "neural network update time: 0.4188039302825928 seconds.\n",
      "eval score on batch: 0.21610518075994678\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "737/737 [==============================] - 0s 60us/step\n",
      "737/737 [==============================] - 0s 70us/step\n",
      "updating with 737, 737 training batch.\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 242us/step - loss: 0.1858\n",
      "Epoch 1/1\n",
      "737/737 [==============================] - 0s 228us/step - loss: 0.1849\n",
      "player 1 wins: 12\n",
      "player 2 wins: 14\n",
      "simulations took 86.39103889465332 seconds.\n",
      "neural network update time: 0.35451698303222656 seconds.\n",
      "eval score on batch: 0.18509234555363857\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "81 outputs loaded.\n",
      "727/727 [==============================] - 0s 59us/step\n",
      "727/727 [==============================] - 0s 61us/step\n",
      "updating with 727, 727 training batch.\n",
      "Epoch 1/1\n",
      "727/727 [==============================] - 0s 204us/step - loss: 0.2045\n",
      "Epoch 1/1\n",
      "727/727 [==============================] - 0s 207us/step - loss: 0.2020\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 81.36294102668762 seconds.\n",
      "neural network update time: 0.30736422538757324 seconds.\n",
      "eval score on batch: 0.20440425661275935\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "697/697 [==============================] - 0s 84us/step\n",
      "697/697 [==============================] - 0s 78us/step\n",
      "updating with 697, 697 training batch.\n",
      "Epoch 1/1\n",
      "697/697 [==============================] - 0s 304us/step - loss: 0.1884\n",
      "Epoch 1/1\n",
      "697/697 [==============================] - 0s 329us/step - loss: 0.1876\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 86.26971983909607 seconds.\n",
      "neural network update time: 0.4490242004394531 seconds.\n",
      "eval score on batch: 0.19227240428093345\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "733/733 [==============================] - 0s 83us/step\n",
      "733/733 [==============================] - 0s 82us/step\n",
      "updating with 733, 733 training batch.\n",
      "Epoch 1/1\n",
      "733/733 [==============================] - 0s 310us/step - loss: 0.2019\n",
      "Epoch 1/1\n",
      "733/733 [==============================] - 0s 276us/step - loss: 0.2025\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 90.39255976676941 seconds.\n",
      "neural network update time: 0.437000036239624 seconds.\n",
      "eval score on batch: 0.2096070636465312\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "839/839 [==============================] - 0s 84us/step\n",
      "839/839 [==============================] - 0s 70us/step\n",
      "updating with 839, 839 training batch.\n",
      "Epoch 1/1\n",
      "839/839 [==============================] - 0s 315us/step - loss: 0.1891\n",
      "Epoch 1/1\n",
      "839/839 [==============================] - 0s 336us/step - loss: 0.1893\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 88.6985981464386 seconds.\n",
      "neural network update time: 0.5589098930358887 seconds.\n",
      "eval score on batch: 0.19095058583273442\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "645/645 [==============================] - 0s 69us/step\n",
      "645/645 [==============================] - 0s 84us/step\n",
      "updating with 645, 645 training batch.\n",
      "Epoch 1/1\n",
      "645/645 [==============================] - 0s 261us/step - loss: 0.1923\n",
      "Epoch 1/1\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.195 - 0s 301us/step - loss: 0.1923\n",
      "player 1 wins: 10\n",
      "player 2 wins: 14\n",
      "simulations took 92.19939994812012 seconds.\n",
      "neural network update time: 0.3756542205810547 seconds.\n",
      "eval score on batch: 0.1924826727530291\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "640/640 [==============================] - 0s 64us/step\n",
      "640/640 [==============================] - 0s 68us/step\n",
      "updating with 640, 640 training batch.\n",
      "Epoch 1/1\n",
      "640/640 [==============================] - 0s 272us/step - loss: 0.2067\n",
      "Epoch 1/1\n",
      "640/640 [==============================] - 0s 335us/step - loss: 0.2060\n",
      "player 1 wins: 14\n",
      "player 2 wins: 11\n",
      "simulations took 83.70503497123718 seconds.\n",
      "neural network update time: 0.39551305770874023 seconds.\n",
      "eval score on batch: 0.21390050649642944\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "698/698 [==============================] - 0s 64us/step\n",
      "698/698 [==============================] - 0s 64us/step\n",
      "updating with 698, 698 training batch.\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 277us/step - loss: 0.2044\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 246us/step - loss: 0.2071\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 83.966561794281 seconds.\n",
      "neural network update time: 0.37422704696655273 seconds.\n",
      "eval score on batch: 0.21961069871327255\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "750/750 [==============================] - 0s 63us/step\n",
      "750/750 [==============================] - 0s 60us/step\n",
      "updating with 750, 750 training batch.\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 254us/step - loss: 0.1995\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 272us/step - loss: 0.1985\n",
      "player 1 wins: 15\n",
      "player 2 wins: 13\n",
      "simulations took 84.7106728553772 seconds.\n",
      "neural network update time: 0.40140414237976074 seconds.\n",
      "eval score on batch: 0.20455494145552316\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "673/673 [==============================] - 0s 68us/step\n",
      "673/673 [==============================] - 0s 62us/step\n",
      "updating with 673, 673 training batch.\n",
      "Epoch 1/1\n",
      "673/673 [==============================] - 0s 274us/step - loss: 0.2028\n",
      "Epoch 1/1\n",
      "673/673 [==============================] - 0s 262us/step - loss: 0.2025\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 83.43101477622986 seconds.\n",
      "neural network update time: 0.3684349060058594 seconds.\n",
      "eval score on batch: 0.2023475880652976\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "760/760 [==============================] - 0s 42us/step\n",
      "760/760 [==============================] - 0s 40us/step\n",
      "updating with 760, 760 training batch.\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 185us/step - loss: 0.1890\n",
      "Epoch 1/1\n",
      "760/760 [==============================] - 0s 186us/step - loss: 0.1893\n",
      "player 1 wins: 17\n",
      "player 2 wins: 11\n",
      "simulations took 70.84579300880432 seconds.\n",
      "neural network update time: 0.28778600692749023 seconds.\n",
      "eval score on batch: 0.19242110362178402\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "620/620 [==============================] - 0s 41us/step\n",
      "620/620 [==============================] - 0s 41us/step\n",
      "updating with 620, 620 training batch.\n",
      "Epoch 1/1\n",
      "620/620 [==============================] - 0s 189us/step - loss: 0.1972\n",
      "Epoch 1/1\n",
      "620/620 [==============================] - 0s 190us/step - loss: 0.1966\n",
      "player 1 wins: 12\n",
      "player 2 wins: 11\n",
      "simulations took 62.21625995635986 seconds.\n",
      "neural network update time: 0.242110013961792 seconds.\n",
      "eval score on batch: 0.19920955595950926\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "624/624 [==============================] - 0s 55us/step\n",
      "624/624 [==============================] - 0s 44us/step\n",
      "updating with 624, 624 training batch.\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 206us/step - loss: 0.1899\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 213us/step - loss: 0.1907\n",
      "player 1 wins: 14\n",
      "player 2 wins: 9\n",
      "simulations took 62.77528095245361 seconds.\n",
      "neural network update time: 0.2692890167236328 seconds.\n",
      "eval score on batch: 0.19230214195946851\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "732/732 [==============================] - 0s 53us/step\n",
      "732/732 [==============================] - 0s 59us/step\n",
      "updating with 732, 732 training batch.\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 269us/step - loss: 0.2089\n",
      "Epoch 1/1\n",
      "732/732 [==============================] - 0s 233us/step - loss: 0.2019\n",
      "player 1 wins: 12\n",
      "player 2 wins: 16\n",
      "simulations took 60.16294503211975 seconds.\n",
      "neural network update time: 0.37640905380249023 seconds.\n",
      "eval score on batch: 0.21709666991494392\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "685/685 [==============================] - 0s 40us/step\n",
      "685/685 [==============================] - 0s 41us/step\n",
      "updating with 685, 685 training batch.\n",
      "Epoch 1/1\n",
      "685/685 [==============================] - 0s 183us/step - loss: 0.1974\n",
      "Epoch 1/1\n",
      "685/685 [==============================] - 0s 211us/step - loss: 0.1974\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 61.24663186073303 seconds.\n",
      "neural network update time: 0.2762119770050049 seconds.\n",
      "eval score on batch: 0.19915630598790454\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "678/678 [==============================] - 0s 40us/step\n",
      "678/678 [==============================] - 0s 40us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 179us/step - loss: 0.1916\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 184us/step - loss: 0.1904\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 60.84771108627319 seconds.\n",
      "neural network update time: 0.2511420249938965 seconds.\n",
      "eval score on batch: 0.19044525976393065\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "817/817 [==============================] - 0s 44us/step\n",
      "817/817 [==============================] - 0s 35us/step\n",
      "updating with 817, 817 training batch.\n",
      "Epoch 1/1\n",
      "817/817 [==============================] - 0s 179us/step - loss: 0.1927\n",
      "Epoch 1/1\n",
      "817/817 [==============================] - 0s 180us/step - loss: 0.1934\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 60.48808217048645 seconds.\n",
      "neural network update time: 0.29932308197021484 seconds.\n",
      "eval score on batch: 0.1971856354618379\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "742/742 [==============================] - 0s 42us/step\n",
      "742/742 [==============================] - 0s 44us/step\n",
      "updating with 742, 742 training batch.\n",
      "Epoch 1/1\n",
      "742/742 [==============================] - 0s 195us/step - loss: 0.1878\n",
      "Epoch 1/1\n",
      "742/742 [==============================] - 0s 198us/step - loss: 0.1870\n",
      "player 1 wins: 16\n",
      "player 2 wins: 11\n",
      "simulations took 61.73256492614746 seconds.\n",
      "neural network update time: 0.2981710433959961 seconds.\n",
      "eval score on batch: 0.18677215891305954\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "706/706 [==============================] - 0s 46us/step\n",
      "706/706 [==============================] - 0s 49us/step\n",
      "updating with 706, 706 training batch.\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 202us/step - loss: 0.1966\n",
      "Epoch 1/1\n",
      "706/706 [==============================] - 0s 203us/step - loss: 0.1970\n",
      "player 1 wins: 18\n",
      "player 2 wins: 10\n",
      "simulations took 59.47173023223877 seconds.\n",
      "neural network update time: 0.29271507263183594 seconds.\n",
      "eval score on batch: 0.19848980189150361\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "673/673 [==============================] - 0s 42us/step\n",
      "673/673 [==============================] - 0s 41us/step\n",
      "updating with 673, 673 training batch.\n",
      "Epoch 1/1\n",
      "673/673 [==============================] - 0s 176us/step - loss: 0.1962\n",
      "Epoch 1/1\n",
      "673/673 [==============================] - 0s 184us/step - loss: 0.1959\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 60.192813873291016 seconds.\n",
      "neural network update time: 0.2475740909576416 seconds.\n",
      "eval score on batch: 0.1956344968189567\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "740/740 [==============================] - 0s 41us/step\n",
      "740/740 [==============================] - 0s 44us/step\n",
      "updating with 740, 740 training batch.\n",
      "Epoch 1/1\n",
      "740/740 [==============================] - 0s 189us/step - loss: 0.1903\n",
      "Epoch 1/1\n",
      "740/740 [==============================] - 0s 189us/step - loss: 0.1967\n",
      "player 1 wins: 9\n",
      "player 2 wins: 18\n",
      "simulations took 62.131327867507935 seconds.\n",
      "neural network update time: 0.28577113151550293 seconds.\n",
      "eval score on batch: 0.21585094098102403\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "82 outputs loaded.\n",
      "811/811 [==============================] - 0s 39us/step\n",
      "811/811 [==============================] - 0s 39us/step\n",
      "updating with 811, 811 training batch.\n",
      "Epoch 1/1\n",
      "811/811 [==============================] - 0s 179us/step - loss: 0.1966\n",
      "Epoch 1/1\n",
      "811/811 [==============================] - 0s 190us/step - loss: 0.1889\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 61.951438188552856 seconds.\n",
      "neural network update time: 0.3045039176940918 seconds.\n",
      "eval score on batch: 0.2113990314625222\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "715/715 [==============================] - 0s 47us/step\n",
      "715/715 [==============================] - 0s 60us/step\n",
      "updating with 715, 715 training batch.\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 207us/step - loss: 0.1768\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 217us/step - loss: 0.1819\n",
      "player 1 wins: 8\n",
      "player 2 wins: 18\n",
      "simulations took 63.15608191490173 seconds.\n",
      "neural network update time: 0.3108499050140381 seconds.\n",
      "eval score on batch: 0.21217385992095186\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "703/703 [==============================] - 0s 57us/step\n",
      "703/703 [==============================] - 0s 51us/step\n",
      "updating with 703, 703 training batch.\n",
      "Epoch 1/1\n",
      "703/703 [==============================] - 0s 190us/step - loss: 0.2184\n",
      "Epoch 1/1\n",
      "703/703 [==============================] - 0s 192us/step - loss: 0.2160\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 59.599164724349976 seconds.\n",
      "neural network update time: 0.27465295791625977 seconds.\n",
      "eval score on batch: 0.23290533764263982\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "790/790 [==============================] - 0s 50us/step\n",
      "790/790 [==============================] - 0s 45us/step\n",
      "updating with 790, 790 training batch.\n",
      "Epoch 1/1\n",
      "790/790 [==============================] - 0s 244us/step - loss: 0.1888\n",
      "Epoch 1/1\n",
      "790/790 [==============================] - 0s 211us/step - loss: 0.1870\n",
      "player 1 wins: 10\n",
      "player 2 wins: 19\n",
      "simulations took 62.14751100540161 seconds.\n",
      "neural network update time: 0.36815714836120605 seconds.\n",
      "eval score on batch: 0.1956210504981536\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "735/735 [==============================] - 0s 40us/step\n",
      "735/735 [==============================] - 0s 39us/step\n",
      "updating with 735, 735 training batch.\n",
      "Epoch 1/1\n",
      "735/735 [==============================] - 0s 175us/step - loss: 0.1959\n",
      "Epoch 1/1\n",
      "735/735 [==============================] - 0s 178us/step - loss: 0.1969\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 64.56867504119873 seconds.\n",
      "neural network update time: 0.26450395584106445 seconds.\n",
      "eval score on batch: 0.21235052797259116\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "741/741 [==============================] - 0s 44us/step\n",
      "741/741 [==============================] - 0s 41us/step\n",
      "updating with 741, 741 training batch.\n",
      "Epoch 1/1\n",
      "741/741 [==============================] - 0s 196us/step - loss: 0.2023\n",
      "Epoch 1/1\n",
      "741/741 [==============================] - 0s 189us/step - loss: 0.1999\n",
      "player 1 wins: 12\n",
      "player 2 wins: 16\n",
      "simulations took 60.44628190994263 seconds.\n",
      "neural network update time: 0.2916228771209717 seconds.\n",
      "eval score on batch: 0.20675458868809737\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "709/709 [==============================] - 0s 42us/step\n",
      "709/709 [==============================] - 0s 43us/step\n",
      "updating with 709, 709 training batch.\n",
      "Epoch 1/1\n",
      "709/709 [==============================] - 0s 174us/step - loss: 0.1965\n",
      "Epoch 1/1\n",
      "709/709 [==============================] - 0s 180us/step - loss: 0.2011\n",
      "player 1 wins: 17\n",
      "player 2 wins: 10\n",
      "simulations took 60.74412202835083 seconds.\n",
      "neural network update time: 0.25594019889831543 seconds.\n",
      "eval score on batch: 0.21636708261253385\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "661/661 [==============================] - 0s 42us/step\n",
      "661/661 [==============================] - 0s 41us/step\n",
      "updating with 661, 661 training batch.\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 194us/step - loss: 0.2173\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 211us/step - loss: 0.2120\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 60.16723704338074 seconds.\n",
      "neural network update time: 0.27485108375549316 seconds.\n",
      "eval score on batch: 0.23859510667419198\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "702/702 [==============================] - 0s 41us/step\n",
      "702/702 [==============================] - 0s 38us/step\n",
      "updating with 702, 702 training batch.\n",
      "Epoch 1/1\n",
      "702/702 [==============================] - 0s 171us/step - loss: 0.2026\n",
      "Epoch 1/1\n",
      "702/702 [==============================] - 0s 172us/step - loss: 0.1985\n",
      "player 1 wins: 16\n",
      "player 2 wins: 10\n",
      "simulations took 61.502021074295044 seconds.\n",
      "neural network update time: 0.24781107902526855 seconds.\n",
      "eval score on batch: 0.22288223218034814\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "641/641 [==============================] - 0s 44us/step\n",
      "641/641 [==============================] - 0s 39us/step\n",
      "updating with 641, 641 training batch.\n",
      "Epoch 1/1\n",
      "641/641 [==============================] - 0s 180us/step - loss: 0.2013\n",
      "Epoch 1/1\n",
      "641/641 [==============================] - 0s 193us/step - loss: 0.2012\n",
      "player 1 wins: 12\n",
      "player 2 wins: 12\n",
      "simulations took 61.4780330657959 seconds.\n",
      "neural network update time: 0.24521780014038086 seconds.\n",
      "eval score on batch: 0.20941304719996712\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "779/779 [==============================] - 0s 49us/step\n",
      "779/779 [==============================] - 0s 46us/step\n",
      "updating with 779, 779 training batch.\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 193us/step - loss: 0.1956\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 206us/step - loss: 0.1977\n",
      "player 1 wins: 16\n",
      "player 2 wins: 13\n",
      "simulations took 61.08730912208557 seconds.\n",
      "neural network update time: 0.31778669357299805 seconds.\n",
      "eval score on batch: 0.19636465772622969\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "673/673 [==============================] - 0s 44us/step\n",
      "673/673 [==============================] - 0s 42us/step\n",
      "updating with 673, 673 training batch.\n",
      "Epoch 1/1\n",
      "673/673 [==============================] - 0s 188us/step - loss: 0.2000\n",
      "Epoch 1/1\n",
      "673/673 [==============================] - 0s 191us/step - loss: 0.1951\n",
      "player 1 wins: 12\n",
      "player 2 wins: 13\n",
      "simulations took 61.81154012680054 seconds.\n",
      "neural network update time: 0.26003408432006836 seconds.\n",
      "eval score on batch: 0.1973322553571468\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "715/715 [==============================] - 0s 44us/step\n",
      "715/715 [==============================] - 0s 38us/step\n",
      "updating with 715, 715 training batch.\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 182us/step - loss: 0.2028\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 186us/step - loss: 0.2037\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 58.827662229537964 seconds.\n",
      "neural network update time: 0.26941800117492676 seconds.\n",
      "eval score on batch: 0.209203582220561\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "585/585 [==============================] - 0s 44us/step\n",
      "585/585 [==============================] - 0s 40us/step\n",
      "updating with 585, 585 training batch.\n",
      "Epoch 1/1\n",
      "585/585 [==============================] - 0s 178us/step - loss: 0.1986\n",
      "Epoch 1/1\n",
      "585/585 [==============================] - 0s 191us/step - loss: 0.1999\n",
      "player 1 wins: 11\n",
      "player 2 wins: 11\n",
      "simulations took 61.89862680435181 seconds.\n",
      "neural network update time: 0.220628023147583 seconds.\n",
      "eval score on batch: 0.2029638210677693\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "821/821 [==============================] - 0s 41us/step\n",
      "821/821 [==============================] - 0s 39us/step\n",
      "updating with 821, 821 training batch.\n",
      "Epoch 1/1\n",
      "821/821 [==============================] - 0s 183us/step - loss: 0.1923\n",
      "Epoch 1/1\n",
      "821/821 [==============================] - 0s 183us/step - loss: 0.1919\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 61.1947238445282 seconds.\n",
      "neural network update time: 0.30652308464050293 seconds.\n",
      "eval score on batch: 0.1924988048266842\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "647/647 [==============================] - 0s 50us/step\n",
      "647/647 [==============================] - 0s 38us/step\n",
      "updating with 647, 647 training batch.\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 179us/step - loss: 0.1937\n",
      "Epoch 1/1\n",
      "647/647 [==============================] - 0s 195us/step - loss: 0.1941\n",
      "player 1 wins: 12\n",
      "player 2 wins: 12\n",
      "simulations took 61.37414193153381 seconds.\n",
      "neural network update time: 0.24700212478637695 seconds.\n",
      "eval score on batch: 0.1960482863533718\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "696/696 [==============================] - 0s 42us/step\n",
      "696/696 [==============================] - 0s 38us/step\n",
      "updating with 696, 696 training batch.\n",
      "Epoch 1/1\n",
      "696/696 [==============================] - 0s 232us/step - loss: 0.1885\n",
      "Epoch 1/1\n",
      "696/696 [==============================] - 0s 191us/step - loss: 0.1891\n",
      "player 1 wins: 13\n",
      "player 2 wins: 12\n",
      "simulations took 62.09270215034485 seconds.\n",
      "neural network update time: 0.3000190258026123 seconds.\n",
      "eval score on batch: 0.18903218201179614\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "682/682 [==============================] - 0s 44us/step\n",
      "682/682 [==============================] - 0s 41us/step\n",
      "updating with 682, 682 training batch.\n",
      "Epoch 1/1\n",
      "682/682 [==============================] - 0s 190us/step - loss: 0.1989\n",
      "Epoch 1/1\n",
      "682/682 [==============================] - 0s 181us/step - loss: 0.1993\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 60.104379177093506 seconds.\n",
      "neural network update time: 0.2582089900970459 seconds.\n",
      "eval score on batch: 0.2032404977861038\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "731/731 [==============================] - 0s 42us/step\n",
      "731/731 [==============================] - 0s 36us/step\n",
      "updating with 731, 731 training batch.\n",
      "Epoch 1/1\n",
      "731/731 [==============================] - 0s 173us/step - loss: 0.1925\n",
      "Epoch 1/1\n",
      "731/731 [==============================] - 0s 174us/step - loss: 0.1929\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 61.30923795700073 seconds.\n",
      "neural network update time: 0.2592940330505371 seconds.\n",
      "eval score on batch: 0.19283685528465563\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "749/749 [==============================] - 0s 43us/step\n",
      "749/749 [==============================] - 0s 38us/step\n",
      "updating with 749, 749 training batch.\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 195us/step - loss: 0.1917\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 183us/step - loss: 0.1909\n",
      "player 1 wins: 18\n",
      "player 2 wins: 10\n",
      "simulations took 60.56572914123535 seconds.\n",
      "neural network update time: 0.2895848751068115 seconds.\n",
      "eval score on batch: 0.20464906618944076\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "83 outputs loaded.\n",
      "579/579 [==============================] - 0s 46us/step\n",
      "579/579 [==============================] - 0s 43us/step\n",
      "updating with 579, 579 training batch.\n",
      "Epoch 1/1\n",
      "579/579 [==============================] - 0s 204us/step - loss: 0.2082\n",
      "Epoch 1/1\n",
      "579/579 [==============================] - 0s 183us/step - loss: 0.2094\n",
      "player 1 wins: 7\n",
      "player 2 wins: 15\n",
      "simulations took 61.56100916862488 seconds.\n",
      "neural network update time: 0.23004603385925293 seconds.\n",
      "eval score on batch: 0.24881615424094422\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "776/776 [==============================] - 0s 42us/step\n",
      "776/776 [==============================] - 0s 42us/step\n",
      "updating with 776, 776 training batch.\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 200us/step - loss: 0.2157\n",
      "Epoch 1/1\n",
      "776/776 [==============================] - 0s 179us/step - loss: 0.2175\n",
      "player 1 wins: 18\n",
      "player 2 wins: 11\n",
      "simulations took 60.24059796333313 seconds.\n",
      "neural network update time: 0.30174899101257324 seconds.\n",
      "eval score on batch: 0.2586623917228168\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "726/726 [==============================] - 0s 42us/step\n",
      "726/726 [==============================] - 0s 40us/step\n",
      "updating with 726, 726 training batch.\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 183us/step - loss: 0.1967\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 175us/step - loss: 0.1977\n",
      "player 1 wins: 19\n",
      "player 2 wins: 10\n",
      "simulations took 58.9637508392334 seconds.\n",
      "neural network update time: 0.26596689224243164 seconds.\n",
      "eval score on batch: 0.19858379949387112\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "708/708 [==============================] - 0s 42us/step\n",
      "708/708 [==============================] - 0s 40us/step\n",
      "updating with 708, 708 training batch.\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 188us/step - loss: 0.1972\n",
      "Epoch 1/1\n",
      "708/708 [==============================] - 0s 227us/step - loss: 0.2040\n",
      "player 1 wins: 10\n",
      "player 2 wins: 17\n",
      "simulations took 60.40430474281311 seconds.\n",
      "neural network update time: 0.29953908920288086 seconds.\n",
      "eval score on batch: 0.22966603907403974\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "719/719 [==============================] - 0s 42us/step\n",
      "719/719 [==============================] - 0s 40us/step\n",
      "updating with 719, 719 training batch.\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 178us/step - loss: 0.2023\n",
      "Epoch 1/1\n",
      "719/719 [==============================] - 0s 191us/step - loss: 0.1938\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 62.100707054138184 seconds.\n",
      "neural network update time: 0.27082371711730957 seconds.\n",
      "eval score on batch: 0.21719944955825143\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "496/496 [==============================] - 0s 46us/step\n",
      "496/496 [==============================] - 0s 43us/step\n",
      "updating with 496, 496 training batch.\n",
      "Epoch 1/1\n",
      "496/496 [==============================] - 0s 180us/step - loss: 0.1923\n",
      "Epoch 1/1\n",
      "496/496 [==============================] - 0s 180us/step - loss: 0.1926\n",
      "player 1 wins: 12\n",
      "player 2 wins: 7\n",
      "simulations took 62.227848291397095 seconds.\n",
      "neural network update time: 0.18340826034545898 seconds.\n",
      "eval score on batch: 0.1931083793601682\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "773/773 [==============================] - 0s 39us/step\n",
      "773/773 [==============================] - 0s 39us/step\n",
      "updating with 773, 773 training batch.\n",
      "Epoch 1/1\n",
      "773/773 [==============================] - 0s 196us/step - loss: 0.1950\n",
      "Epoch 1/1\n",
      "773/773 [==============================] - 0s 179us/step - loss: 0.1974\n",
      "player 1 wins: 14\n",
      "player 2 wins: 14\n",
      "simulations took 61.903472900390625 seconds.\n",
      "neural network update time: 0.29558396339416504 seconds.\n",
      "eval score on batch: 0.19932449289524076\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "751/751 [==============================] - 0s 40us/step\n",
      "751/751 [==============================] - 0s 39us/step\n",
      "updating with 751, 751 training batch.\n",
      "Epoch 1/1\n",
      "751/751 [==============================] - 0s 183us/step - loss: 0.2008\n",
      "Epoch 1/1\n",
      "751/751 [==============================] - 0s 188us/step - loss: 0.2029\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 59.83583903312683 seconds.\n",
      "neural network update time: 0.28357887268066406 seconds.\n",
      "eval score on batch: 0.20090790473410833\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "718/718 [==============================] - 0s 42us/step\n",
      "718/718 [==============================] - 0s 40us/step\n",
      "updating with 718, 718 training batch.\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 189us/step - loss: 0.1948\n",
      "Epoch 1/1\n",
      "718/718 [==============================] - 0s 192us/step - loss: 0.1953\n",
      "player 1 wins: 10\n",
      "player 2 wins: 18\n",
      "simulations took 59.99071407318115 seconds.\n",
      "neural network update time: 0.27973508834838867 seconds.\n",
      "eval score on batch: 0.19828271322097618\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "535/535 [==============================] - 0s 42us/step\n",
      "535/535 [==============================] - 0s 39us/step\n",
      "updating with 535, 535 training batch.\n",
      "Epoch 1/1\n",
      "535/535 [==============================] - 0s 186us/step - loss: 0.1975\n",
      "Epoch 1/1\n",
      "535/535 [==============================] - 0s 195us/step - loss: 0.1969\n",
      "player 1 wins: 8\n",
      "player 2 wins: 13\n",
      "simulations took 61.15034079551697 seconds.\n",
      "neural network update time: 0.21029925346374512 seconds.\n",
      "eval score on batch: 0.19973538002678168\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "726/726 [==============================] - 0s 47us/step\n",
      "726/726 [==============================] - 0s 48us/step\n",
      "updating with 726, 726 training batch.\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 204us/step - loss: 0.1901\n",
      "Epoch 1/1\n",
      "726/726 [==============================] - 0s 210us/step - loss: 0.1898\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 62.77697682380676 seconds.\n",
      "neural network update time: 0.30649805068969727 seconds.\n",
      "eval score on batch: 0.19318882753615865\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "622/622 [==============================] - 0s 58us/step\n",
      "622/622 [==============================] - 0s 53us/step\n",
      "updating with 622, 622 training batch.\n",
      "Epoch 1/1\n",
      "622/622 [==============================] - 0s 205us/step - loss: 0.2021\n",
      "Epoch 1/1\n",
      "622/622 [==============================] - 0s 213us/step - loss: 0.2023\n",
      "player 1 wins: 12\n",
      "player 2 wins: 12\n",
      "simulations took 60.596932888031006 seconds.\n",
      "neural network update time: 0.26856184005737305 seconds.\n",
      "eval score on batch: 0.20167346791606816\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "820/820 [==============================] - 0s 47us/step\n",
      "820/820 [==============================] - 0s 42us/step\n",
      "updating with 820, 820 training batch.\n",
      "Epoch 1/1\n",
      "820/820 [==============================] - 0s 175us/step - loss: 0.1821\n",
      "Epoch 1/1\n",
      "820/820 [==============================] - 0s 176us/step - loss: 0.1823\n",
      "player 1 wins: 11\n",
      "player 2 wins: 18\n",
      "simulations took 61.9185791015625 seconds.\n",
      "neural network update time: 0.2935822010040283 seconds.\n",
      "eval score on batch: 0.1869432326587962\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "750/750 [==============================] - 0s 61us/step\n",
      "750/750 [==============================] - 0s 51us/step\n",
      "updating with 750, 750 training batch.\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 194us/step - loss: 0.1852\n",
      "Epoch 1/1\n",
      "750/750 [==============================] - 0s 202us/step - loss: 0.1839\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 61.719260931015015 seconds.\n",
      "neural network update time: 0.305189847946167 seconds.\n",
      "eval score on batch: 0.1848735409018894\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "697/697 [==============================] - 0s 46us/step\n",
      "697/697 [==============================] - 0s 38us/step\n",
      "updating with 697, 697 training batch.\n",
      "Epoch 1/1\n",
      "697/697 [==============================] - 0s 208us/step - loss: 0.2009\n",
      "Epoch 1/1\n",
      "697/697 [==============================] - 0s 207us/step - loss: 0.2016\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 59.603078842163086 seconds.\n",
      "neural network update time: 0.29514288902282715 seconds.\n",
      "eval score on batch: 0.20094374168551976\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "796/796 [==============================] - 0s 41us/step\n",
      "796/796 [==============================] - 0s 38us/step\n",
      "updating with 796, 796 training batch.\n",
      "Epoch 1/1\n",
      "796/796 [==============================] - 0s 220us/step - loss: 0.1986\n",
      "Epoch 1/1\n",
      "796/796 [==============================] - 0s 199us/step - loss: 0.1984\n",
      "player 1 wins: 16\n",
      "player 2 wins: 14\n",
      "simulations took 59.79279804229736 seconds.\n",
      "neural network update time: 0.34061694145202637 seconds.\n",
      "eval score on batch: 0.19914082434009667\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "754/754 [==============================] - 0s 39us/step\n",
      "754/754 [==============================] - 0s 41us/step\n",
      "updating with 754, 754 training batch.\n",
      "Epoch 1/1\n",
      "754/754 [==============================] - 0s 195us/step - loss: 0.1989\n",
      "Epoch 1/1\n",
      "754/754 [==============================] - 0s 188us/step - loss: 0.1990\n",
      "player 1 wins: 17\n",
      "player 2 wins: 12\n",
      "simulations took 60.22942304611206 seconds.\n",
      "neural network update time: 0.2963871955871582 seconds.\n",
      "eval score on batch: 0.19856900214378176\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "742/742 [==============================] - 0s 47us/step\n",
      "742/742 [==============================] - 0s 38us/step\n",
      "updating with 742, 742 training batch.\n",
      "Epoch 1/1\n",
      "742/742 [==============================] - 0s 180us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "742/742 [==============================] - 0s 182us/step - loss: 0.1906\n",
      "player 1 wins: 11\n",
      "player 2 wins: 16\n",
      "simulations took 61.83759307861328 seconds.\n",
      "neural network update time: 0.2742800712585449 seconds.\n",
      "eval score on batch: 0.2058035627690287\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "554/554 [==============================] - 0s 43us/step\n",
      "554/554 [==============================] - 0s 40us/step\n",
      "updating with 554, 554 training batch.\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 194us/step - loss: 0.2073\n",
      "Epoch 1/1\n",
      "554/554 [==============================] - 0s 200us/step - loss: 0.2155\n",
      "player 1 wins: 11\n",
      "player 2 wins: 11\n",
      "simulations took 60.93416905403137 seconds.\n",
      "neural network update time: 0.22323131561279297 seconds.\n",
      "eval score on batch: 0.22192235601669183\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "774/774 [==============================] - 0s 52us/step\n",
      "774/774 [==============================] - 0s 42us/step\n",
      "updating with 774, 774 training batch.\n",
      "Epoch 1/1\n",
      "774/774 [==============================] - 0s 181us/step - loss: 0.1910\n",
      "Epoch 1/1\n",
      "774/774 [==============================] - 0s 179us/step - loss: 0.1907\n",
      "player 1 wins: 13\n",
      "player 2 wins: 15\n",
      "simulations took 61.88981795310974 seconds.\n",
      "neural network update time: 0.2841191291809082 seconds.\n",
      "eval score on batch: 0.19153283999207638\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "84 outputs loaded.\n",
      "792/792 [==============================] - 0s 44us/step\n",
      "792/792 [==============================] - 0s 37us/step\n",
      "updating with 792, 792 training batch.\n",
      "Epoch 1/1\n",
      "792/792 [==============================] - 0s 182us/step - loss: 0.1899\n",
      "Epoch 1/1\n",
      "792/792 [==============================] - 0s 185us/step - loss: 0.1888\n",
      "player 1 wins: 12\n",
      "player 2 wins: 17\n",
      "simulations took 61.85402488708496 seconds.\n",
      "neural network update time: 0.296525239944458 seconds.\n",
      "eval score on batch: 0.18976328559596128\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "704/704 [==============================] - 0s 43us/step\n",
      "704/704 [==============================] - 0s 41us/step\n",
      "updating with 704, 704 training batch.\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 183us/step - loss: 0.1879\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 187us/step - loss: 0.1875\n",
      "player 1 wins: 10\n",
      "player 2 wins: 16\n",
      "simulations took 61.47813391685486 seconds.\n",
      "neural network update time: 0.2669639587402344 seconds.\n",
      "eval score on batch: 0.18701956742866474\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "747/747 [==============================] - 0s 40us/step\n",
      "747/747 [==============================] - 0s 40us/step\n",
      "updating with 747, 747 training batch.\n",
      "Epoch 1/1\n",
      "747/747 [==============================] - 0s 181us/step - loss: 0.1960\n",
      "Epoch 1/1\n",
      "747/747 [==============================] - 0s 184us/step - loss: 0.1955\n",
      "player 1 wins: 11\n",
      "player 2 wins: 18\n",
      "simulations took 60.05377793312073 seconds.\n",
      "neural network update time: 0.27877306938171387 seconds.\n",
      "eval score on batch: 0.19515672990356583\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "681/681 [==============================] - 0s 62us/step\n",
      "681/681 [==============================] - 0s 77us/step\n",
      "updating with 681, 681 training batch.\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 228us/step - loss: 0.1919\n",
      "Epoch 1/1\n",
      "681/681 [==============================] - 0s 269us/step - loss: 0.1948\n",
      "player 1 wins: 15\n",
      "player 2 wins: 10\n",
      "simulations took 61.78703689575195 seconds.\n",
      "neural network update time: 0.3451669216156006 seconds.\n",
      "eval score on batch: 0.20831011424645257\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "631/631 [==============================] - 0s 48us/step\n",
      "631/631 [==============================] - 0s 37us/step\n",
      "updating with 631, 631 training batch.\n",
      "Epoch 1/1\n",
      "631/631 [==============================] - 0s 173us/step - loss: 0.1959\n",
      "Epoch 1/1\n",
      "631/631 [==============================] - 0s 181us/step - loss: 0.1952\n",
      "player 1 wins: 12\n",
      "player 2 wins: 11\n",
      "simulations took 62.62302207946777 seconds.\n",
      "neural network update time: 0.22821474075317383 seconds.\n",
      "eval score on batch: 0.20128432728598122\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "698/698 [==============================] - 0s 43us/step\n",
      "698/698 [==============================] - 0s 50us/step\n",
      "updating with 698, 698 training batch.\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 167us/step - loss: 0.2025\n",
      "Epoch 1/1\n",
      "698/698 [==============================] - 0s 168us/step - loss: 0.2034\n",
      "player 1 wins: 14\n",
      "player 2 wins: 13\n",
      "simulations took 60.127690076828 seconds.\n",
      "neural network update time: 0.23859715461730957 seconds.\n",
      "eval score on batch: 0.20273887844259897\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "596/596 [==============================] - 0s 44us/step\n",
      "596/596 [==============================] - 0s 43us/step\n",
      "updating with 596, 596 training batch.\n",
      "Epoch 1/1\n",
      "596/596 [==============================] - 0s 153us/step - loss: 0.1922\n",
      "Epoch 1/1\n",
      "596/596 [==============================] - 0s 152us/step - loss: 0.1956\n",
      "player 1 wins: 9\n",
      "player 2 wins: 13\n",
      "simulations took 56.75353026390076 seconds.\n",
      "neural network update time: 0.18703198432922363 seconds.\n",
      "eval score on batch: 0.19895586331418696\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "689/689 [==============================] - 0s 40us/step\n",
      "689/689 [==============================] - 0s 40us/step\n",
      "updating with 689, 689 training batch.\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 159us/step - loss: 0.1967\n",
      "Epoch 1/1\n",
      "689/689 [==============================] - 0s 157us/step - loss: 0.1957\n",
      "player 1 wins: 11\n",
      "player 2 wins: 15\n",
      "simulations took 55.60795998573303 seconds.\n",
      "neural network update time: 0.22269797325134277 seconds.\n",
      "eval score on batch: 0.19558185192381933\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "571/571 [==============================] - 0s 44us/step\n",
      "571/571 [==============================] - 0s 40us/step\n",
      "updating with 571, 571 training batch.\n",
      "Epoch 1/1\n",
      "571/571 [==============================] - 0s 157us/step - loss: 0.1886\n",
      "Epoch 1/1\n",
      "571/571 [==============================] - 0s 151us/step - loss: 0.1948\n",
      "player 1 wins: 13\n",
      "player 2 wins: 8\n",
      "simulations took 56.37650513648987 seconds.\n",
      "neural network update time: 0.1807880401611328 seconds.\n",
      "eval score on batch: 0.20762592240933153\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "799/799 [==============================] - 0s 41us/step\n",
      "799/799 [==============================] - 0s 43us/step\n",
      "updating with 799, 799 training batch.\n",
      "Epoch 1/1\n",
      "799/799 [==============================] - 0s 158us/step - loss: 0.2158\n",
      "Epoch 1/1\n",
      "799/799 [==============================] - 0s 158us/step - loss: 0.2154\n",
      "player 1 wins: 16\n",
      "player 2 wins: 16\n",
      "simulations took 52.30999398231506 seconds.\n",
      "neural network update time: 0.2585790157318115 seconds.\n",
      "eval score on batch: 0.22219232708514108\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "624/624 [==============================] - 0s 44us/step\n",
      "624/624 [==============================] - 0s 40us/step\n",
      "updating with 624, 624 training batch.\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 154us/step - loss: 0.1983\n",
      "Epoch 1/1\n",
      "624/624 [==============================] - 0s 155us/step - loss: 0.1991\n",
      "player 1 wins: 10\n",
      "player 2 wins: 14\n",
      "simulations took 55.060200929641724 seconds.\n",
      "neural network update time: 0.19809985160827637 seconds.\n",
      "eval score on batch: 0.1992913266787162\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "756/756 [==============================] - 0s 42us/step\n",
      "756/756 [==============================] - 0s 40us/step\n",
      "updating with 756, 756 training batch.\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 155us/step - loss: 0.1993\n",
      "Epoch 1/1\n",
      "756/756 [==============================] - 0s 152us/step - loss: 0.1998\n",
      "player 1 wins: 13\n",
      "player 2 wins: 16\n",
      "simulations took 54.35352110862732 seconds.\n",
      "neural network update time: 0.23733806610107422 seconds.\n",
      "eval score on batch: 0.1996673918748028\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "749/749 [==============================] - 0s 41us/step\n",
      "749/749 [==============================] - 0s 37us/step\n",
      "updating with 749, 749 training batch.\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 156us/step - loss: 0.1879\n",
      "Epoch 1/1\n",
      "749/749 [==============================] - 0s 164us/step - loss: 0.1880\n",
      "player 1 wins: 12\n",
      "player 2 wins: 15\n",
      "simulations took 56.14715385437012 seconds.\n",
      "neural network update time: 0.2450699806213379 seconds.\n",
      "eval score on batch: 0.18749352430648733\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "716/716 [==============================] - 0s 38us/step\n",
      "716/716 [==============================] - 0s 55us/step\n",
      "updating with 716, 716 training batch.\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 183us/step - loss: 0.1884\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 157us/step - loss: 0.1929\n",
      "player 1 wins: 18\n",
      "player 2 wins: 9\n",
      "simulations took 55.050782918930054 seconds.\n",
      "neural network update time: 0.24891304969787598 seconds.\n",
      "eval score on batch: 0.2070919954629001\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "645/645 [==============================] - 0s 39us/step\n",
      "645/645 [==============================] - 0s 40us/step\n",
      "updating with 645, 645 training batch.\n",
      "Epoch 1/1\n",
      "645/645 [==============================] - 0s 156us/step - loss: 0.1970\n",
      "Epoch 1/1\n",
      "645/645 [==============================] - 0s 156us/step - loss: 0.1965\n",
      "player 1 wins: 14\n",
      "player 2 wins: 10\n",
      "simulations took 55.908742904663086 seconds.\n",
      "neural network update time: 0.20644283294677734 seconds.\n",
      "eval score on batch: 0.19977885491626207\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "678/678 [==============================] - 0s 38us/step\n",
      "678/678 [==============================] - 0s 39us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 153us/step - loss: 0.1921\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.1967\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 56.064465045928955 seconds.\n",
      "neural network update time: 0.21492886543273926 seconds.\n",
      "eval score on batch: 0.2023280462793385\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "679/679 [==============================] - 0s 44us/step\n",
      "679/679 [==============================] - 0s 39us/step\n",
      "updating with 679, 679 training batch.\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 163us/step - loss: 0.2020\n",
      "Epoch 1/1\n",
      "679/679 [==============================] - 0s 160us/step - loss: 0.2011\n",
      "player 1 wins: 13\n",
      "player 2 wins: 13\n",
      "simulations took 54.727465867996216 seconds.\n",
      "neural network update time: 0.2236647605895996 seconds.\n",
      "eval score on batch: 0.20831992441045247\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "665/665 [==============================] - 0s 40us/step\n",
      "665/665 [==============================] - 0s 40us/step\n",
      "updating with 665, 665 training batch.\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 160us/step - loss: 0.1890\n",
      "Epoch 1/1\n",
      "665/665 [==============================] - 0s 167us/step - loss: 0.1916\n",
      "player 1 wins: 8\n",
      "player 2 wins: 17\n",
      "simulations took 55.75746393203735 seconds.\n",
      "neural network update time: 0.2229297161102295 seconds.\n",
      "eval score on batch: 0.20488713276117368\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "814/814 [==============================] - 0s 39us/step\n",
      "814/814 [==============================] - 0s 39us/step\n",
      "updating with 814, 814 training batch.\n",
      "Epoch 1/1\n",
      "814/814 [==============================] - 0s 153us/step - loss: 0.2100\n",
      "Epoch 1/1\n",
      "814/814 [==============================] - 0s 154us/step - loss: 0.2062\n",
      "player 1 wins: 18\n",
      "player 2 wins: 13\n",
      "simulations took 54.402663230895996 seconds.\n",
      "neural network update time: 0.25566577911376953 seconds.\n",
      "eval score on batch: 0.2409835125672052\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "658/658 [==============================] - 0s 46us/step\n",
      "658/658 [==============================] - 0s 42us/step\n",
      "updating with 658, 658 training batch.\n",
      "Epoch 1/1\n",
      "658/658 [==============================] - 0s 154us/step - loss: 0.2026\n",
      "Epoch 1/1\n",
      "658/658 [==============================] - 0s 158us/step - loss: 0.1955\n",
      "player 1 wins: 9\n",
      "player 2 wins: 15\n",
      "simulations took 56.267935037612915 seconds.\n",
      "neural network update time: 0.21112608909606934 seconds.\n",
      "eval score on batch: 0.2301361193048193\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "85 outputs loaded.\n",
      "779/779 [==============================] - 0s 41us/step\n",
      "779/779 [==============================] - 0s 38us/step\n",
      "updating with 779, 779 training batch.\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 157us/step - loss: 0.1800\n",
      "Epoch 1/1\n",
      "779/779 [==============================] - 0s 155us/step - loss: 0.1812\n",
      "player 1 wins: 10\n",
      "player 2 wins: 18\n",
      "simulations took 55.528706789016724 seconds.\n",
      "neural network update time: 0.24785399436950684 seconds.\n",
      "eval score on batch: 0.18235573029583166\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "777/777 [==============================] - 0s 43us/step\n",
      "777/777 [==============================] - 0s 41us/step\n",
      "updating with 777, 777 training batch.\n",
      "Epoch 1/1\n",
      "777/777 [==============================] - 0s 158us/step - loss: 0.1972\n",
      "Epoch 1/1\n",
      "777/777 [==============================] - 0s 156us/step - loss: 0.1963\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 54.823445081710815 seconds.\n",
      "neural network update time: 0.2489757537841797 seconds.\n",
      "eval score on batch: 0.20047615747059788\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "843/843 [==============================] - 0s 45us/step\n",
      "843/843 [==============================] - 0s 42us/step\n",
      "updating with 843, 843 training batch.\n",
      "Epoch 1/1\n",
      "843/843 [==============================] - 0s 157us/step - loss: 0.1869\n",
      "Epoch 1/1\n",
      "843/843 [==============================] - 0s 157us/step - loss: 0.1866\n",
      "player 1 wins: 15\n",
      "player 2 wins: 15\n",
      "simulations took 55.71949481964111 seconds.\n",
      "neural network update time: 0.2706003189086914 seconds.\n",
      "eval score on batch: 0.18734999348918774\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "774/774 [==============================] - 0s 39us/step\n",
      "774/774 [==============================] - 0s 40us/step\n",
      "updating with 774, 774 training batch.\n",
      "Epoch 1/1\n",
      "774/774 [==============================] - 0s 156us/step - loss: 0.1963\n",
      "Epoch 1/1\n",
      "774/774 [==============================] - 0s 154us/step - loss: 0.1987\n",
      "player 1 wins: 15\n",
      "player 2 wins: 14\n",
      "simulations took 55.11472702026367 seconds.\n",
      "neural network update time: 0.24546504020690918 seconds.\n",
      "eval score on batch: 0.19961831911204705\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "653/653 [==============================] - 0s 44us/step\n",
      "653/653 [==============================] - 0s 37us/step\n",
      "updating with 653, 653 training batch.\n",
      "Epoch 1/1\n",
      "653/653 [==============================] - 0s 162us/step - loss: 0.2001\n",
      "Epoch 1/1\n",
      "653/653 [==============================] - 0s 164us/step - loss: 0.1996\n",
      "player 1 wins: 11\n",
      "player 2 wins: 14\n",
      "simulations took 56.94127607345581 seconds.\n",
      "neural network update time: 0.21828699111938477 seconds.\n",
      "eval score on batch: 0.2024023351608954\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "678/678 [==============================] - 0s 42us/step\n",
      "678/678 [==============================] - 0s 39us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 154us/step - loss: 0.1881\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 154us/step - loss: 0.1906\n",
      "player 1 wins: 17\n",
      "player 2 wins: 8\n",
      "simulations took 57.86639595031738 seconds.\n",
      "neural network update time: 0.21393585205078125 seconds.\n",
      "eval score on batch: 0.2118803782492964\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "704/704 [==============================] - 0s 38us/step\n",
      "704/704 [==============================] - 0s 40us/step\n",
      "updating with 704, 704 training batch.\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 159us/step - loss: 0.2019\n",
      "Epoch 1/1\n",
      "704/704 [==============================] - 0s 148us/step - loss: 0.1948\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 56.81602883338928 seconds.\n",
      "neural network update time: 0.2213120460510254 seconds.\n",
      "eval score on batch: 0.20767619156024672\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "661/661 [==============================] - 0s 40us/step\n",
      "661/661 [==============================] - 0s 39us/step\n",
      "updating with 661, 661 training batch.\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 156us/step - loss: 0.1971\n",
      "Epoch 1/1\n",
      "661/661 [==============================] - 0s 151us/step - loss: 0.1927\n",
      "player 1 wins: 10\n",
      "player 2 wins: 15\n",
      "simulations took 55.63342070579529 seconds.\n",
      "neural network update time: 0.20842719078063965 seconds.\n",
      "eval score on batch: 0.19936831270692568\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "729/729 [==============================] - 0s 44us/step\n",
      "729/729 [==============================] - 0s 36us/step\n",
      "updating with 729, 729 training batch.\n",
      "Epoch 1/1\n",
      "729/729 [==============================] - 0s 152us/step - loss: 0.1791\n",
      "Epoch 1/1\n",
      "729/729 [==============================] - 0s 153us/step - loss: 0.1801\n",
      "player 1 wins: 8\n",
      "player 2 wins: 20\n",
      "simulations took 54.3034348487854 seconds.\n",
      "neural network update time: 0.22825002670288086 seconds.\n",
      "eval score on batch: 0.1799648167011015\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "628/628 [==============================] - 0s 53us/step\n",
      "628/628 [==============================] - 0s 48us/step\n",
      "updating with 628, 628 training batch.\n",
      "Epoch 1/1\n",
      "628/628 [==============================] - 0s 156us/step - loss: 0.1987\n",
      "Epoch 1/1\n",
      "628/628 [==============================] - 0s 152us/step - loss: 0.1978\n",
      "player 1 wins: 10\n",
      "player 2 wins: 14\n",
      "simulations took 55.40184712409973 seconds.\n",
      "neural network update time: 0.19895005226135254 seconds.\n",
      "eval score on batch: 0.20507820795296103\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "716/716 [==============================] - 0s 39us/step\n",
      "716/716 [==============================] - 0s 39us/step\n",
      "updating with 716, 716 training batch.\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 153us/step - loss: 0.1895\n",
      "Epoch 1/1\n",
      "716/716 [==============================] - 0s 153us/step - loss: 0.1877\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 56.72787690162659 seconds.\n",
      "neural network update time: 0.22435903549194336 seconds.\n",
      "eval score on batch: 0.19384195202800272\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "674/674 [==============================] - 0s 49us/step\n",
      "674/674 [==============================] - 0s 38us/step\n",
      "updating with 674, 674 training batch.\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 173us/step - loss: 0.1971\n",
      "Epoch 1/1\n",
      "674/674 [==============================] - 0s 242us/step - loss: 0.1977\n",
      "player 1 wins: 8\n",
      "player 2 wins: 18\n",
      "simulations took 57.31847596168518 seconds.\n",
      "neural network update time: 0.28516697883605957 seconds.\n",
      "eval score on batch: 0.23022215328083892\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "832/832 [==============================] - 0s 39us/step\n",
      "832/832 [==============================] - 0s 39us/step\n",
      "updating with 832, 832 training batch.\n",
      "Epoch 1/1\n",
      "832/832 [==============================] - 0s 160us/step - loss: 0.2036\n",
      "Epoch 1/1\n",
      "832/832 [==============================] - 0s 162us/step - loss: 0.2063\n",
      "player 1 wins: 17\n",
      "player 2 wins: 14\n",
      "simulations took 55.027393102645874 seconds.\n",
      "neural network update time: 0.2732868194580078 seconds.\n",
      "eval score on batch: 0.24298507662919852\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "678/678 [==============================] - 0s 41us/step\n",
      "678/678 [==============================] - 0s 38us/step\n",
      "updating with 678, 678 training batch.\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 159us/step - loss: 0.2009\n",
      "Epoch 1/1\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.1994\n",
      "player 1 wins: 15\n",
      "player 2 wins: 11\n",
      "simulations took 56.17122220993042 seconds.\n",
      "neural network update time: 0.2193450927734375 seconds.\n",
      "eval score on batch: 0.19862046136979597\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "597/597 [==============================] - 0s 42us/step\n",
      "597/597 [==============================] - 0s 37us/step\n",
      "updating with 597, 597 training batch.\n",
      "Epoch 1/1\n",
      "597/597 [==============================] - 0s 156us/step - loss: 0.1839\n",
      "Epoch 1/1\n",
      "597/597 [==============================] - 0s 163us/step - loss: 0.1851\n",
      "player 1 wins: 16\n",
      "player 2 wins: 7\n",
      "simulations took 55.35641026496887 seconds.\n",
      "neural network update time: 0.1958320140838623 seconds.\n",
      "eval score on batch: 0.19210678009531607\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "740/740 [==============================] - 0s 45us/step\n",
      "740/740 [==============================] - 0s 45us/step\n",
      "updating with 740, 740 training batch.\n",
      "Epoch 1/1\n",
      "740/740 [==============================] - 0s 157us/step - loss: 0.1879\n",
      "Epoch 1/1\n",
      "740/740 [==============================] - 0s 155us/step - loss: 0.1853\n",
      "player 1 wins: 17\n",
      "player 2 wins: 10\n",
      "simulations took 55.82425284385681 seconds.\n",
      "neural network update time: 0.23645901679992676 seconds.\n",
      "eval score on batch: 0.1892408212848209\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "629/629 [==============================] - 0s 38us/step\n",
      "629/629 [==============================] - 0s 42us/step\n",
      "updating with 629, 629 training batch.\n",
      "Epoch 1/1\n",
      "629/629 [==============================] - 0s 153us/step - loss: 0.1932\n",
      "Epoch 1/1\n",
      "629/629 [==============================] - 0s 152us/step - loss: 0.1915\n",
      "player 1 wins: 12\n",
      "player 2 wins: 11\n",
      "simulations took 56.1482949256897 seconds.\n",
      "neural network update time: 0.19678688049316406 seconds.\n",
      "eval score on batch: 0.19189424543613376\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "715/715 [==============================] - 0s 39us/step\n",
      "715/715 [==============================] - 0s 40us/step\n",
      "updating with 715, 715 training batch.\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 156us/step - loss: 0.1779\n",
      "Epoch 1/1\n",
      "715/715 [==============================] - 0s 154us/step - loss: 0.1777\n",
      "player 1 wins: 16\n",
      "player 2 wins: 9\n",
      "simulations took 57.020161151885986 seconds.\n",
      "neural network update time: 0.22762393951416016 seconds.\n",
      "eval score on batch: 0.18394959506455003\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "683/683 [==============================] - 0s 42us/step\n",
      "683/683 [==============================] - 0s 42us/step\n",
      "updating with 683, 683 training batch.\n",
      "Epoch 1/1\n",
      "683/683 [==============================] - 0s 156us/step - loss: 0.1970\n",
      "Epoch 1/1\n",
      "683/683 [==============================] - 0s 155us/step - loss: 0.1988\n",
      "player 1 wins: 9\n",
      "player 2 wins: 16\n",
      "simulations took 55.941200971603394 seconds.\n",
      "neural network update time: 0.21738791465759277 seconds.\n",
      "eval score on batch: 0.22650377497644236\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "772/772 [==============================] - 0s 40us/step\n",
      "772/772 [==============================] - 0s 41us/step\n",
      "updating with 772, 772 training batch.\n",
      "Epoch 1/1\n",
      "772/772 [==============================] - 0s 155us/step - loss: 0.2043\n",
      "Epoch 1/1\n",
      "772/772 [==============================] - 0s 157us/step - loss: 0.2019\n",
      "player 1 wins: 16\n",
      "player 2 wins: 12\n",
      "simulations took 55.58668112754822 seconds.\n",
      "neural network update time: 0.24668598175048828 seconds.\n",
      "eval score on batch: 0.22807210255317736\n",
      "epsilons agent1 and agent2: 0.09958616740567054 0.09958616740567054\n",
      "86 outputs loaded.\n",
      "31263.987075805664 seconds.\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## Change number of games to simulate here\n",
    "#_games = 10000\n",
    "##################################################\n",
    "time0 = time.time()\n",
    "for x in range(27):\n",
    "    outputs.append(big_sim_parallel_nn(agent1, agent2, n_steps=20, games_per_step=32, nn_to_update=[nn_1, nn_2], parallel_threads=6))\n",
    "    print(len(outputs), \"outputs loaded.\")\n",
    "print(time.time()-time0, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game_step 0 updating with 666, 666 training batch.\n",
      "player 1 wins: 26\n",
      "player 2 wins: 6\n",
      "simulations took 54.458451986312866 seconds.\n",
      "neural network update time: 0.00013375282287597656 seconds.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-75d25f1d451f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# no decay, fully random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# set agent1 to no random moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreturns3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_sim_parallel_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames_per_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_to_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# no update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-43e026acaf9c>\u001b[0m in \u001b[0;36mbig_sim_parallel_nn\u001b[0;34m(agent1, agent2, n_steps, games_per_step, nn_to_update, parallel_threads)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"simulations took\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neural network update time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_update_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval score on batch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epsilons agent1 and agent2:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mepsilons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "random_agent = nn_agent(player = -1,  nn = nn_1, epsilon_min = 0, epsilon = 1, epsilon_decay = 1)  # no decay, fully random\n",
    "agent1.epsilon = 0 # set agent1 to no random moves\n",
    "returns3 = big_sim_parallel_nn(agent1, random_agent, n_steps=1, games_per_step=32, nn_to_update=[], parallel_threads=6) # no update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.593888888888888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59738/60/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5444204"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qtable1.q_dict)\n",
    "#agent1.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = [2,2,2,2,2]\n",
    "\n",
    "with open('test.pickle', 'wb') as file:\n",
    "    pickle.dump(x, file, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('test.pickle', 'rb') as file:\n",
    "    y = pickle.load(file)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = []\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    ns.append(v[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5444199,       3,       1,       0,       0,       0,       0,\n",
       "              0,       0,       1]),\n",
       " array([1.000000e+00, 1.797250e+04, 3.594400e+04, 5.391550e+04,\n",
       "        7.188700e+04, 8.985850e+04, 1.078300e+05, 1.258015e+05,\n",
       "        1.437730e+05, 1.617445e+05, 1.797160e+05]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(np.array(ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153873\n",
      "67959\n",
      "46602\n",
      "33784\n",
      "12230\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in ns if x != 1]))\n",
    "print(len([x for x in ns if x > 2]))\n",
    "print(len([x for x in ns if x > 3]))\n",
    "print(len([x for x in ns if x > 4]))\n",
    "print(len([x for x in ns if x > 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120020100111022211102201112221201221] (0.5, 1)\n",
      "[020011100211022012102201112221201221] (0.5, 1)\n",
      "[002011202211010012102201112201201221] (0.5, 1)\n",
      "[002011202211010012210201010201122221] (0.5, 1)\n",
      "[020011100211022012210001010201122221] (0.5, 1)\n",
      "[110020112001200220022012002010111221] (0.5, 1)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in qtable1.q_dict.items():\n",
    "    print(k,v)\n",
    "    i += 1\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([94371,     6,     0,     0,     0,     0,     0,     0,     0,\n",
       "            1]),\n",
       " array([2.00000e+00, 1.15380e+04, 2.30740e+04, 3.46100e+04, 4.61460e+04,\n",
       "        5.76820e+04, 6.92180e+04, 8.07540e+04, 9.22900e+04, 1.03826e+05,\n",
       "        1.15362e+05]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram([x for x in ns if x != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
